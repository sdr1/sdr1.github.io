<!doctype html>
<html
  dir="ltr"
  lang="en"
  data-theme=""
  
    class="html theme--light"
  
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=56609&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <title>
    
      
        Regression |
      Steven Rashin

  </title>

  <meta name="generator" content="Hugo 0.125.4"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta name="author" content="Steven Rashin" />
  <meta
    name="description"
    content="Data Scientist combining data science tools and social science research to study behavior"
  />
  
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.009f917038f30ebd1f2147e8e1dfd40fc1b799422a7869aa0da12af1fd1bf8ba.css"
      integrity="sha256-AJ&#43;RcDjzDr0fIUfo4d/UD8G3mUIqeGmqDaEq8f0b&#43;Lo="
      crossorigin="anonymous"
      type="text/css"
    />
  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.73ccfdf28df555e11009c13c20ced067af3cb021504cba43644c705930428b00.css"
    integrity="sha256-c8z98o31VeEQCcE8IM7QZ688sCFQTLpDZExwWTBCiwA="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css"
    integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css"
    integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css"
    integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css"
    integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicons/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png" />

  <link rel="canonical" href="http://localhost:56609/post/ols-website-version/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js"
    integrity="sha256-&#43;RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js"
      integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc="
      crossorigin="anonymous"
    ></script>
  

  

  


  
  <meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:56609/images/site-feature-image.png"><meta name="twitter:title" content="Regression">
<meta name="twitter:description" content="TL; DR: Checklist Get summary stats for the variables. Check for outliers or weird patterns">



  
  <meta property="og:url" content="http://localhost:56609/post/ols-website-version/">
  <meta property="og:site_name" content="My blog">
  <meta property="og:title" content="Regression">
  <meta property="og:description" content="TL; DR: Checklist Get summary stats for the variables. Check for outliers or weird patterns">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="og:image" content="http://localhost:56609/images/site-feature-image.png">



  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "post",
        "name": "Regression",
        "headline": "Regression",
        "alternativeHeadline": "",
        "description": "
      
        TL; DR: Checklist Get summary stats for the variables. Check for outliers or weird patterns


      


    ",
        "inLanguage": "en",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:56609\/post\/ols-website-version\/"
        },
        "author" : {
            "@type": "Person",
            "name": "Steven Rashin"
        },
        "creator" : {
            "@type": "Person",
            "name": "Steven Rashin"
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": "Steven Rashin"
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": "Steven Rashin"
        },
        "copyrightYear" : "0001",
        "dateCreated": "0001-01-01T00:00:00.00Z",
        "datePublished": "0001-01-01T00:00:00.00Z",
        "dateModified": "0001-01-01T00:00:00.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "Steven Rashin",
            "url": "http://localhost:56609/",
            "logo": {
                "@type": "ImageObject",
                "url": "http:\/\/localhost:56609\/favicons\/favicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
        
        "http://localhost:56609/images/site-feature-image.png"


      
      ]

    ,
        "url" : "http:\/\/localhost:56609\/post\/ols-website-version\/",
        "wordCount" : "16372",
        "genre" : [ ],
        "keywords" : [ ]
    }
  </script>


</head>
<body class="body">
    <div class="wrapper">
      <aside
        
          class="wrapper__sidebar"
        
      ><div
  class="sidebar
    animated fadeInDown
  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/images/rashin-headshot.png"
        alt="profile picture"
      />
      
        <div class="sidebar__introduction-title">
          <a href="/">Steven Rashin</a>
        </div>
      
      <div class="sidebar__introduction-description">
        <p>Data Scientist combining data science tools and social science research to study behavior</p>
      </div>
      <div class="sidebar__introduction-resume">
          <a href="https://www.dropbox.com/scl/fi/aifg9jgg86pj2fbx2quv5/Steven_Rashin_DS_Jan_24.pdf?rlkey=1bhnjbhbib7rtlplmaoqryn43&dl=0"><span style="font-size: larger;"><b>Resume</b></span> </a>
      </div>
    </div>
    <ul class="sidebar__list">
      
        <li class="sidebar__list-item">
          <a
            href="https://www.linkedin.com/in/steven-rashin/"
            target="_blank"
            rel="noopener me"
            aria-label="Linkedin"
            title="Linkedin"
          >
            <i class="fab fa-linkedin fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="https://github.com/sdr1"
            target="_blank"
            rel="noopener me"
            aria-label="GitHub"
            title="GitHub"
          >
            <i class="fab fa-github fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
        <li class="sidebar__list-item">
          <a
            href="mailto:srashin@gmail.com"
            target="_blank"
            rel="noopener me"
            aria-label="e-mail"
            title="e-mail"
          >
            <i class="fas fa-envelope fa-2x" aria-hidden="true"></i>
          </a>
        </li>
      
    </ul>
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Steven Rashin
        2024
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script></div>
</aside>
      <main
        
          class="wrapper__main"
        
      >
        <header class="header"><div
  class="
    animated fadeInDown
  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/"
              
              title=""
              >Bio</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/research/"
              
              title=""
              >Research</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/teaching/"
              
              title=""
              >Teaching</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/software/"
              
              title=""
              >Software</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/post/"
              
              title=""
              >Posts/Tutorials</a
            >
          </li>
        

      
        
        
          <li class="nav__list-item">
            
            <a
              
              href="/contact/"
              
              title=""
              >Contact</a
            >
          </li>
        

      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>
      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown
    "
  >
    
    <div class="post__content">
      <h1>Regression</h1>
      
        <ul class="post__meta">
          <li class="post__meta-item">
            <em class="fas fa-calendar-day post__meta-icon"></em>
            <span class="post__meta-text"
              >
                
                  1/1/0001
                

              
            </span>
          </li>
          <li class="post__meta-item">
            <em class="fas fa-stopwatch post__meta-icon"></em>
            <span class="post__meta-text">77-minute read</span>
          </li>
        </ul>
      <script src="OLS-website-version_files/libs/kePrint-0.0.1/kePrint.js"></script>
<link href="OLS-website-version_files/libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="OLS-website-version_files/libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="OLS-website-version_files/libs/bsTable-3.3.7/bootstrapTable.js"></script>
<h1 id="tl-dr-checklist">TL; DR: Checklist</h1>
<ol>
<li>
<p>Get summary stats for the variables. Check for outliers or weird patterns</p>
</li>
<li>
<p>Plot the independent variables against the dependent variable. Same goal as above.</p>
</li>
<li>
<p>Figure out the model and standard errors - you most likely want robust or cluster robust standard errors</p>
</li>
<li>
<p>Run the regression. If you have over 5 fixed effects (FEs), you can use a couple of approaches - if you don&rsquo;t care about the FEs themselves, demean the dependent and independent variable of interest by group (i.e. <img alt="y_{\\text{demeaned}}=\\alpha + \\beta_1 x_\\text{1 demeaned} + \\beta_2 x_\\text{2 demeaned}" src="https://latex.codecogs.com/svg.latex?y_%7B%5Ctext%7Bdemeaned%7D%7D%3D%5Calpha%20%2B%20%5Cbeta_1%20x_%5Ctext%7B1%20demeaned%7D%20%2B%20%5Cbeta_2%20x_%5Ctext%7B2%20demeaned%7D" title="y_{\\text{demeaned}}=\\alpha + \\beta_1 x_\\text{1 demeaned} + \\beta_2 x_\\text{2 demeaned}">) , or use plm or some other package</p>
</li>
<li>
<p>Check regression diagnostics</p>
<ul>
<li>Residuals vs Fitted - checks linear relationship assumption of linear regression. A linear relationship will demonstrate a horizontal red line here. Deviations from a horizontal line suggest nonlinearity and that a different approach may be necessary.</li>
<li>Normal Q-Q - checks whether or not the residuals (the difference between the observed and predicted values) from the model are normally distributed. The best fit models points fall along the dashed line on the plot. Deviation from this line suggests that a different analytical approach may be required.</li>
<li>Scale-Location - checks the homoscedasticity of the model. A horizontal red line with points equally spread out indicates a well-fit model. A non-horizontal line or points that cluster together suggests that your data are not homoscedastic.</li>
<li>Residuals vs Leverage - helps to identify outlier or extreme values that may disproportionately affect the model&rsquo;s results. Their inclusion or exclusion from the analysis may affect the results of the analysis. Note that the top three most extreme values are identified with numbers next to the points in all four plots. You can also do this using the <a href="/post/ols-website-version/#the-hat-matrix">The Hat Matrix!</a></li>
<li>Can also use <a href="https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html">https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html</a></li>
</ul>
</li>
<li>
<p>Interpret the coefficients (see <a href="/post/ols-website-version/#sec-interpret">Section 12</a>)</p>
<ul>
<li>No logged DV, no logged IV
<ul>
<li>A one unit increase in x, increases y by coefficient (<img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta">) units</li>
</ul>
</li>
<li>No logged DV, logged IV
<ul>
<li>Divide the coefficient by 100, this tells us that a 1% increase in the independent variable increases (decreases) the dependent variable by coefficient/100 units . For an 10% increase multiply the coefficient by log(1.1) (for x increase log(1.x)). Suppose our coefficient is 5. The interpretation is that a one percent increase in x increases the dependent variable by 0.05. For every 10% increase in the independent variable, the dependent variable increases by 5 * log(1.1) = 0.47</li>
</ul>
</li>
<li>Logged DV, No logged IV
<ul>
<li>Exponentiate the coefficient, subtract one from this number, and multiply by 100. This gives the percent increase (or decrease) in the y for every one-unit increase in the independent variable. exp(coef) &ndash; 1) * 100. Suppose your coefficient is 0.5, (exp(0.5)-1)*100 = 64.9, a one unit increase in your independent variable increases your dependent variably by 64.9%. If our coefficient were 5, the result would be 14,741.32%. This is unlikely!</li>
</ul>
</li>
<li>Logged DV, logged IV
<ul>
<li>Interpret the coefficient as the percent increase in the dependent variable for every 1% increase in the independent variable. Suppose the coefficient is, again, 5. For every 1% increase in the independent variable, the dependent variable increases by 5%. Suppose, instead, we wanted an x percent increase - use the formula <img alt="(1.x^5 &ndash; 1) * 100" src="https://latex.codecogs.com/svg.latex?%281.x%5E5%20%E2%80%93%201%29%20%2A%20100" title="(1.x^5 – 1) * 100">. For example, a 10% increase in our independent variable increases our dependent variable by <img alt="(1.1^5 - 1) * 100 = 61.05\\" src="https://latex.codecogs.com/svg.latex?%281.1%5E5%20-%201%29%20%2A%20100%20%3D%2061.05%5C%25" title="(1.1^5 - 1) * 100 = 61.05\\%"></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Put regression into table form</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">ggfortify</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">gridExtra</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># simulate some data:</span>
</span></span><span class="line"><span class="cl"><span class="n">x1</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">15</span><span class="p">,</span> <span class="m">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x2</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">30</span><span class="p">,</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#group &lt;- ifelse(x1&gt;50,1,0)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">&lt;-</span> <span class="m">5</span> <span class="o">+</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">+</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sim_data</span> <span class="o">&lt;-</span> <span class="nf">tibble</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">x2</span> <span class="o">=</span> <span class="n">x2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># summaries of variables</span>
</span></span><span class="line"><span class="cl"><span class="n">sim_data</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="n">skimr</span><span class="o">::</span><span class="nf">skim</span><span class="p">()</span>
</span></span></code></pre></div><table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Name</td>
<td style="text-align:left">Piped data</td>
</tr>
<tr>
<td style="text-align:left">Number of rows</td>
<td style="text-align:left">1000</td>
</tr>
<tr>
<td style="text-align:left">Number of columns</td>
<td style="text-align:left">3</td>
</tr>
<tr>
<td style="text-align:left">_______________________</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">Column type frequency:</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">numeric</td>
<td style="text-align:left">3</td>
</tr>
<tr>
<td style="text-align:left">________________________</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">Group variables</td>
<td style="text-align:left">None</td>
</tr>
</tbody>
</table>
<p>Data summary</p>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">skim_variable</th>
<th style="text-align:right">n_missing</th>
<th style="text-align:right">complete_rate</th>
<th style="text-align:right">mean</th>
<th style="text-align:right">sd</th>
<th style="text-align:right">p0</th>
<th style="text-align:right">p25</th>
<th style="text-align:right">p50</th>
<th style="text-align:right">p75</th>
<th style="text-align:right">p100</th>
<th style="text-align:left">hist</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">x1</td>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">15.01</td>
<td style="text-align:right">5.08</td>
<td style="text-align:right">0.51</td>
<td style="text-align:right">11.42</td>
<td style="text-align:right">14.82</td>
<td style="text-align:right">18.37</td>
<td style="text-align:right">29.96</td>
<td style="text-align:left">▁▅▇▅▁</td>
</tr>
<tr>
<td style="text-align:left">x2</td>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">30.26</td>
<td style="text-align:right">10.32</td>
<td style="text-align:right">0.46</td>
<td style="text-align:right">23.20</td>
<td style="text-align:right">30.52</td>
<td style="text-align:right">37.27</td>
<td style="text-align:right">64.39</td>
<td style="text-align:left">▁▅▇▃▁</td>
</tr>
<tr>
<td style="text-align:left">y</td>
<td style="text-align:right">0</td>
<td style="text-align:right">1</td>
<td style="text-align:right">50.56</td>
<td style="text-align:right">14.77</td>
<td style="text-align:right">-3.03</td>
<td style="text-align:right">40.78</td>
<td style="text-align:right">50.87</td>
<td style="text-align:right">59.99</td>
<td style="text-align:right">95.68</td>
<td style="text-align:left">▁▂▇▅▁</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># plot variables against DV to check for weirdness</span>
</span></span><span class="line"><span class="cl"><span class="n">sim_data</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">gather</span><span class="p">(</span><span class="n">key</span> <span class="o">=</span> <span class="s">&#34;other_variable&#34;</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="s">&#34;value&#34;</span><span class="p">,</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">ggplot</span><span class="p">(</span><span class="n">.,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">value</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">))</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">facet_wrap</span><span class="p">(</span><span class="o">~</span><span class="n">other_variable</span><span class="p">)</span><span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">scale_color_viridis_d</span><span class="p">()</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">labs</span><span class="p">(</span><span class="n">caption</span> <span class="o">=</span> <span class="s">&#34;Check that each x is linearly related to the y&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># model, can also do cluster robust</span>
</span></span><span class="line"><span class="cl"><span class="n">std</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">y</span> <span class="o">~</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">robust</span> <span class="o">&lt;-</span> <span class="n">estimatr</span><span class="o">::</span><span class="nf">lm_robust</span><span class="p">(</span><span class="n">y</span> <span class="o">~</span> <span class="n">x1</span><span class="o">+</span><span class="n">x2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Output the diagnostic plots.  Look for weirdness (won&#39;t find it here because we&#39;re in a nice simualted sandbox)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">residvfitted</span> <span class="o">&lt;-</span> <span class="n">ggplot2</span><span class="o">::</span><span class="nf">autoplot</span><span class="p">(</span><span class="n">std</span><span class="p">,</span> <span class="n">which</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">))</span> <span class="o">+</span> <span class="nf">labs</span><span class="p">(</span><span class="n">caption</span> <span class="o">=</span> <span class="s">&#34;Residuals vs Fitted - checks linear relationship assumption 
</span></span></span><span class="line"><span class="cl"><span class="s">        of linear regression. A linear relationship will demonstrate a
</span></span></span><span class="line"><span class="cl"><span class="s">        horizontal red line here. Deviations from a horizontal line
</span></span></span><span class="line"><span class="cl"><span class="s">        suggest nonlinearity and that a different approach may be
</span></span></span><span class="line"><span class="cl"><span class="s">        necessary.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">qq</span> <span class="o">&lt;-</span> <span class="n">ggplot2</span><span class="o">::</span><span class="nf">autoplot</span><span class="p">(</span><span class="n">std</span><span class="p">,</span> <span class="n">which</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">))</span> <span class="o">+</span> <span class="nf">labs</span><span class="p">(</span><span class="n">caption</span> <span class="o">=</span> <span class="s">&#34;Normal Q-Q - checks whether or not the residuals (the difference
</span></span></span><span class="line"><span class="cl"><span class="s">        between the observed and predicted values) from the model are
</span></span></span><span class="line"><span class="cl"><span class="s">        normally distributed. The best fit models points fall along the
</span></span></span><span class="line"><span class="cl"><span class="s">        dashed line on the plot. Deviation from this line suggests that
</span></span></span><span class="line"><span class="cl"><span class="s">        a different analytical approach may be required.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">scalevlocation</span> <span class="o">&lt;-</span>   <span class="n">ggplot2</span><span class="o">::</span><span class="nf">autoplot</span><span class="p">(</span><span class="n">std</span><span class="p">,</span> <span class="n">which</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">))</span> <span class="o">+</span> <span class="nf">labs</span><span class="p">(</span><span class="n">caption</span> <span class="o">=</span> <span class="s">&#34; Scale-Location - checks the homoscedasticity of the model. A
</span></span></span><span class="line"><span class="cl"><span class="s">        horizontal red line with points equally spread out indicates a
</span></span></span><span class="line"><span class="cl"><span class="s">        well-fit model. A non-horizontal line or points that cluster
</span></span></span><span class="line"><span class="cl"><span class="s">        together suggests that your data are not homoscedastic.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="n">residvleeverage</span> <span class="o">&lt;-</span> <span class="n">ggplot2</span><span class="o">::</span><span class="nf">autoplot</span><span class="p">(</span><span class="n">std</span><span class="p">,</span> <span class="n">which</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">))</span> <span class="o">+</span> <span class="nf">labs</span><span class="p">(</span><span class="n">caption</span> <span class="o">=</span> <span class="s">&#34;Residuals vs Leverage - helps to identify outlier or extreme
</span></span></span><span class="line"><span class="cl"><span class="s">        values that may disproportionately affect the model&#39;s results.
</span></span></span><span class="line"><span class="cl"><span class="s">        Their inclusion or exclusion from the analysis may affect the
</span></span></span><span class="line"><span class="cl"><span class="s">        results of the analysis. Note that the top three most extreme
</span></span></span><span class="line"><span class="cl"><span class="s">        values are identified with numbers next to the points in all
</span></span></span><span class="line"><span class="cl"><span class="s">        four plots. You can also do this using the [The Hat Matrix!]&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl"><span class="n">residvfitted</span> <span class="o">+</span> <span class="n">qq</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">scalevlocation</span> <span class="o">+</span> <span class="n">residvleeverage</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Lots of diagnostic options, see https://declaredesign.org/r/estimatr/articles/estimatr-in-the-tidyverse.html</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Model Summary for the output</span>
</span></span><span class="line"><span class="cl"><span class="n">modelsummary</span><span class="o">::</span><span class="nf">modelsummary</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
</span></span></code></pre></div><table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center"> (1)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">(Intercept)</td>
<td style="text-align:center">4.746</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(1.299)</td>
</tr>
<tr>
<td style="text-align:left">x1</td>
<td style="text-align:center">1.130</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(0.059)</td>
</tr>
<tr>
<td style="text-align:left">x2</td>
<td style="text-align:center">0.953</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(0.029)</td>
</tr>
<tr>
<td style="text-align:left">Num.Obs.</td>
<td style="text-align:center">1000</td>
</tr>
<tr>
<td style="text-align:left">R2</td>
<td style="text-align:center">0.586</td>
</tr>
<tr>
<td style="text-align:left">R2 Adj.</td>
<td style="text-align:center">0.585</td>
</tr>
<tr>
<td style="text-align:left">AIC</td>
<td style="text-align:center">7348.1</td>
</tr>
<tr>
<td style="text-align:left">BIC</td>
<td style="text-align:center">7367.8</td>
</tr>
<tr>
<td style="text-align:left">Log.Lik.</td>
<td style="text-align:center">−3670.060</td>
</tr>
<tr>
<td style="text-align:left">F</td>
<td style="text-align:center">705.912</td>
</tr>
<tr>
<td style="text-align:left">RMSE</td>
<td style="text-align:center">9.50</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">modelsummary</span><span class="o">::</span><span class="nf">modelsummary</span><span class="p">(</span><span class="n">robust</span><span class="p">)</span>
</span></span></code></pre></div><table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center"> (1)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">(Intercept)</td>
<td style="text-align:center">4.746</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(1.370)</td>
</tr>
<tr>
<td style="text-align:left">x1</td>
<td style="text-align:center">1.130</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(0.056)</td>
</tr>
<tr>
<td style="text-align:left">x2</td>
<td style="text-align:center">0.953</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(0.031)</td>
</tr>
<tr>
<td style="text-align:left">Num.Obs.</td>
<td style="text-align:center">1000</td>
</tr>
<tr>
<td style="text-align:left">R2</td>
<td style="text-align:center">0.586</td>
</tr>
<tr>
<td style="text-align:left">R2 Adj.</td>
<td style="text-align:center">0.585</td>
</tr>
<tr>
<td style="text-align:left">AIC</td>
<td style="text-align:center">7348.1</td>
</tr>
<tr>
<td style="text-align:left">BIC</td>
<td style="text-align:center">7367.8</td>
</tr>
<tr>
<td style="text-align:left">RMSE</td>
<td style="text-align:center">9.50</td>
</tr>
</tbody>
</table>
<img src="figs/Checklist%20in%20code-1.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<img src="figs/Checklist%20in%20code-2.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<img src="figs/Checklist%20in%20code-3.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<h2 id="resources">Resources</h2>
<p>These are the sources I used to create this!</p>
<h3 id="less-math-heavy-or-math-heavy-with-intuitive-explanations">Less Math Heavy or Math Heavy with Intuitive Explanations</h3>
<ul>
<li>A visual introduction to probability <a href="https://seeing-theory.brown.edu/">https://seeing-theory.brown.edu/</a></li>
<li>Conditional probability visual guide <a href="https://setosa.io/conditional/">https://setosa.io/conditional/</a></li>
<li>Biltzstein and Hwang&rsquo;s probability textbook <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view">https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view</a></li>
<li>Ethan Bueno de Mesquita and Anthony Fowler&rsquo;s Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis</li>
<li>All Models are Wrong (a good visual guide to OLS) <a href="https://allmodelsarewrong.github.io/">https://allmodelsarewrong.github.io/</a></li>
<li>Section 5.7 of <a href="https://jhudatascience.org/tidyversecourse/model.html#model-diagnostics">https://jhudatascience.org/tidyversecourse/model.html#model-diagnostics</a></li>
<li>The bias-variance tradeoff <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></li>
</ul>
<h3 id="bring-on-the-pain-aka-math">Bring on the Pain (aka Math)</h3>
<ul>
<li>A guide to OLS via matrices <a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf">https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf</a></li>
<li>Brandon Stewart&rsquo;s class on applied social science stats - this is the best econometrics class I&rsquo;ve seen from a slide perspective <a href="https://bstewart.scholar.princeton.edu/soc500">https://bstewart.scholar.princeton.edu/soc500</a></li>
<li>Mauricio Romero&rsquo;s lecture on Ordinary Least Squares <a href="https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf">https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf</a></li>
<li><a href="http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf">http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf</a></li>
<li>Angrist and Pischke&rsquo;s Mostly Harmless Econometrics</li>
<li>Notes on MHE with a good derivation of the law of iterated expectations <a href="https://www.leonardgoff.com/resources/mostlyharmlesslecturenotes.pdf">https://www.leonardgoff.com/resources/mostlyharmlesslecturenotes.pdf</a></li>
</ul>
<h1 id="preliminaries">Preliminaries</h1>
<p>There are a lot of ways to implement Ordinary Least Squares (Before we do, note that ordinary means that all observations are treated equally. If we don&rsquo;t want to do that, we can use weighted least squares or WLS.)</p>
<p>This guide covers OLS via:</p>
<ul>
<li>Linear Algebra</li>
<li>Method of Moments</li>
<li>Maximum Likelihood</li>
<li>Gradient Descent (See Gradient Descent Guide for more details)</li>
</ul>
<p>Remember: OLS is an estimator - it&rsquo;s a machine that we plug data into and we get out estimates. As long as there is variation in our x, we get estimates (regardless of whether they make sense!).</p>
<p>We use it because we care about the relationship between an outcome, an independent variable of interest, while controlling for various factors. That is, the correlation cov(x,y) / <img alt="\\sigma_x \\sigma_y" src="https://latex.codecogs.com/svg.latex?%5Csigma_x%20%5Csigma_y" title="\\sigma_x \\sigma_y"> doesn&rsquo;t capture the relationship we care about because there may be confounding (i.e. a common cause).</p>
<h1 id="our-goalsterms">Our Goals/Terms</h1>
<h2 id="goals">Goals</h2>
<p>There are a few different ways to describe the goals of social science research:</p>
<ol>
<li>
<p>Our goal is to learn about the data generating process that generated the sample (Brandon Stewart). Another way DGP are also described as real world process(es) that creates/generates the data we&rsquo;re interested in. In other words, the DGP describes the rules that created variation in the population itself and in our data. DGPs are usually unknown, unless you&rsquo;re simulating.</p>
</li>
<li>
<p>The goal of quantitative social science is not limited to uncovering causally identified facts; the goal is to harness (many) pieces of evidence to obtain an inference to the best explanation&mdash;both within and across studies. (Stewart and Spirling)</p>
</li>
<li>
<p>The goal of statistical inference is to learn about the unobserved population distribution, which can be characterized by parameters. We want to estimate these population parameters. (What is a parameter? It&rsquo;s a particular aspect of a population distribution)</p>
</li>
</ol>
<p>Note that sometimes the unobserved population distribution and the DGP are sometimes considered the same. Matt Blackwell and Brandon Stewart, for example, sometimes call the population distribution the data generating process (DGP).</p>
<h2 id="terms">Terms</h2>
<p>Estimands are the parameters that we aim to estimate (in other words &ldquo;the estimand is the object of inquiry&mdash;it is the precise quantity about which we marshal data to draw an inference&rdquo;). These are often written in Greek letters. Think of these as the truth. They are the result of the true data generating process.</p>
<p>Estimators are functions of sample data (i.e. statistics/rules) which we use to learn about the estimands. They are often written using modified Greek letters (e.g., <img alt="\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D" title="\\hat{\\beta}">). These are procedures (e.g. means, variances, OLS). In concrete terms, Ordinary Least Squares is an estimator of some estimand, which was created by some data generating process (DGP).</p>
<p>Estimates are particular values of estimators realized in a given sample. They are denoted by English letters (e.g., X, <img alt="x_i" src="https://latex.codecogs.com/svg.latex?x_i" title="x_i">) and are data from our sample. For example, the mean wealth of a person who completes college may be an estimate we care about.</p>
<p>Note that, theoretically, estimate = estimand + bias + noise (see e.g. Fowler and Bueno de Mesquita&rsquo;s book). Where bias is a systematic error and noise is an idiosyncratic one that is particular to any observation. So our population estimate is a function of the true estimand, systematic bias, and irreducible noise. Our goal is for the bias to be as close to 0 as possible. This, however, is hard!</p>
<h3 id="dgp-rightarrowhttpslatexcodecogscomsvglatex5crightarrow-rightarrow-sample-rightarrowhttpslatexcodecogscomsvglatex5crightarrow-rightarrow-dgp">DGP <img alt="\\rightarrow" src="https://latex.codecogs.com/svg.latex?%5Crightarrow" title="\\rightarrow"> Sample <img alt="\\rightarrow" src="https://latex.codecogs.com/svg.latex?%5Crightarrow" title="\\rightarrow"> DGP</h3>
<p>Putting this together:</p>
<ol>
<li>
<p>(Unobserved) There is a data-generating process that creates the data/parameters we care about. E.g., some people get cancer because of a mutation, candidates for office spend money strategically to win elections, lobbyists argue that policy should change, etc&hellip; These processes create data that we&rsquo;re interested in such as cancer rates/election spending/lobbying results</p>
</li>
<li>
<p>We decide to care about a population parameter - the estimand - the true effect of an intervention (e.g. the true effect of a cancer drug on mortality or the true effect of spending on campaign outcomes). ``Each theoretical estimand is linked to an empirical estimand involving only observable quantities (e.g. a difference in means in a population) by assumptions about the relationship between the data we observe and the data we do not&hellip; The distinction between the theoretical and empirical estimands is subtle but important: the former may involve unobservable quantities such as counterfactuals while the latter involves only observable data.&quot;</p>
</li>
<li>
<p>We then gather a sample of the population (or, maybe the whole population itself but then we say this population is a realization of a super-population), and measure the effect of this intervention on that sample. Say gather 1000 cancer patients or all congressional elections from 2000 - 2022.</p>
</li>
<li>
<p>Once we have sample, we select an estimator (e.g. mean, regression, etc&hellip;), which we use to learn about the estimand. Particular values of the estimator are estimates (e.g., population mean).</p>
</li>
<li>
<p>If we&rsquo;ve done this well, we&rsquo;re able to use our sample to talk about the population.</p>
</li>
</ol>
<p>So we have: data <img alt="\\rightarrow" src="https://latex.codecogs.com/svg.latex?%5Crightarrow" title="\\rightarrow"> calculation <img alt="\\rightarrow" src="https://latex.codecogs.com/svg.latex?%5Crightarrow" title="\\rightarrow"> estimate <img alt="\\rightarrow" src="https://latex.codecogs.com/svg.latex?%5Crightarrow" title="\\rightarrow"> (maybe) truth</p>
<p>Sources:</p>
<ul>
<li>
<p><a href="https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture3handout.pdf">https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture3handout.pdf</a></p>
</li>
<li>
<p><a href="https://twitter.com/nickchk/status/1272993322395557888">https://twitter.com/nickchk/status/1272993322395557888</a></p>
</li>
<li>
<p><a href="https://jamanetwork.com/journals/jama/fullarticle/2783611">https://jamanetwork.com/journals/jama/fullarticle/2783611</a></p>
</li>
<li>
<p><a href="https://doi.org/10.1177%2F00031224211004187">https://doi.org/10.1177%2F00031224211004187</a></p>
</li>
<li>
<p>Fowler and Bueno de Mesquita Book</p>
</li>
<li>
<p>What Good is a Regression? paper by Spirling and Stweart</p>
</li>
</ul>
<h2 id="ols-big-picture">OLS Big Picture</h2>
<ol>
<li>
<p>Regression provides the best linear predictor for the dependent variable in the same way that the conditional expectation function (CEF) is the best unrestricted predictor of the dependent variable</p>
</li>
<li>
<p>If we prefer to think of approximating <img alt="E(y_i|x_i)" src="https://latex.codecogs.com/svg.latex?E%28y_i%7Cx_i%29" title="E(y_i|x_i)"> as opposed to predicting <img alt="y_i" src="https://latex.codecogs.com/svg.latex?y_i" title="y_i">, even if the CEF is nonlinear, regression provides the best linear approximation to it (Angrist and Pischke)</p>
</li>
<li>
<p>If the CEF is linear (i.e. if the process that produces the population distribution is linear), then it makes the most sense to use linear regression to estimate it. Restated, the population regression function is the best we can do in the class of all linear functions to approximate <img alt="E(y_i|x_i)" src="https://latex.codecogs.com/svg.latex?E%28y_i%7Cx_i%29" title="E(y_i|x_i)"> (Angrist and Pischke)</p>
</li>
<li>
<p>Linear regression may be interesting even if the underlying CEF is not linear. <img alt="E(y_i|x_i)" src="https://latex.codecogs.com/svg.latex?E%28y_i%7Cx_i%29" title="E(y_i|x_i)">, is the minimum mean squared error predictor of <img alt="y_i" src="https://latex.codecogs.com/svg.latex?y_i" title="y_i"> given <img alt="x_i" src="https://latex.codecogs.com/svg.latex?x_i" title="x_i"> in the class of all functions of <img alt="x_i" src="https://latex.codecogs.com/svg.latex?x_i" title="x_i"> (Angrist and Pischke)</p>
</li>
<li>
<p><img alt="\\beta_{OLS}" src="https://latex.codecogs.com/svg.latex?%5Cbeta_%7BOLS%7D" title="\\beta_{OLS}"> is an estimator of a parameter we do not observe</p>
</li>
<li>
<p>The standard error is the standard deviation of the estimator</p>
</li>
<li>
<p>A confidence interval is an interval where we know with some probability the true estimate lives</p>
</li>
<li>
<p>A p-value is the largest probability of obtaining results at least as extreme as those actually observed, under the assumption that the null hypothesis is correct.</p>
</li>
<li>
<p>Regression anatomy helps us understand OLS as a &ldquo;matching estimator&rdquo; (try to compare observations that are alike in the Xs). (Note that this comes from MHE&rsquo;s regression anatomy theorem. Suppose you have a multivariate regression <img alt="Y_i = \\beta_0 +\\beta_1X_{1i}+\\beta_2X_{2i}+\\epsilon_i" src="https://latex.codecogs.com/svg.latex?Y_i%20%3D%20%5Cbeta_0%20%2B%5Cbeta_1X_%7B1i%7D%2B%5Cbeta_2X_%7B2i%7D%2B%5Cepsilon_i" title="Y_i = \\beta_0 +\\beta_1X_{1i}+\\beta_2X_{2i}+\\epsilon_i"> and two auxillary regressions of <img alt="X_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7B1i%7D" title="X_{1i}"> on <img alt="X_{2i}" src="https://latex.codecogs.com/svg.latex?X_%7B2i%7D" title="X_{2i}">, and vice versa. The regression anatomy theorem states that <img alt="\\beta_1" src="https://latex.codecogs.com/svg.latex?%5Cbeta_1" title="\\beta_1"> captures the effect of <img alt="\\tilde{x}_{1i}" src="https://latex.codecogs.com/svg.latex?%5Ctilde%7Bx%7D_%7B1i%7D" title="\\tilde{x}_{1i}"> (the residuals from the regression of <img alt="X_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7B1i%7D" title="X_{1i}"> on <img alt="X_{2i}" src="https://latex.codecogs.com/svg.latex?X_%7B2i%7D" title="X_{2i}">) - the part of <img alt="X_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7B1i%7D" title="X_{1i}"> not explained by <img alt="X_2" src="https://latex.codecogs.com/svg.latex?X_2" title="X_2">.). Similarly <img alt="\\beta_2" src="https://latex.codecogs.com/svg.latex?%5Cbeta_2" title="\\beta_2"> captures the effect of <img alt="\\tilde{x}_{2i}" src="https://latex.codecogs.com/svg.latex?%5Ctilde%7Bx%7D_%7B2i%7D" title="\\tilde{x}_{2i}"> (the residuals from the regression of <img alt="X_{2i}" src="https://latex.codecogs.com/svg.latex?X_%7B2i%7D" title="X_{2i}"> on <img alt="X_{ii}" src="https://latex.codecogs.com/svg.latex?X_%7Bii%7D" title="X_{ii}">) - the part of <img alt="X_{2i}" src="https://latex.codecogs.com/svg.latex?X_%7B2i%7D" title="X_{2i}"> not explained by <img alt="X_1" src="https://latex.codecogs.com/svg.latex?X_1" title="X_1">.)</p>
</li>
</ol>
<p>Source</p>
<ul>
<li><a href="https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf">https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf</a> - <a href="http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf">http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf</a></li>
</ul>
<h1 id="is-my-regression-causal">Is My Regression Causal?</h1>
<p>Short answer: unless you have some exogenous variation or randomization, the answer is no.</p>
<p>Sekhon: &ldquo;Without an experiment, a natural experiment, a discontinuity, or some other strong design, no amount of econometric or statistical modeling can make the move from correlation to causation persuasive. (Sekhon, 2009, p. 503)</p>
<p>Angrist and Pischke: Regression is causal when the corresponding conditional expectation function (CEF) is causal. If, for example <img alt="Y_i" src="https://latex.codecogs.com/svg.latex?Y_i" title="Y_i"> is fall grades and <img alt="D_i" src="https://latex.codecogs.com/svg.latex?D_i" title="D_i"> is a treatment dummy indicating students receiving randomized GPA incentives, then <img alt="E[Y_i|D_i,Wi]" src="https://latex.codecogs.com/svg.latex?E%5BY_i%7CD_i%2CWi%5D" title="E[Y_i|D_i,Wi]"> has a causal interpretation, revealing differences in average potential GPAs indexed by <img alt="D_i" src="https://latex.codecogs.com/svg.latex?D_i" title="D_i">, conditional on control variables, <img alt="W_i" src="https://latex.codecogs.com/svg.latex?W_i" title="W_i">. The regression of <img alt="Y_i" src="https://latex.codecogs.com/svg.latex?Y_i" title="Y_i"> on <img alt="D_i" src="https://latex.codecogs.com/svg.latex?D_i" title="D_i"> and <img alt="W_i" src="https://latex.codecogs.com/svg.latex?W_i" title="W_i"> inherits this CEFs causal interpretation</p>
<p>At the end of the day, OLS (and other matching/weighting estimators) &ldquo;mop up&rdquo; imbalances that makes CIA plausible</p>
<p>Thought experiment necessary to test CIA:</p>
<ul>
<li>
<p>How could it be that two units that are identical with respect to all meaningful background factors nonetheless receive different treatment?</p>
</li>
<li>
<p>Your answer to this question is your source of identification</p>
</li>
</ul>
<p>Sources</p>
<ul>
<li>
<p><a href="https://www.annualreviews.org/doi/full/10.1146/annurev.polisci.11.060606.135444">https://www.annualreviews.org/doi/full/10.1146/annurev.polisci.11.060606.135444</a></p>
</li>
<li>
<p><a href="http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf">http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n07MRU_R1.pdf</a></p>
</li>
<li>
<p><a href="https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf">https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf</a></p>
</li>
</ul>
<h1 id="deriving-ols">Deriving OLS</h1>
<h2 id="method-of-moments">Method of Moments</h2>
<p>This is the method used in Scott Cunningham&rsquo;s Causal Inference Mixtape and Angrist and Pischke&rsquo;s Mostly Harmless Econometrics.</p>
<p>Think of the population regression as a moment of the population distribution. This proceeds in two steps:</p>
<ol>
<li>
<p>First we derive an estimator for the population regression coefficient</p>
</li>
<li>
<p>Then we replace it with the sample analog.</p>
</li>
</ol>
<p>Let&rsquo;s assume that the data generating process in the sky follows the following form:</p>
<p><img alt="y_i = \\beta_0 + \\beta_1x_i + u_i" src="https://latex.codecogs.com/svg.latex?y_i%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1x_i%20%2B%20u_i" title="y_i = \\beta_0 + \\beta_1x_i + u_i"></p>
<p>That is, every y is a function of an intercept <img alt="\\beta_0" src="https://latex.codecogs.com/svg.latex?%5Cbeta_0" title="\\beta_0">, a coefficient <img alt="\\beta_1" src="https://latex.codecogs.com/svg.latex?%5Cbeta_1" title="\\beta_1"> on <img alt="x_i" src="https://latex.codecogs.com/svg.latex?x_i" title="x_i"> and a random error <img alt="u_i" src="https://latex.codecogs.com/svg.latex?u_i" title="u_i">. To make it concrete, assume that your earnings at age 40 are only systematically determined by your education and some random error <img alt="u_i" src="https://latex.codecogs.com/svg.latex?u_i" title="u_i">. So nothing else matters systematically. That is, majoring in electrical engineering has the exact same returns as majoring in underwater basket weaving if they both take four years to complete. Except for a random error (e.g., sometimes electrical engineers decide to live as artists). This is a very strong assumption because we have to believe that everyone&rsquo;s ability is the same.</p>
<p>How do we go from the above to a regression?</p>
<p>We need two assumptions, laid out below:</p>
<ol>
<li>
<p>We need to assume <img alt="E(u_i)=0" src="https://latex.codecogs.com/svg.latex?E%28u_i%29%3D0" title="E(u_i)=0">. That is, the expected value of any error term is 0. This doesn&rsquo;t mean that every error term is 0 for each i. Quite the contrary! Some errors are positive, some are negative, but the mean of the distribution is 0.</p>
</li>
<li>
<p>We also need to assume <img alt="E(u_i|x_i)=E(u_i)" src="https://latex.codecogs.com/svg.latex?E%28u_i%7Cx_i%29%3DE%28u_i%29" title="E(u_i|x_i)=E(u_i)"> This assumption - mean independence - that states that the disturbances average out to 0 for any value of X. Put differently, no observations of the independent variables convey any information about the expected value of the disturbance.</p>
<ul>
<li>
<p>This implies that: <img alt="E(u_i | x_i)=E(u_i)=0" src="https://latex.codecogs.com/svg.latex?E%28u_i%20%7C%20x_i%29%3DE%28u_i%29%3D0" title="E(u_i | x_i)=E(u_i)=0"> and <img alt="E(u_ix_i)=E(E(u_i|x_i))=0" src="https://latex.codecogs.com/svg.latex?E%28u_ix_i%29%3DE%28E%28u_i%7Cx_i%29%29%3D0" title="E(u_ix_i)=E(E(u_i|x_i))=0">. From these we can derive the population regression coefficients.</p>
</li>
<li>
<p>At a population level:</p>
</li>
</ul>
</li>
</ol>
<p><img alt="E[Y|X] = \\beta_0 + \\beta_1x_i" src="https://latex.codecogs.com/svg.latex?E%5BY%7CX%5D%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1x_i" title="E[Y|X] = \\beta_0 + \\beta_1x_i"></p>
<p><img alt="E[Y|X]" src="https://latex.codecogs.com/svg.latex?E%5BY%7CX%5D" title="E[Y|X]"> is a population level regression function or conditional expectation function (i.e. mean). We solve for <img alt="\\beta_0" src="https://latex.codecogs.com/svg.latex?%5Cbeta_0" title="\\beta_0"> and <img alt="\\beta_1" src="https://latex.codecogs.com/svg.latex?%5Cbeta_1" title="\\beta_1"></p>
<p>Solving for <img alt="\\beta_0" src="https://latex.codecogs.com/svg.latex?%5Cbeta_0" title="\\beta_0"></p>
<p><img alt="E[u_i|x_i] = E[y_i-\\beta_0-\\beta_1x_i] = 0" src="https://latex.codecogs.com/svg.latex?E%5Bu_i%7Cx_i%5D%20%3D%20E%5By_i-%5Cbeta_0-%5Cbeta_1x_i%5D%20%3D%200" title="E[u_i|x_i] = E[y_i-\\beta_0-\\beta_1x_i] = 0"></p>
<p><img alt="E[y_i]-\\beta_0-E[\\beta_1x_i] = 0" src="https://latex.codecogs.com/svg.latex?E%5By_i%5D-%5Cbeta_0-E%5B%5Cbeta_1x_i%5D%20%3D%200" title="E[y_i]-\\beta_0-E[\\beta_1x_i] = 0"></p>
<p>Above, note that <img alt="\\beta_0" src="https://latex.codecogs.com/svg.latex?%5Cbeta_0" title="\\beta_0"> is a constant so <img alt="E[\\beta_0]=\\beta_0" src="https://latex.codecogs.com/svg.latex?E%5B%5Cbeta_0%5D%3D%5Cbeta_0" title="E[\\beta_0]=\\beta_0"></p>
<p><img alt="E[y_i]-\\beta_1 E[x_i] = \\beta_0" src="https://latex.codecogs.com/svg.latex?E%5By_i%5D-%5Cbeta_1%20E%5Bx_i%5D%20%3D%20%5Cbeta_0" title="E[y_i]-\\beta_1 E[x_i] = \\beta_0"></p>
<p>Now do <img alt="\\beta_1" src="https://latex.codecogs.com/svg.latex?%5Cbeta_1" title="\\beta_1">:</p>
<p><img alt="E[u_ix_i] = 0" src="https://latex.codecogs.com/svg.latex?E%5Bu_ix_i%5D%20%3D%200" title="E[u_ix_i] = 0"></p>
<p>Replace <img alt="u_i" src="https://latex.codecogs.com/svg.latex?u_i" title="u_i"> with <img alt="y-\\beta_0-\\beta_1x_i" src="https://latex.codecogs.com/svg.latex?y-%5Cbeta_0-%5Cbeta_1x_i" title="y-\\beta_0-\\beta_1x_i">. A note about this proof - be careful with expectations and with minus signs</p>
<p><img alt="E[(y_i-\\beta_0-\\beta_1x_i)x_i] = 0" src="https://latex.codecogs.com/svg.latex?E%5B%28y_i-%5Cbeta_0-%5Cbeta_1x_i%29x_i%5D%20%3D%200" title="E[(y_i-\\beta_0-\\beta_1x_i)x_i] = 0"></p>
<p>We note <img alt="x_i" src="https://latex.codecogs.com/svg.latex?x_i" title="x_i"> is a scaler, so we can move it to the front.</p>
<p><img alt="E [x_i(y_i-\\beta_0-\\beta_1 x_i)]= 0" src="https://latex.codecogs.com/svg.latex?E%20%5Bx_i%28y_i-%5Cbeta_0-%5Cbeta_1%20x_i%29%5D%3D%200" title="E [x_i(y_i-\\beta_0-\\beta_1 x_i)]= 0"></p>
<p>Now replace <img alt="\\beta_0" src="https://latex.codecogs.com/svg.latex?%5Cbeta_0" title="\\beta_0"> with what we derived above <img alt="E[y_i]-\\beta_1 E[x_i] = \\beta_0" src="https://latex.codecogs.com/svg.latex?E%5By_i%5D-%5Cbeta_1%20E%5Bx_i%5D%20%3D%20%5Cbeta_0" title="E[y_i]-\\beta_1 E[x_i] = \\beta_0"></p>
<p>Remember that since the term is <img alt="-\\beta_1" src="https://latex.codecogs.com/svg.latex?-%5Cbeta_1" title="-\\beta_1"> we need to distribute the minus!</p>
<p><img alt="E[x_i(y_i - E[y_i]+\\beta_1 E[x_i] - \\beta_1 x_i)] = 0" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%28y_i%20-%20E%5By_i%5D%2B%5Cbeta_1%20E%5Bx_i%5D%20-%20%5Cbeta_1%20x_i%29%5D%20%3D%200" title="E[x_i(y_i - E[y_i]+\\beta_1 E[x_i] - \\beta_1 x_i)] = 0"></p>
<p><img alt="E[x_i(y_i - E[y_i] - \\beta_1 x_i  + \\beta_1 E[x_i]) ] = 0" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%28y_i%20-%20E%5By_i%5D%20-%20%5Cbeta_1%20x_i%20%20%2B%20%5Cbeta_1%20E%5Bx_i%5D%29%20%5D%20%3D%200" title="E[x_i(y_i - E[y_i] - \\beta_1 x_i  + \\beta_1 E[x_i]) ] = 0"></p>
<p>Factor out <img alt="\\beta_1" src="https://latex.codecogs.com/svg.latex?%5Cbeta_1" title="\\beta_1">. Watch the minus sign on the <img alt="E[x_i]" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%5D" title="E[x_i]">!</p>
<p><img alt="E[x_i(y_i - E[y_i] - \\beta_1 (x_i  - E[x_i])) ] = 0" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%28y_i%20-%20E%5By_i%5D%20-%20%5Cbeta_1%20%28x_i%20%20-%20E%5Bx_i%5D%29%29%20%5D%20%3D%200" title="E[x_i(y_i - E[y_i] - \\beta_1 (x_i  - E[x_i])) ] = 0"></p>
<p><img alt="E[x_i(y_i - E[y_i])] = \\beta_1 E [x_i  (x_i  - E[x_i]) ]" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%28y_i%20-%20E%5By_i%5D%29%5D%20%3D%20%5Cbeta_1%20E%20%5Bx_i%20%20%28x_i%20%20-%20E%5Bx_i%5D%29%20%5D" title="E[x_i(y_i - E[y_i])] = \\beta_1 E [x_i  (x_i  - E[x_i]) ]"></p>
<p>On the right hand side, we note that <img alt="E[(x_i - E[x_i])^2] = E[x_i^2 - E[x]^2]" src="https://latex.codecogs.com/svg.latex?E%5B%28x_i%20-%20E%5Bx_i%5D%29%5E2%5D%20%3D%20E%5Bx_i%5E2%20-%20E%5Bx%5D%5E2%5D" title="E[(x_i - E[x_i])^2] = E[x_i^2 - E[x]^2]">. Given that this took me a few hours to figure out, here&rsquo;s the proof. Note that we start with <img alt="E[(x_i - E[x_i])^2] = E[x_i^2 - 2 x_i E[x_i]-E[x_i]^2]" src="https://latex.codecogs.com/svg.latex?E%5B%28x_i%20-%20E%5Bx_i%5D%29%5E2%5D%20%3D%20E%5Bx_i%5E2%20-%202%20x_i%20E%5Bx_i%5D-E%5Bx_i%5D%5E2%5D" title="E[(x_i - E[x_i])^2] = E[x_i^2 - 2 x_i E[x_i]-E[x_i]^2]">. Now we distribute the expectations to all the terms. <img alt="E[x_i^2] - E[2 x_i E[x_i]]+E[E[x_i]^2]]" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%5E2%5D%20-%20E%5B2%20x_i%20E%5Bx_i%5D%5D%2BE%5BE%5Bx_i%5D%5E2%5D%5D" title="E[x_i^2] - E[2 x_i E[x_i]]+E[E[x_i]^2]]">. Now clean up the expectations. <img alt="E[x_i^2] - 2 E[ x_i^2 ]+E[x_i]^2 = E[x_i^2]-E[x_i]^2 = E[(x_i-E[x_i])(x_i-E[x_i])]" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%5E2%5D%20-%202%20E%5B%20x_i%5E2%20%5D%2BE%5Bx_i%5D%5E2%20%3D%20E%5Bx_i%5E2%5D-E%5Bx_i%5D%5E2%20%3D%20E%5B%28x_i-E%5Bx_i%5D%29%28x_i-E%5Bx_i%5D%29%5D" title="E[x_i^2] - 2 E[ x_i^2 ]+E[x_i]^2 = E[x_i^2]-E[x_i]^2 = E[(x_i-E[x_i])(x_i-E[x_i])]"></p>
<p><img alt="E[x_i(y_i - E[y_i])] = \\beta_1 E[(x_i-E[x_i])(x_i-E[x_i])]" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%28y_i%20-%20E%5By_i%5D%29%5D%20%3D%20%5Cbeta_1%20E%5B%28x_i-E%5Bx_i%5D%29%28x_i-E%5Bx_i%5D%29%5D" title="E[x_i(y_i - E[y_i])] = \\beta_1 E[(x_i-E[x_i])(x_i-E[x_i])]"></p>
<p>Note that we can do the same thing to the left hand side too. So <img alt="E[x_i(y_i - E[y_i])] = E[(x_i-E[x_i])(y_i-E[y_i])]" src="https://latex.codecogs.com/svg.latex?E%5Bx_i%28y_i%20-%20E%5By_i%5D%29%5D%20%3D%20E%5B%28x_i-E%5Bx_i%5D%29%28y_i-E%5By_i%5D%29%5D" title="E[x_i(y_i - E[y_i])] = E[(x_i-E[x_i])(y_i-E[y_i])]"></p>
<p>Now, putting it all together:</p>
<p><img alt="\\frac{E[(x_i-E[x_i])(y_i-E[y_i])]}{E[(x_i-E[x_i])(x_i-E[x_i])]} = \\beta_1" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7BE%5B%28x_i-E%5Bx_i%5D%29%28y_i-E%5By_i%5D%29%5D%7D%7BE%5B%28x_i-E%5Bx_i%5D%29%28x_i-E%5Bx_i%5D%29%5D%7D%20%3D%20%5Cbeta_1" title="\\frac{E[(x_i-E[x_i])(y_i-E[y_i])]}{E[(x_i-E[x_i])(x_i-E[x_i])]} = \\beta_1"></p>
<p><img alt="\\frac{\\text{population covariance}}{\\text{population variance  }} = \\beta_1" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Ctext%7Bpopulation%20covariance%7D%7D%7B%5Ctext%7Bpopulation%20variance%20%20%7D%7D%20%3D%20%5Cbeta_1" title="\\frac{\\text{population covariance}}{\\text{population variance  }} = \\beta_1"></p>
<p>But! we don&rsquo;t have the population x or y or E[x] or E[y]. So we have to use the sample analogs.</p>
<p><img alt="\\frac{1}{N}\\sum_{i=1}^N [Y_i - \\hat{\\beta}_0-\\hat{\\beta}_1 X_i] = 0" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20%5BY_i%20-%20%5Chat%7B%5Cbeta%7D_0-%5Chat%7B%5Cbeta%7D_1%20X_i%5D%20%3D%200" title="\\frac{1}{N}\\sum_{i=1}^N [Y_i - \\hat{\\beta}_0-\\hat{\\beta}_1 X_i] = 0"></p>
<p>and</p>
<p><img alt="\\frac{1}{N}\\sum_{i=1}^N [X_i(Y_i - \\hat{\\beta}_0-\\hat{\\beta}_1 X_i)] = 0" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20%5BX_i%28Y_i%20-%20%5Chat%7B%5Cbeta%7D_0-%5Chat%7B%5Cbeta%7D_1%20X_i%29%5D%20%3D%200" title="\\frac{1}{N}\\sum_{i=1}^N [X_i(Y_i - \\hat{\\beta}_0-\\hat{\\beta}_1 X_i)] = 0"></p>
<p>We can rewrite the solution for <img alt="\\beta_0" src="https://latex.codecogs.com/svg.latex?%5Cbeta_0" title="\\beta_0"> as</p>
<p><img alt="\\beta_0 = \\bar{Y}-\\hat{\\beta_1}\\bar{X}" src="https://latex.codecogs.com/svg.latex?%5Cbeta_0%20%3D%20%5Cbar%7BY%7D-%5Chat%7B%5Cbeta_1%7D%5Cbar%7BX%7D" title="\\beta_0 = \\bar{Y}-\\hat{\\beta_1}\\bar{X}"></p>
<p>If we plug that into the second equation, we get:</p>
<p><img alt="\\frac{1}{N}\\sum_{i=1}^N X_i[Y_i - \\bar{Y}-\\hat{\\beta_1}\\bar{X} -\\hat{\\beta}_1 X_i] = 0" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20X_i%5BY_i%20-%20%5Cbar%7BY%7D-%5Chat%7B%5Cbeta_1%7D%5Cbar%7BX%7D%20-%5Chat%7B%5Cbeta%7D_1%20X_i%5D%20%3D%200" title="\\frac{1}{N}\\sum_{i=1}^N X_i[Y_i - \\bar{Y}-\\hat{\\beta_1}\\bar{X} -\\hat{\\beta}_1 X_i] = 0"></p>
<p>Now separate the <img alt="\\beta_1" src="https://latex.codecogs.com/svg.latex?%5Cbeta_1" title="\\beta_1">&rsquo;s to the right hand side</p>
<p><img alt="\\frac{1}{N}\\sum_{i=1}^N X_i(Y_i - \\bar{Y}) = \\hat{\\beta_1} \\frac{1}{N}\\sum_{i=1}^N X_i (X_i - \\bar{X})" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20X_i%28Y_i%20-%20%5Cbar%7BY%7D%29%20%3D%20%5Chat%7B%5Cbeta_1%7D%20%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20X_i%20%28X_i%20-%20%5Cbar%7BX%7D%29" title="\\frac{1}{N}\\sum_{i=1}^N X_i(Y_i - \\bar{Y}) = \\hat{\\beta_1} \\frac{1}{N}\\sum_{i=1}^N X_i (X_i - \\bar{X})"></p>
<p><img alt="\\hat{\\beta_1} = \\frac{\\frac{1}{N}\\sum_{i=1}^N X_i(X_i - \\bar{Y}) }{\\frac{1}{N}\\sum_{i=1}^N X_i (X_i - \\bar{X})}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta_1%7D%20%3D%20%5Cfrac%7B%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20X_i%28X_i%20-%20%5Cbar%7BY%7D%29%20%7D%7B%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20X_i%20%28X_i%20-%20%5Cbar%7BX%7D%29%7D" title="\\hat{\\beta_1} = \\frac{\\frac{1}{N}\\sum_{i=1}^N X_i(X_i - \\bar{Y}) }{\\frac{1}{N}\\sum_{i=1}^N X_i (X_i - \\bar{X})}"></p>
<p>Which is equivalent to:</p>
<p><img alt="\\hat{\\beta_1} = \\frac{\\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X})(Y_i-\\bar{Y}) }{\\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X}) (X_i - \\bar{X})} = \\frac{\\text{Covariance(X,Y)}}{\\text{Variance(X)}}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta_1%7D%20%3D%20%5Cfrac%7B%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20%28X_i%20-%20%5Cbar%7BX%7D%29%28Y_i-%5Cbar%7BY%7D%29%20%7D%7B%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%20%28X_i%20-%20%5Cbar%7BX%7D%29%20%28X_i%20-%20%5Cbar%7BX%7D%29%7D%20%3D%20%5Cfrac%7B%5Ctext%7BCovariance%28X%2CY%29%7D%7D%7B%5Ctext%7BVariance%28X%29%7D%7D" title="\\hat{\\beta_1} = \\frac{\\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X})(Y_i-\\bar{Y}) }{\\frac{1}{N}\\sum_{i=1}^N (X_i - \\bar{X}) (X_i - \\bar{X})} = \\frac{\\text{Covariance(X,Y)}}{\\text{Variance(X)}}"></p>
<h2 id="linear-algebra">Linear Algebra</h2>
<p>In this section we derive the OLS model using linear algebra. Before getting into the derivatives, note the properties of transposes and matrix derivatives necessary to do this in the section below.</p>
<h3 id="preliminaries-1">Preliminaries</h3>
<ul>
<li>X is an <img alt="x \\times k + 1" src="https://latex.codecogs.com/svg.latex?x%20%5Ctimes%20k%20%2B%201" title="x \\times k + 1"> matrix with k variables and n observations. The plus 1 means we have an intercept as denoted by a vector of ones in the X matrix. This means <img alt="X'" src="https://latex.codecogs.com/svg.latex?X%27" title="X&#39;"> or <img alt="X^T" src="https://latex.codecogs.com/svg.latex?X%5ET" title="X^T"> is <img alt="(k + 1) \\times n" src="https://latex.codecogs.com/svg.latex?%28k%20%2B%201%29%20%5Ctimes%20n" title="(k + 1) \\times n"></li>
<li>e/u are <img alt="n \\times 1" src="https://latex.codecogs.com/svg.latex?n%20%5Ctimes%201" title="n \\times 1"> vectors</li>
<li>y is an <img alt="n \\times 1" src="https://latex.codecogs.com/svg.latex?n%20%5Ctimes%201" title="n \\times 1"> vector</li>
<li><img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is an <img alt="n \\times 1" src="https://latex.codecogs.com/svg.latex?n%20%5Ctimes%201" title="n \\times 1"> vector</li>
<li>A and B are matrices, a and b are vectors</li>
<li>n is the number of observations, k is the number of predictors</li>
<li><img alt="y_i" src="https://latex.codecogs.com/svg.latex?y_i" title="y_i"> denotes the i&rsquo;th value of <img alt="y" src="https://latex.codecogs.com/svg.latex?y" title="y"></li>
</ul>
<h3 id="matrix-transpose-and-derivative-rules">Matrix Transpose and Derivative Rules</h3>
<p>Transpose rules:</p>
<ul>
<li>(AB)&rsquo; = B&rsquo;A'</li>
<li>(a&rsquo;Bc)&rsquo; = c&rsquo;B&rsquo;a</li>
<li>a&rsquo;b = b&rsquo;a - (A + B)C = AC + BC</li>
<li>(a + b)&lsquo;C = a&rsquo;C + b&rsquo;C</li>
<li><img alt="AB \\neq BA" src="https://latex.codecogs.com/svg.latex?AB%20%5Cneq%20BA" title="AB \\neq BA"></li>
</ul>
<p>Derivative rules for taking the derivative with respect to X</p>
<ul>
<li><img alt="X'B \\rightarrow B" src="https://latex.codecogs.com/svg.latex?X%27B%20%5Crightarrow%20B" title="X&#39;B \\rightarrow B"></li>
<li><img alt="x'b \\rightarrow b" src="https://latex.codecogs.com/svg.latex?x%27b%20%5Crightarrow%20b" title="x&#39;b \\rightarrow b"></li>
<li><img alt="x'x \\rightarrow 2x" src="https://latex.codecogs.com/svg.latex?x%27x%20%5Crightarrow%202x" title="x&#39;x \\rightarrow 2x"></li>
<li><img alt="x'Bx \\rightarrow 2Bx" src="https://latex.codecogs.com/svg.latex?x%27Bx%20%5Crightarrow%202Bx" title="x&#39;Bx \\rightarrow 2Bx"></li>
</ul>
<p>See <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf">http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf</a></p>
<h3 id="the-derivation">The Derivation</h3>
<p>Our goal here is to derive the coefficient <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta">.</p>
<p>So we start with argmin <img alt="e'e" src="https://latex.codecogs.com/svg.latex?e%27e" title="e&#39;e"></p>
<p><img alt="e'e = (y-X\\hat{\\beta})'(y-X\\hat{\\beta})" src="https://latex.codecogs.com/svg.latex?e%27e%20%3D%20%28y-X%5Chat%7B%5Cbeta%7D%29%27%28y-X%5Chat%7B%5Cbeta%7D%29" title="e&#39;e = (y-X\\hat{\\beta})&#39;(y-X\\hat{\\beta})"></p>
<p><img alt="= y'y - y' X \\hat{\\beta} - \\hat{\\beta}'X'y + \\hat{\\beta}'X'X\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%3D%20y%27y%20-%20y%27%20X%20%5Chat%7B%5Cbeta%7D%20-%20%5Chat%7B%5Cbeta%7D%27X%27y%20%2B%20%5Chat%7B%5Cbeta%7D%27X%27X%5Chat%7B%5Cbeta%7D" title="= y&#39;y - y&#39; X \\hat{\\beta} - \\hat{\\beta}&#39;X&#39;y + \\hat{\\beta}&#39;X&#39;X\\hat{\\beta}"></p>
<p>Note above that <img alt="y'" src="https://latex.codecogs.com/svg.latex?y%27" title="y&#39;"> is <img alt="1 \\times n" src="https://latex.codecogs.com/svg.latex?1%20%5Ctimes%20n" title="1 \\times n">. So <img alt="y'X\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?y%27X%5Chat%7B%5Cbeta%7D" title="y&#39;X\\hat{\\beta}"> is <img alt="(1 \\times n) (k \\times n) (n \\times 1)" src="https://latex.codecogs.com/svg.latex?%281%20%5Ctimes%20n%29%20%28k%20%5Ctimes%20n%29%20%28n%20%5Ctimes%201%29" title="(1 \\times n) (k \\times n) (n \\times 1)"> also <img alt="\\hat{\\beta}'X'y" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%27X%27y" title="\\hat{\\beta}&#39;X&#39;y"> is <img alt="(1 \\times k) (k \\times n) (n \\times 1) \\rightarrow 1 \\times 1" src="https://latex.codecogs.com/svg.latex?%281%20%5Ctimes%20k%29%20%28k%20%5Ctimes%20n%29%20%28n%20%5Ctimes%201%29%20%5Crightarrow%201%20%5Ctimes%201" title="(1 \\times k) (k \\times n) (n \\times 1) \\rightarrow 1 \\times 1">. We choose <img alt="\\hat{\\beta}'X'y" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%27X%27y" title="\\hat{\\beta}&#39;X&#39;y"> to make the final step of the proof work. (We could also rearrange because X&rsquo;y = y&rsquo;X)</p>
<p><img alt="=  y'y - 2 \\hat{\\beta}'X'y +    \\hat{\\beta}'X'X\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%3D%20%20y%27y%20-%202%20%5Chat%7B%5Cbeta%7D%27X%27y%20%2B%20%20%20%20%5Chat%7B%5Cbeta%7D%27X%27X%5Chat%7B%5Cbeta%7D" title="=  y&#39;y - 2 \\hat{\\beta}&#39;X&#39;y +    \\hat{\\beta}&#39;X&#39;X\\hat{\\beta}"></p>
<p>We minimize this quantity by taking the partial derivative with respect to <img alt="\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D" title="\\hat{\\beta}"> and setting it equal to 0 and solving for <img alt="\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D" title="\\hat{\\beta}"></p>
<p><img alt="\\frac{\\partial e'e}{\\partial \\beta} = -2X'y + 2X'X\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%20e%27e%7D%7B%5Cpartial%20%5Cbeta%7D%20%3D%20-2X%27y%20%2B%202X%27X%5Chat%7B%5Cbeta%7D" title="\\frac{\\partial e&#39;e}{\\partial \\beta} = -2X&#39;y + 2X&#39;X\\hat{\\beta}"></p>
<p><img alt="0 = -2X'y + 2X'X\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?0%20%3D%20-2X%27y%20%2B%202X%27X%5Chat%7B%5Cbeta%7D" title="0 = -2X&#39;y + 2X&#39;X\\hat{\\beta}"></p>
<p><img alt="2X'y = 2X'X\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?2X%27y%20%3D%202X%27X%5Chat%7B%5Cbeta%7D" title="2X&#39;y = 2X&#39;X\\hat{\\beta}"></p>
<p><img alt="X'y = X'X\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?X%27y%20%3D%20X%27X%5Chat%7B%5Cbeta%7D" title="X&#39;y = X&#39;X\\hat{\\beta}"></p>
<p><img alt="(X'X)^{-1} X'y = \\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%28X%27X%29%5E%7B-1%7D%20X%27y%20%3D%20%5Chat%7B%5Cbeta%7D" title="(X&#39;X)^{-1} X&#39;y = \\hat{\\beta}"></p>
<p>The best guide for what this looks like in practice is <a href="https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf">https://mauricio-romero.com/pdfs/Microeconometria/20212/Lecture%207%20-%20OLS%20review.pdf</a> slides 99-105 (page numbers are n plus 1)</p>
<h2 id="maximum-likelihood-derivation-of-ols">Maximum Likelihood Derivation of OLS</h2>
<p>Assume that <img alt="\\epsilon_i \\sim N(0,\\sigma^2)" src="https://latex.codecogs.com/svg.latex?%5Cepsilon_i%20%5Csim%20N%280%2C%5Csigma%5E2%29" title="\\epsilon_i \\sim N(0,\\sigma^2)">. That means we know that <img alt="y_i \\sim N(\\hat{b}'x_i,\\sigma^2)" src="https://latex.codecogs.com/svg.latex?y_i%20%5Csim%20N%28%5Chat%7Bb%7D%27x_i%2C%5Csigma%5E2%29" title="y_i \\sim N(\\hat{b}&#39;x_i,\\sigma^2)"></p>
<p>We start with the joint distribution of <img alt="y_1,y_2,y_3 \\dots y_n" src="https://latex.codecogs.com/svg.latex?y_1%2Cy_2%2Cy_3%20%5Cdots%20y_n" title="y_1,y_2,y_3 \\dots y_n"></p>
<p><img alt="P(y|X,b,\\sigma^2) = \\prod_{i=1}^nf(y_i;X,b,\\sigma^2)" src="https://latex.codecogs.com/svg.latex?P%28y%7CX%2Cb%2C%5Csigma%5E2%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5Enf%28y_i%3BX%2Cb%2C%5Csigma%5E2%29" title="P(y|X,b,\\sigma^2) = \\prod_{i=1}^nf(y_i;X,b,\\sigma^2)"></p>
<p><img alt="P(y|X,b,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sigma^2 \\sqrt{2 \\pi} } \\text{exp}(-\\frac{1}{2\\sigma^2}(y_i-b'x_i)^2)" src="https://latex.codecogs.com/svg.latex?P%28y%7CX%2Cb%2C%5Csigma%5E2%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5En%20%5Cfrac%7B1%7D%7B%5Csigma%5E2%20%5Csqrt%7B2%20%5Cpi%7D%20%7D%20%5Ctext%7Bexp%7D%28-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%28y_i-b%27x_i%29%5E2%29" title="P(y|X,b,\\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sigma^2 \\sqrt{2 \\pi} } \\text{exp}(-\\frac{1}{2\\sigma^2}(y_i-b&#39;x_i)^2)"></p>
<p>Distribute the <img alt="\\prod" src="https://latex.codecogs.com/svg.latex?%5Cprod" title="\\prod"> to the exponent, but remember that it replicates the constant <img alt="\\frac{1}{2\\pi\\sigma^2}" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B1%7D%7B2%5Cpi%5Csigma%5E2%7D" title="\\frac{1}{2\\pi\\sigma^2}"> <img alt="c^n" src="https://latex.codecogs.com/svg.latex?c%5En" title="c^n"> times where c is the constant and n are the number of replications of the product notation.</p>
<p><img alt="(2 \\pi \\sigma^2)^{\\frac{-n}{2}} \\text{exp}(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b'x_i)^2)" src="https://latex.codecogs.com/svg.latex?%282%20%5Cpi%20%5Csigma%5E2%29%5E%7B%5Cfrac%7B-n%7D%7B2%7D%7D%20%5Ctext%7Bexp%7D%28-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28y_i-b%27x_i%29%5E2%29" title="(2 \\pi \\sigma^2)^{\\frac{-n}{2}} \\text{exp}(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b&#39;x_i)^2)"></p>
<p>Now we take the logarithm of each side. This gives us the log likelihood which we&rsquo;re going to&hellip; maximize!</p>
<p><img alt="l = \\ln[P(y|X,b,\\sigma^2)]" src="https://latex.codecogs.com/svg.latex?l%20%3D%20%5Cln%5BP%28y%7CX%2Cb%2C%5Csigma%5E2%29%5D" title="l = \\ln[P(y|X,b,\\sigma^2)]"></p>
<p><img alt="\\ln(l) = \\ln((2 \\pi \\sigma^2)^{\\frac{-n}{2}} \\text{exp}(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b'x_i)^2) )" src="https://latex.codecogs.com/svg.latex?%5Cln%28l%29%20%3D%20%5Cln%28%282%20%5Cpi%20%5Csigma%5E2%29%5E%7B%5Cfrac%7B-n%7D%7B2%7D%7D%20%5Ctext%7Bexp%7D%28-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28y_i-b%27x_i%29%5E2%29%20%29" title="\\ln(l) = \\ln((2 \\pi \\sigma^2)^{\\frac{-n}{2}} \\text{exp}(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b&#39;x_i)^2) )"></p>
<p><img alt="l = -\\frac{n}{2}\\ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b'x_i)^2" src="https://latex.codecogs.com/svg.latex?l%20%3D%20-%5Cfrac%7Bn%7D%7B2%7D%5Cln%282%5Cpi%5Csigma%5E2%29-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%5Csum_%7Bi%3D1%7D%5En%28y_i-b%27x_i%29%5E2" title="l = -\\frac{n}{2}\\ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-b&#39;x_i)^2"></p>
<p>Here&rsquo;s a neat little step - replace the row-notation above with matrix notation.</p>
<p><img alt="\\ln l = -\\frac{n}{2}\\ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2} (\\textbf{y}-\\textbf{Xb})'(\\textbf{y}-\\textbf{Xb})" src="https://latex.codecogs.com/svg.latex?%5Cln%20l%20%3D%20-%5Cfrac%7Bn%7D%7B2%7D%5Cln%282%5Cpi%5Csigma%5E2%29-%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%20%28%5Ctextbf%7By%7D-%5Ctextbf%7BXb%7D%29%27%28%5Ctextbf%7By%7D-%5Ctextbf%7BXb%7D%29" title="\\ln l = -\\frac{n}{2}\\ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2} (\\textbf{y}-\\textbf{Xb})&#39;(\\textbf{y}-\\textbf{Xb})"></p>
<p>Now we have an expression for the log likelihood - so we want to &hellip; maximize it (i.e. for maximum likelihood!). What we&rsquo;re going to do in a few steps is take the partial derivative with respect to b (i.e. <img alt="\\frac{\\partial l}{\\partial b}" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%20l%7D%7B%5Cpartial%20b%7D" title="\\frac{\\partial l}{\\partial b}">) and set the whole thing to 0. This means that, at this point, we don&rsquo;t care about terms that don&rsquo;t have a b in them. I.e. the first term <img alt="-\\frac{n}{2}\\ln(2\\pi\\sigma^2)" src="https://latex.codecogs.com/svg.latex?-%5Cfrac%7Bn%7D%7B2%7D%5Cln%282%5Cpi%5Csigma%5E2%29" title="-\\frac{n}{2}\\ln(2\\pi\\sigma^2)"> does not have a b in it at all, so we can just call that <img alt="c" src="https://latex.codecogs.com/svg.latex?c" title="c">.</p>
<p><img alt="\\ln l = c - \\frac{1}{2\\sigma^2} (\\textbf{y}-\\textbf{Xb})'(\\textbf{y}-\\textbf{Xb})" src="https://latex.codecogs.com/svg.latex?%5Cln%20l%20%3D%20c%20-%20%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%20%28%5Ctextbf%7By%7D-%5Ctextbf%7BXb%7D%29%27%28%5Ctextbf%7By%7D-%5Ctextbf%7BXb%7D%29" title="\\ln l = c - \\frac{1}{2\\sigma^2} (\\textbf{y}-\\textbf{Xb})&#39;(\\textbf{y}-\\textbf{Xb})"></p>
<p><img alt="\\ln l = c - \\frac{1}{2\\sigma^2} (\\textbf{y'y} + \\textbf{b'X'Xb} - \\textbf{2b'X'y})" src="https://latex.codecogs.com/svg.latex?%5Cln%20l%20%3D%20c%20-%20%5Cfrac%7B1%7D%7B2%5Csigma%5E2%7D%20%28%5Ctextbf%7By%27y%7D%20%2B%20%5Ctextbf%7Bb%27X%27Xb%7D%20-%20%5Ctextbf%7B2b%27X%27y%7D%29" title="\\ln l = c - \\frac{1}{2\\sigma^2} (\\textbf{y&#39;y} + \\textbf{b&#39;X&#39;Xb} - \\textbf{2b&#39;X&#39;y})"></p>
<p>Now do the partial derivative and set them equal to 0.</p>
<p><img alt="\\frac{\\partial l}{\\partial b} =  2\\textbf{X'Xb} - \\textbf{2X'y} = 0" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%20l%7D%7B%5Cpartial%20b%7D%20%3D%20%202%5Ctextbf%7BX%27Xb%7D%20-%20%5Ctextbf%7B2X%27y%7D%20%3D%200" title="\\frac{\\partial l}{\\partial b} =  2\\textbf{X&#39;Xb} - \\textbf{2X&#39;y} = 0"></p>
<p><img alt="\\frac{\\partial l}{\\partial b} \\rightarrow 2\\textbf{X'Xb} = \\textbf{2X'y}" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%20l%7D%7B%5Cpartial%20b%7D%20%5Crightarrow%202%5Ctextbf%7BX%27Xb%7D%20%3D%20%5Ctextbf%7B2X%27y%7D" title="\\frac{\\partial l}{\\partial b} \\rightarrow 2\\textbf{X&#39;Xb} = \\textbf{2X&#39;y}"></p>
<p><img alt="\\hat{\\textbf{b}}_{ML} =  (\\textbf{X'X})^{-1}\\textbf{X'y}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Ctextbf%7Bb%7D%7D_%7BML%7D%20%3D%20%20%28%5Ctextbf%7BX%27X%7D%29%5E%7B-1%7D%5Ctextbf%7BX%27y%7D" title="\\hat{\\textbf{b}}_{ML} =  (\\textbf{X&#39;X})^{-1}\\textbf{X&#39;y}"></p>
<p>To get the variance - take the partial with respect to <img alt="\\sigma" src="https://latex.codecogs.com/svg.latex?%5Csigma" title="\\sigma"></p>
<p>Source:</p>
<ul>
<li><a href="https://allmodelsarewrong.github.io/olsml.html">https://allmodelsarewrong.github.io/olsml.html</a></li>
</ul>
<h2 id="gradient-descent-for-ols">Gradient Descent for OLS</h2>
<p>See Gradient Descent help file! Here we&rsquo;re not deriving OLS but we&rsquo;re estimating the coefficients analytically.</p>
<p>Source:</p>
<ul>
<li><a href="https://allmodelsarewrong.github.io/gradient.html">https://allmodelsarewrong.github.io/gradient.html</a></li>
<li><a href="https://www.ocf.berkeley.edu/~janastas/stochastic-gradient-descent-in-r.html">https://www.ocf.berkeley.edu/~janastas/stochastic-gradient-descent-in-r.html</a></li>
</ul>
<h2 id="are-methods-equivalent">Are Methods Equivalent?</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">haven</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">lmtest</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">sandwich</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">sjPlot</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#DGP for x</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">&lt;-</span> <span class="m">50000</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">abs</span><span class="p">(</span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="n">mean</span> <span class="o">=</span> <span class="m">13</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="m">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">&lt;-</span> <span class="m">4000</span>
</span></span><span class="line"><span class="cl"><span class="n">u</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="m">40000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">&lt;-</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">u</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">DGP</span> <span class="o">&lt;-</span> <span class="nf">tibble</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">  <span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">ggplot</span><span class="p">(</span><span class="n">DGP</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">theme_minimal</span><span class="p">()</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">labs</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Truth&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="s">&#34;Simulated X&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="s">&#34;Simulated Y&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span> <span class="o">+</span> <span class="nf">stat_smooth</span><span class="p">(</span><span class="n">method</span> <span class="o">=</span> <span class="s">&#34;lm&#34;</span><span class="p">,</span> <span class="n">formula</span> <span class="o">=</span> <span class="n">y</span> <span class="o">~</span> <span class="n">x</span><span class="p">,</span> <span class="n">geom</span> <span class="o">=</span> <span class="s">&#34;smooth&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Method of moments </span>
</span></span><span class="line"><span class="cl"><span class="nf">cov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="nf">var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] 4077.509
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># Linear Algebra </span>
</span></span><span class="line"><span class="cl"><span class="n">x_mat</span> <span class="o">&lt;-</span> <span class="nf">as.matrix</span><span class="p">(</span><span class="nf">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span> <span class="c1"># remember to add column of ones!</span>
</span></span><span class="line"><span class="cl"><span class="nf">solve</span><span class="p">(</span> <span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">x_mat</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">x_mat</span><span class="p">)</span> <span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">x_mat</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">y</span><span class="p">)</span> 
</span></span></code></pre></div><pre><code>       [,1]
  49588.093
x  4077.509
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># MLE</span>
</span></span><span class="line"><span class="cl"><span class="n">loglikMLE</span> <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span><span class="n">par</span><span class="p">,</span> <span class="n">y</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">  <span class="n">x</span> <span class="o">=</span> <span class="nf">as.vector</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="nf">sum</span><span class="p">(</span><span class="nf">dnorm</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="n">par[1]</span> <span class="o">+</span> <span class="n">par[2]</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="n">par[3]</span><span class="p">,</span> <span class="n">log</span> <span class="o">=</span> <span class="bp">T</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">MLE</span> <span class="o">&lt;-</span> <span class="nf">optim</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="n">intercept</span> <span class="o">=</span> <span class="m">9000</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="m">400</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="m">1</span><span class="p">),</span> <span class="n">fn</span> <span class="o">=</span> <span class="n">loglikMLE</span><span class="p">,</span> <span class="c1">#parameters to be estimated, function to optimise</span>
</span></span><span class="line"><span class="cl">               <span class="n">y</span> <span class="o">=</span> <span class="nf">as.vector</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">control</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">fnscale</span> <span class="o">=</span> <span class="m">-1</span><span class="p">,</span> <span class="n">reltol</span> <span class="o">=</span> <span class="m">1e-16</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">               <span class="c1">#we specify -1 in the controls so that we MAXIMIZE rather than minimize, which is the default</span>
</span></span><span class="line"><span class="cl">               <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">round</span><span class="p">(</span><span class="n">MLE</span><span class="o">$</span><span class="n">par</span><span class="p">,</span><span class="m">1</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>intercept         x     sigma 
  49679.8    4074.2   39949.7 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># Via Gradient Descent </span>
</span></span><span class="line"><span class="cl"><span class="n">cost</span> <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nf">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">X</span> <span class="o">%*%</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="n">^2</span> <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="nf">length</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#We must also set two additional parameters: learning rate and iteration limit</span>
</span></span><span class="line"><span class="cl"><span class="n">alpha</span> <span class="o">&lt;-</span> <span class="m">0.01</span>
</span></span><span class="line"><span class="cl"><span class="n">num_iters</span> <span class="o">&lt;-</span> <span class="m">10000</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># keep history</span>
</span></span><span class="line"><span class="cl"><span class="n">cost_history</span> <span class="o">&lt;-</span> <span class="nf">double</span><span class="p">(</span><span class="n">num_iters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">theta_history</span> <span class="o">&lt;-</span> <span class="nf">list</span><span class="p">(</span><span class="n">num_iters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># initialize coefficients</span>
</span></span><span class="line"><span class="cl"><span class="n">theta</span> <span class="o">&lt;-</span> <span class="nf">matrix</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">0</span><span class="p">),</span> <span class="n">nrow</span><span class="o">=</span><span class="m">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># add a column of 1&#39;s for the intercept coefficient</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">&lt;-</span> <span class="nf">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="nf">matrix</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># gradient descent</span>
</span></span><span class="line"><span class="cl"><span class="kr">for</span> <span class="p">(</span><span class="n">i</span> <span class="kr">in</span> <span class="m">1</span><span class="o">:</span><span class="n">num_iters</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="n">error</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">X</span> <span class="o">%*%</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">delta</span> <span class="o">&lt;-</span> <span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">error</span> <span class="o">/</span> <span class="nf">length</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">theta</span> <span class="o">&lt;-</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">delta</span>
</span></span><span class="line"><span class="cl">  <span class="n">cost_history[i]</span> <span class="o">&lt;-</span> <span class="nf">cost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">theta_history[[i]]</span> <span class="o">&lt;-</span> <span class="n">theta</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">gradient_descent_ols_estimates</span> <span class="o">&lt;-</span> <span class="n">theta[</span><span class="p">,</span><span class="m">1</span><span class="n">]</span> <span class="o">%&gt;%</span> <span class="nf">tibble</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># how close is this to truth</span>
</span></span><span class="line"><span class="cl"><span class="n">est</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">y</span> <span class="o">~</span> <span class="n">x</span><span class="p">,</span> <span class="n">DGP</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">truth_vec</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">est</span><span class="o">$</span><span class="n">coefficients</span> <span class="o">%&gt;%</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">tibble</span><span class="p">()</span> <span class="o">%&gt;%</span> 
</span></span><span class="line"><span class="cl">  <span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_cols</span><span class="p">(</span><span class="n">truth_vec</span><span class="p">)</span> <span class="o">%&gt;%</span> 
</span></span><span class="line"><span class="cl">  <span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_cols</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s">&#34;a&#34;</span><span class="p">,</span><span class="s">&#34;b&#34;</span><span class="p">))</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_cols</span><span class="p">(</span><span class="n">gradient_descent_ols_estimates</span><span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_cols</span><span class="p">(</span><span class="n">MLE</span><span class="o">$</span><span class="n">par[1</span><span class="o">:</span><span class="m">2</span><span class="n">]</span><span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">rename</span><span class="p">(</span><span class="s">&#34;OLS Estimate&#34;</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="s">&#34;Truth&#34;</span> <span class="o">=</span> <span class="m">2</span><span class="p">,</span> <span class="s">&#34;Parameter&#34;</span> <span class="o">=</span> <span class="m">3</span><span class="p">,</span> <span class="s">&#34;Gradient Descent&#34;</span> <span class="o">=</span> <span class="m">4</span><span class="p">,</span> <span class="s">&#34;MLE&#34;</span> <span class="o">=</span> <span class="m">5</span><span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">relocate</span><span class="p">(</span><span class="n">Parameter</span><span class="p">,</span> <span class="n">.before</span> <span class="o">=</span> <span class="n">`OLS Estimate`</span><span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">relocate</span><span class="p">(</span><span class="n">Truth</span><span class="p">,</span> <span class="n">.before</span> <span class="o">=</span> <span class="n">`OLS Estimate`</span><span class="p">)</span>
</span></span></code></pre></div><pre><code># A tibble: 2 × 5
  Parameter Truth `OLS Estimate` `Gradient Descent`    MLE
  &lt;chr&gt;     &lt;dbl&gt;          &lt;dbl&gt;              &lt;dbl&gt;  &lt;dbl&gt;
1 a         50000         49588.             49588. 49680.
2 b          4000          4078.              4078.  4074.
</code></pre>
<img src="figs/unnamed-chunk-1-1.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<h1 id="variance-of-betahttpslatexcodecogscomsvglatex5cbeta-beta---sigma2httpslatexcodecogscomsvglatex5csigma5e2-sigma2">Variance of <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> - <img alt="\\sigma^2" src="https://latex.codecogs.com/svg.latex?%5Csigma%5E2" title="\\sigma^2"></h1>
<p>We&rsquo;ve shown how to derive the OLS model above using method of moments, linear algebra and maximum likelihood.</p>
<p>First a necessary (and, totally pedantic) note:</p>
<ul>
<li>
<p>Errors/disturbances are the vertical distances between observations and the unknown Conditional Expectation Function. Therefore, they are unknown.</p>
</li>
<li>
<p>Residuals are the vertical distances between observations and the estimated regression function. Therefore, they are known.</p>
</li>
</ul>
<p>If <img alt="\\hat{\\beta}= (X'X)^{-1}X'y" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%3D%20%28X%27X%29%5E%7B-1%7DX%27y" title="\\hat{\\beta}= (X&#39;X)^{-1}X&#39;y"> and <img alt="y = X\\beta + u" src="https://latex.codecogs.com/svg.latex?y%20%3D%20X%5Cbeta%20%2B%20u" title="y = X\\beta + u"></p>
<p><img alt="\\hat{\\beta} = (X'X)^{-1}X'(X\\beta + u) = (X'X)^{-1}X'X\\beta + (X'X)^{-1}X'u = I\\beta + (X'X)^{-1}X'u" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%20%3D%20%28X%27X%29%5E%7B-1%7DX%27%28X%5Cbeta%20%2B%20u%29%20%3D%20%28X%27X%29%5E%7B-1%7DX%27X%5Cbeta%20%2B%20%28X%27X%29%5E%7B-1%7DX%27u%20%3D%20I%5Cbeta%20%2B%20%28X%27X%29%5E%7B-1%7DX%27u" title="\\hat{\\beta} = (X&#39;X)^{-1}X&#39;(X\\beta + u) = (X&#39;X)^{-1}X&#39;X\\beta + (X&#39;X)^{-1}X&#39;u = I\\beta + (X&#39;X)^{-1}X&#39;u"></p>
<p>Now take the variance of each side</p>
<p><img alt="V(\\hat{\\beta}|X) = V(\\beta + (X'X)^{-1}X'u|X)" src="https://latex.codecogs.com/svg.latex?V%28%5Chat%7B%5Cbeta%7D%7CX%29%20%3D%20V%28%5Cbeta%20%2B%20%28X%27X%29%5E%7B-1%7DX%27u%7CX%29" title="V(\\hat{\\beta}|X) = V(\\beta + (X&#39;X)^{-1}X&#39;u|X)"></p>
<p><img alt="V(\\hat{\\beta}|X) = V(\\beta|X) + V((X'X)^{-1}X'u|X)" src="https://latex.codecogs.com/svg.latex?V%28%5Chat%7B%5Cbeta%7D%7CX%29%20%3D%20V%28%5Cbeta%7CX%29%20%2B%20V%28%28X%27X%29%5E%7B-1%7DX%27u%7CX%29" title="V(\\hat{\\beta}|X) = V(\\beta|X) + V((X&#39;X)^{-1}X&#39;u|X)"></p>
<p>In the above, note that <img alt="V(\\beta|X)=0" src="https://latex.codecogs.com/svg.latex?V%28%5Cbeta%7CX%29%3D0" title="V(\\beta|X)=0">, since <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is constant</p>
<p>To get to the next step, note that when A is a constant matrix and X is random, <img alt="V(AX) = A V(X) A'" src="https://latex.codecogs.com/svg.latex?V%28AX%29%20%3D%20A%20V%28X%29%20A%27" title="V(AX) = A V(X) A&#39;"> and <img alt="u = y - X\\beta" src="https://latex.codecogs.com/svg.latex?u%20%3D%20y%20-%20X%5Cbeta" title="u = y - X\\beta">. We&rsquo;re not going to substitute in the <img alt="u" src="https://latex.codecogs.com/svg.latex?u" title="u"> but point this out as to why it remains trapped in the variance operator</p>
<p><img alt="V(\\hat{\\beta}|X) = (X'X)^{-1}X' V(u|X) X(X'X)^{-1}" src="https://latex.codecogs.com/svg.latex?V%28%5Chat%7B%5Cbeta%7D%7CX%29%20%3D%20%28X%27X%29%5E%7B-1%7DX%27%20V%28u%7CX%29%20X%28X%27X%29%5E%7B-1%7D" title="V(\\hat{\\beta}|X) = (X&#39;X)^{-1}X&#39; V(u|X) X(X&#39;X)^{-1}"></p>
<p>Note that <img alt="V(u|X) = \\sigma^2" src="https://latex.codecogs.com/svg.latex?V%28u%7CX%29%20%3D%20%5Csigma%5E2" title="V(u|X) = \\sigma^2"> by the Gauss-Markov assumptions</p>
<p>So we&rsquo;re left with:</p>
<p><img alt="V(\\hat{\\beta}|X) = (X'X)^{-1}X' \\sigma^2 X(X'X)^{-1} = \\sigma^2(X'X)^{-1}" src="https://latex.codecogs.com/svg.latex?V%28%5Chat%7B%5Cbeta%7D%7CX%29%20%3D%20%28X%27X%29%5E%7B-1%7DX%27%20%5Csigma%5E2%20X%28X%27X%29%5E%7B-1%7D%20%3D%20%5Csigma%5E2%28X%27X%29%5E%7B-1%7D" title="V(\\hat{\\beta}|X) = (X&#39;X)^{-1}X&#39; \\sigma^2 X(X&#39;X)^{-1} = \\sigma^2(X&#39;X)^{-1}"></p>
<p>But we don&rsquo;t have <img alt="\\sigma^2" src="https://latex.codecogs.com/svg.latex?%5Csigma%5E2" title="\\sigma^2"> - that&rsquo;s a population concept. So we estimate it with <img alt="\\hat{\\sigma}^2=\\frac{u'u}{n-k}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Csigma%7D%5E2%3D%5Cfrac%7Bu%27u%7D%7Bn-k%7D" title="\\hat{\\sigma}^2=\\frac{u&#39;u}{n-k}">.</p>
<p>What does this look like?</p>
<p>It means, essentially, that the diagonals of the variance-covariance matrix, are the standard errors of <img alt="\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D" title="\\hat{\\beta}"></p>
<p>See <a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf">https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf</a></p>
<h3 id="longer-proof">Longer proof</h3>
<p>We can also start from Var(<img alt="\\hat{\\beta}) = E[(\\hat{\\beta}-E[\\beta])]^2" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%29%20%3D%20E%5B%28%5Chat%7B%5Cbeta%7D-E%5B%5Cbeta%5D%29%5D%5E2" title="\\hat{\\beta}) = E[(\\hat{\\beta}-E[\\beta])]^2"></p>
<p>First, replace <img alt="\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D" title="\\hat{\\beta}"> with <img alt="(X'X)^{-1}X'Y" src="https://latex.codecogs.com/svg.latex?%28X%27X%29%5E%7B-1%7DX%27Y" title="(X&#39;X)^{-1}X&#39;Y"></p>
<p><img alt="= E[(X'X)^{-1}X'Y - E[\\beta]]^2" src="https://latex.codecogs.com/svg.latex?%3D%20E%5B%28X%27X%29%5E%7B-1%7DX%27Y%20-%20E%5B%5Cbeta%5D%5D%5E2" title="= E[(X&#39;X)^{-1}X&#39;Y - E[\\beta]]^2"></p>
<p>Now replace <img alt="Y" src="https://latex.codecogs.com/svg.latex?Y" title="Y"> with <img alt="X\\beta + e" src="https://latex.codecogs.com/svg.latex?X%5Cbeta%20%2B%20e" title="X\\beta + e"></p>
<p><img alt="= E[(X'X)^{-1}X'(X\\beta + e) - E[\\beta]]^2" src="https://latex.codecogs.com/svg.latex?%3D%20E%5B%28X%27X%29%5E%7B-1%7DX%27%28X%5Cbeta%20%2B%20e%29%20-%20E%5B%5Cbeta%5D%5D%5E2" title="= E[(X&#39;X)^{-1}X&#39;(X\\beta + e) - E[\\beta]]^2"></p>
<p>We know that <img alt="E[\\beta] = \\beta" src="https://latex.codecogs.com/svg.latex?E%5B%5Cbeta%5D%20%3D%20%5Cbeta" title="E[\\beta] = \\beta">, so make that sub</p>
<p><img alt="= E[(X'X)^{-1}X'(X\\beta + e) - \\beta]^2" src="https://latex.codecogs.com/svg.latex?%3D%20E%5B%28X%27X%29%5E%7B-1%7DX%27%28X%5Cbeta%20%2B%20e%29%20-%20%5Cbeta%5D%5E2" title="= E[(X&#39;X)^{-1}X&#39;(X\\beta + e) - \\beta]^2"></p>
<p>Do the multiplication</p>
<p><img alt="= E[(X'X)^{-1}X'X\\beta + (X'X)^{-1}X'e - \\beta]^2" src="https://latex.codecogs.com/svg.latex?%3D%20E%5B%28X%27X%29%5E%7B-1%7DX%27X%5Cbeta%20%2B%20%28X%27X%29%5E%7B-1%7DX%27e%20-%20%5Cbeta%5D%5E2" title="= E[(X&#39;X)^{-1}X&#39;X\\beta + (X&#39;X)^{-1}X&#39;e - \\beta]^2"></p>
<p>Recognize <img alt="(X'X)^{-1}X'X = I" src="https://latex.codecogs.com/svg.latex?%28X%27X%29%5E%7B-1%7DX%27X%20%3D%20I" title="(X&#39;X)^{-1}X&#39;X = I"></p>
<p><img alt="= E[I\\beta + (X'X)^{-1}X'e - \\beta]^2" src="https://latex.codecogs.com/svg.latex?%3D%20E%5BI%5Cbeta%20%2B%20%28X%27X%29%5E%7B-1%7DX%27e%20-%20%5Cbeta%5D%5E2" title="= E[I\\beta + (X&#39;X)^{-1}X&#39;e - \\beta]^2"></p>
<p>We&rsquo;ve added and subtracted a <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> so they cancel out</p>
<p><img alt="= E[(X'X)^{-1}X'e]^2" src="https://latex.codecogs.com/svg.latex?%3D%20E%5B%28X%27X%29%5E%7B-1%7DX%27e%5D%5E2" title="= E[(X&#39;X)^{-1}X&#39;e]^2"></p>
<p>Expand the square. Note that X is fixed so E(X)=X</p>
<p><img alt="= ((X'X)^{-1}X'e )^ 2" src="https://latex.codecogs.com/svg.latex?%3D%20%28%28X%27X%29%5E%7B-1%7DX%27e%20%29%5E%202" title="= ((X&#39;X)^{-1}X&#39;e )^ 2"></p>
<p>Note that to square in matrix algebra we have to take the prime of the original term (i.e. XX is non-conformable unless X is square, but we can do XX&rsquo; and that would be an nkkn matrix or n*n)</p>
<p><img alt="= (X'X)^{-1}X'e e'X(X'X)^{-1}" src="https://latex.codecogs.com/svg.latex?%3D%20%28X%27X%29%5E%7B-1%7DX%27e%20e%27X%28X%27X%29%5E%7B-1%7D" title="= (X&#39;X)^{-1}X&#39;e e&#39;X(X&#39;X)^{-1}"></p>
<p>Notice that we have <img alt="(X'X)^{-1}X'e e'X" src="https://latex.codecogs.com/svg.latex?%28X%27X%29%5E%7B-1%7DX%27e%20e%27X" title="(X&#39;X)^{-1}X&#39;e e&#39;X"></p>
<p><img alt="= \\sigma^2 I (X'X)^{-1}" src="https://latex.codecogs.com/svg.latex?%3D%20%5Csigma%5E2%20I%20%28X%27X%29%5E%7B-1%7D" title="= \\sigma^2 I (X&#39;X)^{-1}"></p>
<p>From the last step take the square root of the diagonals to get the standard errors</p>
<h2 id="errors-in-matrix-form-in-r">Errors in Matrix Form in R</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">x1</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span> <span class="m">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x2</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">30</span><span class="p">,</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">y1</span> <span class="o">&lt;-</span> <span class="m">4</span> <span class="o">+</span> <span class="m">5</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="m">7</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">+</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">10</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">summary</span><span class="p">(</span><span class="n">m</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">y1</span> <span class="o">~</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span><span class="p">))</span>
</span></span></code></pre></div><pre><code>Call:
lm(formula = y1 ~ x1 + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-32.404  -6.335  -0.301   6.552  32.942 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.58012    1.18698   3.016  0.00262 ** 
x1           5.03844    0.06415  78.548  &lt; 2e-16 ***
x2           7.01745    0.03174 221.063  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 10.06 on 997 degrees of freedom
Multiple R-squared:  0.9823,    Adjusted R-squared:  0.9823 
F-statistic: 2.765e+04 on 2 and 997 DF,  p-value: &lt; 2.2e-16
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">X</span> <span class="o">&lt;-</span> <span class="nf">data.frame</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">&lt;-</span> <span class="nf">as.matrix</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">&lt;-</span> <span class="nf">as.matrix</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">beta</span> <span class="o">=</span> <span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># vcov in R</span>
</span></span><span class="line"><span class="cl"><span class="n">e</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">%*%</span> <span class="n">beta</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># this is as above</span>
</span></span><span class="line"><span class="cl"><span class="n">std_error</span> <span class="o">&lt;-</span> <span class="p">(</span> <span class="nf">t</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">e</span>  <span class="o">/</span> <span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[1]</span> <span class="o">-</span> <span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[2]</span><span class="p">)</span> <span class="p">)</span><span class="n">[1</span><span class="p">,</span><span class="m">1</span><span class="n">]</span> <span class="o">*</span> <span class="p">(</span><span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Robust standard errors</span>
</span></span><span class="line"><span class="cl"><span class="c1">#https://library.virginia.edu/data/articles/understanding-robust-standard-errors</span>
</span></span><span class="line"><span class="cl"><span class="c1"># n / (n-k) * u_hat^2</span>
</span></span><span class="line"><span class="cl"><span class="n">HC1</span> <span class="o">&lt;-</span>  <span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[1]</span> <span class="o">/</span>  <span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[1]</span> <span class="o">-</span> <span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[2]</span><span class="p">)</span> <span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">e</span> <span class="o">%*%</span> <span class="nf">t</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">vce_hc1</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">))</span> <span class="o">%*%</span> <span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">((</span><span class="n">HC1</span> <span class="o">*</span> <span class="nf">diag</span><span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[1]</span><span class="p">))</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">))</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># show all results are equivalent</span>
</span></span><span class="line"><span class="cl"><span class="nf">tibble</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">our_beta</span> <span class="o">=</span> <span class="n">beta</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">their_beta</span> <span class="o">=</span> <span class="nf">lm</span><span class="p">(</span><span class="n">y1</span> <span class="o">~</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span><span class="p">)</span><span class="o">$</span><span class="n">coefficients</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="n">our_error</span> <span class="o">=</span> <span class="nf">sqrt</span><span class="p">(</span><span class="nf">diag</span><span class="p">(</span><span class="n">std_error</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="n">their_error</span> <span class="o">=</span> <span class="nf">sqrt</span><span class="p">(</span><span class="nf">diag</span><span class="p">(</span><span class="nf">vcov</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">y1</span> <span class="o">~</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span><span class="p">)))),</span>
</span></span><span class="line"><span class="cl">  <span class="n">our_robust_ses</span> <span class="o">=</span> <span class="nf">sqrt</span><span class="p">(</span><span class="nf">diag</span><span class="p">(</span><span class="n">vce_hc1</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="n">their_robust_ses</span> <span class="o">=</span> <span class="nf">coeftest</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">vcov.</span> <span class="o">=</span> <span class="nf">vcovHC</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&#39;HC1&#39;</span><span class="p">))</span><span class="n">[</span><span class="p">,</span><span class="m">2</span><span class="n">]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><pre><code># A tibble: 3 × 6
  our_beta[,1] their_beta our_error their_error our_robust_ses their_robust_ses
         &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;
1         3.58       3.58    1.19        1.19           1.15             1.15  
2         5.04       5.04    0.0641      0.0641         0.0665           0.0665
3         7.02       7.02    0.0317      0.0317         0.0310           0.0310
</code></pre>
<h2 id="what-if-the-homoskedasticity-assumption-is-wrong">What if the homoskedasticity assumption is wrong?</h2>
<p>We can use Huber-White standard errors. These state that instead of replacing the <img alt="ee'" src="https://latex.codecogs.com/svg.latex?ee%27" title="ee&#39;"> with <img alt="\\sigma^2I" src="https://latex.codecogs.com/svg.latex?%5Csigma%5E2I" title="\\sigma^2I">, we let the errors vary <img alt="(X'X)^{-1}X'ee'(X'X)^{-1}X'" src="https://latex.codecogs.com/svg.latex?%28X%27X%29%5E%7B-1%7DX%27ee%27%28X%27X%29%5E%7B-1%7DX%27" title="(X&#39;X)^{-1}X&#39;ee&#39;(X&#39;X)^{-1}X&#39;"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># Robust standard errors</span>
</span></span><span class="line"><span class="cl"><span class="c1">#https://library.virginia.edu/data/articles/understanding-robust-standard-errors</span>
</span></span><span class="line"><span class="cl"><span class="c1"># n / (n-k) * u_hat^2</span>
</span></span><span class="line"><span class="cl"><span class="n">HC1</span> <span class="o">&lt;-</span>  <span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[1]</span> <span class="o">/</span>  <span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[1]</span> <span class="o">-</span> <span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[2]</span><span class="p">)</span> <span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">e</span> <span class="o">%*%</span> <span class="nf">t</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">vce_hc1</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">))</span> <span class="o">%*%</span> <span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">((</span><span class="n">HC1</span> <span class="o">*</span> <span class="nf">diag</span><span class="p">(</span><span class="nf">dim</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="n">[1]</span><span class="p">))</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">))</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># calculate using sqrt of diagonal</span>
</span></span><span class="line"><span class="cl"><span class="nf">sqrt</span><span class="p">(</span><span class="nf">diag</span><span class="p">(</span><span class="n">vce_hc1</span><span class="p">)</span>
</span></span></code></pre></div><h1 id="assumptions-of-ols">Assumptions of OLS</h1>
<p>Gauss-Markov Assumptions in Matrix Form.</p>
<ol>
<li>
<p>Linearity: <img alt="y = X\\beta + u" src="https://latex.codecogs.com/svg.latex?y%20%3D%20X%5Cbeta%20%2B%20u" title="y = X\\beta + u"> i.e. <img alt="Y_i = \\beta_0 + \\beta_1X_i + \\beta_2Z_i + ... + u_i" src="https://latex.codecogs.com/svg.latex?Y_i%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1X_i%20%2B%20%5Cbeta_2Z_i%20%2B%20...%20%2B%20u_i" title="Y_i = \\beta_0 + \\beta_1X_i + \\beta_2Z_i + ... + u_i">. This means that the population model is linear in its parameters and correctly specified. In other words, we assume this to be the structural model, i.e., the model describing the true process generating Y. By linear in the parameters, we allow the variables (i.e. the <img alt="X_i" src="https://latex.codecogs.com/svg.latex?X_i" title="X_i"> or <img alt="Z_i" src="https://latex.codecogs.com/svg.latex?Z_i" title="Z_i">) to be non-linear. So we could have <img alt="Y_i = \\beta_0 + \\beta_1(X_i^2) + \\beta_2 \\log (Z_i) + ... + u_i" src="https://latex.codecogs.com/svg.latex?Y_i%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1%28X_i%5E2%29%20%2B%20%5Cbeta_2%20%5Clog%20%28Z_i%29%20%2B%20...%20%2B%20u_i" title="Y_i = \\beta_0 + \\beta_1(X_i^2) + \\beta_2 \\log (Z_i) + ... + u_i"> and this would be ok.</p>
</li>
<li>
<p>Random/iid sample: <img alt="(y_i, x_i')" src="https://latex.codecogs.com/svg.latex?%28y_i%2C%20x_i%27%29" title="(y_i, x_i&#39;)"> are an iid sample from the population. The observed data represent a random sample from the population described by the model. This is violated in time-series, and when the sample doesn&rsquo;t represent the population.</p>
</li>
<li>
<p>No perfect collinearity: <img alt="X" src="https://latex.codecogs.com/svg.latex?X" title="X"> is an <img alt="n \\times (k + 1)" src="https://latex.codecogs.com/svg.latex?n%20%5Ctimes%20%28k%20%2B%201%29" title="n \\times (k + 1)"> matrix with rank k + 1. There is variation in the explanatory variable. This is the identification condition that allows us to invert a matrix and get <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta">. In the bivariate case, this means that there is variation in x.</p>
</li>
<li>
<p>Zero conditional mean: <img alt="E[u|X] = 0" src="https://latex.codecogs.com/svg.latex?E%5Bu%7CX%5D%20%3D%200" title="E[u|X] = 0">. Expected value of the error term is zero conditional on all values of the explanatory variable. The assumption implies that <img alt="E(y) = X\\beta" src="https://latex.codecogs.com/svg.latex?E%28y%29%20%3D%20X%5Cbeta" title="E(y) = X\\beta">. This is important since it essentially says that we get the mean function right.</p>
</li>
</ol>
<p>Recall that <img alt="u" src="https://latex.codecogs.com/svg.latex?u" title="u"> represents all unobserved factors that influence <img alt="Y" src="https://latex.codecogs.com/svg.latex?Y" title="Y">. If such unobserved factors are also correlated with X, Cov<img alt="(X, u) \\neq 0" src="https://latex.codecogs.com/svg.latex?%28X%2C%20u%29%20%5Cneq%200" title="(X, u) \\neq 0">. For example, assume we care about explaining wages as only a function of education. We must assume that ability is the same for everyone regardless of education (i.e. E[ability|education = low] = E[ability|education = high]). That is, everyone has the same ability and the only thing that varies is education!</p>
<ol>
<li>
<p>Homoskedasticity: <img alt="var(u|X) = \\sigma_u^2\\textbf{I}_n" src="https://latex.codecogs.com/svg.latex?var%28u%7CX%29%20%3D%20%5Csigma_u%5E2%5Ctextbf%7BI%7D_n" title="var(u|X) = \\sigma_u^2\\textbf{I}_n"> The error term has the same variance conditional on all values of the explanatory variable.</p>
</li>
<li>
<p>Normality: <img alt="u|X\\sim N(0,\\sigma_u^2\\textbf{I}_n)" src="https://latex.codecogs.com/svg.latex?u%7CX%5Csim%20N%280%2C%5Csigma_u%5E2%5Ctextbf%7BI%7D_n%29" title="u|X\\sim N(0,\\sigma_u^2\\textbf{I}_n)">. The error term is independent of the explanatory variables and normally distributed.</p>
</li>
</ol>
<h1 id="uses-of-the-assumptions">Uses of the assumptions</h1>
<p>If we don&rsquo;t have variation in x, we can&rsquo;t calculate the coefficients. (If we don&rsquo;t have variation in y, <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> = 0)</p>
<p>If 1-4 hold estimates are unbiased (i.e. sample beta = population beta), and consistent (OLS estimator would converge to the true population parameter as the sample size get larger, and tends to infinity.) Note that ``an estimator is unbiased if the expected value of the sampling distribution of the estimators is equal the true population parameter value. An estimator is consistent if, as the sample size increases, tends to infinity, the estimates converge to the true population parameter.&rdquo;</p>
<p>If 1-5 are known as the Gauss-Markov assumptions and, if they hold, the estimates are BLUE (best linear unbiased estimator), they also allow for large-sample inference. Best means lowest variance. Linear means among linear estimators. Unbiased means that sample beta is the population beta. If you violate homoskedasticity, then OLS isn&rsquo;t the best among the linear-unbiased-estimators.</p>
<p>1-6 allow for small-sample inference</p>
<p>See:</p>
<ul>
<li>
<p><a href="https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture5_handout2020.pdf">https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture5_handout2020.pdf</a></p>
</li>
<li>
<p><a href="https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture6_handout2020.pdf">https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture6_handout2020.pdf</a></p>
</li>
<li>
<p><a href="https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture7_handout2020.pdf">https://bstewart.scholar.princeton.edu/sites/g/files/toruqf4016/files/bstewart/files/lecture7_handout2020.pdf</a></p>
</li>
</ul>
<h2 id="mechanical-properties-of-ols">Mechanical Properties of OLS</h2>
<ol>
<li>The observed values of <strong>X</strong> are uncorrelated with the residuals. (Note we&rsquo;re just assuming that X is uncorrelated with the unobserved disturbances)</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">fit1</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">price</span> <span class="o">~</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">cor</span><span class="p">(</span><span class="n">fit1</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">mpg</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] 8.338145e-17
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">cor</span><span class="p">(</span><span class="n">fit1</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">weight</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] 9.020833e-17
</code></pre>
<ol>
<li>The sum of the residuals is 0</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">sum</span><span class="p">(</span><span class="n">fit1</span><span class="o">$</span><span class="n">residuals</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] -3.637979e-12
</code></pre>
<ol>
<li>The sample mean of the residuals is 0</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">mean</span><span class="p">(</span><span class="n">fit1</span><span class="o">$</span><span class="n">residuals</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] -9.832375e-14
</code></pre>
<ol>
<li>The regression hyperplane passes through the means of the observed values <img alt="\\bar{X}" src="https://latex.codecogs.com/svg.latex?%5Cbar%7BX%7D" title="\\bar{X}"> and <img alt="\\bar{y}" src="https://latex.codecogs.com/svg.latex?%5Cbar%7By%7D" title="\\bar{y}">. We know this because we know that <img alt="\\bar{e} = 0" src="https://latex.codecogs.com/svg.latex?%5Cbar%7Be%7D%20%3D%200" title="\\bar{e} = 0"> and <img alt="e = y-X\\beta" src="https://latex.codecogs.com/svg.latex?e%20%3D%20y-X%5Cbeta" title="e = y-X\\beta">. If we divide by n - the number of observations, we&rsquo;re left with <img alt="0=\\bar{e}=\\bar{y}-\\bar{X}\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?0%3D%5Cbar%7Be%7D%3D%5Cbar%7By%7D-%5Cbar%7BX%7D%5Chat%7B%5Cbeta%7D" title="0=\\bar{e}=\\bar{y}-\\bar{X}\\hat{\\beta}"> therefore <img alt="\\bar{y}=\\bar{X}\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Cbar%7By%7D%3D%5Cbar%7BX%7D%5Chat%7B%5Cbeta%7D" title="\\bar{y}=\\bar{X}\\hat{\\beta}"></li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># We have a regression price = intercept + weight + mpg + error</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># mean of covariate mpg</span>
</span></span><span class="line"><span class="cl"><span class="nf">mean</span><span class="p">(</span><span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">mpg</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] 21.2973
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># mean of covariate weight</span>
</span></span><span class="line"><span class="cl"><span class="nf">mean</span><span class="p">(</span><span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">weight</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] 3019.459
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># mean of dependent variable price</span>
</span></span><span class="line"><span class="cl"><span class="nf">mean</span><span class="p">(</span><span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">price</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] 6165.257
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># show equal</span>
</span></span><span class="line"><span class="cl"><span class="nf">round</span><span class="p">(</span><span class="nf">mean</span><span class="p">(</span><span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">price</span><span class="p">),</span><span class="m">3</span><span class="p">)</span> <span class="o">==</span> <span class="nf">round</span><span class="p">(</span><span class="nf">predict</span><span class="p">(</span><span class="n">fit1</span><span class="p">,</span> <span class="n">newdata</span> <span class="o">=</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">weight</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">weight</span><span class="p">),</span> <span class="n">mpg</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">mpg</span><span class="p">))),</span> <span class="m">3</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>   1 
TRUE 
</code></pre>
<ol>
<li>The predicted values of y are uncorrelated with the residuals</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">cor</span><span class="p">(</span><span class="n">fit1</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">fit1</span><span class="o">$</span><span class="n">fitted.values</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] -5.475433e-17
</code></pre>
<ol>
<li>The mean of the predicted Y&rsquo;s for the sample will equal the mean of the observed y&rsquo;s</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">mean</span><span class="p">(</span><span class="n">fit1</span><span class="o">$</span><span class="n">fitted.values</span><span class="p">)</span> <span class="o">==</span> <span class="nf">mean</span><span class="p">(</span><span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span><span class="o">$</span><span class="n">price</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] TRUE
</code></pre>
<h2 id="showing-unbiasdness">Showing Unbiasdness</h2>
<p>What does this mean? That <img alt="E[\\hat{\\beta}]=\\beta" src="https://latex.codecogs.com/svg.latex?E%5B%5Chat%7B%5Cbeta%7D%5D%3D%5Cbeta" title="E[\\hat{\\beta}]=\\beta">. In words, our estimated <img alt="\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D" title="\\hat{\\beta}"> is equal to the population <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"></p>
<p>To begin, we start with our definition of <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> from above. We note that this requires the assumption of linearity in parameters AND no multicollinearity.</p>
<p><img alt="\\hat{\\beta} = (X'X)^{-1} X'y" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%20%3D%20%28X%27X%29%5E%7B-1%7D%20X%27y" title="\\hat{\\beta} = (X&#39;X)^{-1} X&#39;y"></p>
<p>Now we replace y with the definition of y (that is <img alt="y = X\\beta" src="https://latex.codecogs.com/svg.latex?y%20%3D%20X%5Cbeta" title="y = X\\beta"> + u). Note the lack of a hat, this is important!</p>
<p><img alt="\\hat{\\beta} = (X'X)^{-1} X'(X \\beta + u)" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%20%3D%20%28X%27X%29%5E%7B-1%7D%20X%27%28X%20%5Cbeta%20%2B%20u%29" title="\\hat{\\beta} = (X&#39;X)^{-1} X&#39;(X \\beta + u)"></p>
<p>Distribute the <img alt="(X'X)^{-1} X'" src="https://latex.codecogs.com/svg.latex?%28X%27X%29%5E%7B-1%7D%20X%27" title="(X&#39;X)^{-1} X&#39;"></p>
<p><img alt="\\hat{\\beta} = (X'X)^{-1} X'X\\beta + (X'X)^{-1} X'u" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%20%3D%20%28X%27X%29%5E%7B-1%7D%20X%27X%5Cbeta%20%2B%20%28X%27X%29%5E%7B-1%7D%20X%27u" title="\\hat{\\beta} = (X&#39;X)^{-1} X&#39;X\\beta + (X&#39;X)^{-1} X&#39;u"></p>
<p>Note that <img alt="(X'X)^{-1} X'X = I" src="https://latex.codecogs.com/svg.latex?%28X%27X%29%5E%7B-1%7D%20X%27X%20%3D%20I" title="(X&#39;X)^{-1} X&#39;X = I"></p>
<p><img alt="\\hat{\\beta} = I\\beta + (X'X)^{-1} X'u" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%20%3D%20I%5Cbeta%20%2B%20%28X%27X%29%5E%7B-1%7D%20X%27u" title="\\hat{\\beta} = I\\beta + (X&#39;X)^{-1} X&#39;u"></p>
<p>We&rsquo;re not done yet BUT we can see that unless <img alt="E[u|X]=0" src="https://latex.codecogs.com/svg.latex?E%5Bu%7CX%5D%3D0" title="E[u|X]=0">, <img alt="\\hat{\\beta} \\neq \\beta" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%20%5Cneq%20%5Cbeta" title="\\hat{\\beta} \\neq \\beta">.</p>
<p>Now, take conditional expectation of each side</p>
<p><img alt="E[\\hat{\\beta}|X] =  E[\\beta|X] + E[(X'X)^{-1} X'u|X]" src="https://latex.codecogs.com/svg.latex?E%5B%5Chat%7B%5Cbeta%7D%7CX%5D%20%3D%20%20E%5B%5Cbeta%7CX%5D%20%2B%20E%5B%28X%27X%29%5E%7B-1%7D%20X%27u%7CX%5D" title="E[\\hat{\\beta}|X] =  E[\\beta|X] + E[(X&#39;X)^{-1} X&#39;u|X]"></p>
<p><img alt="E[\\hat{\\beta}|X] =  \\beta + (X'X)^{-1} X'E[u|X]" src="https://latex.codecogs.com/svg.latex?E%5B%5Chat%7B%5Cbeta%7D%7CX%5D%20%3D%20%20%5Cbeta%20%2B%20%28X%27X%29%5E%7B-1%7D%20X%27E%5Bu%7CX%5D" title="E[\\hat{\\beta}|X] =  \\beta + (X&#39;X)^{-1} X&#39;E[u|X]"></p>
<p>As long as we have the zero conditional mean assumption <img alt="E[u|X]=0" src="https://latex.codecogs.com/svg.latex?E%5Bu%7CX%5D%3D0" title="E[u|X]=0"> then <img alt="E[\\hat{\\beta}|X]=\\beta" src="https://latex.codecogs.com/svg.latex?E%5B%5Chat%7B%5Cbeta%7D%7CX%5D%3D%5Cbeta" title="E[\\hat{\\beta}|X]=\\beta"></p>
<h1 id="properties-of-ols">Properties of OLS</h1>
<h2 id="regression-anatomy-or-frisch-waugh-lowell">Regression Anatomy or Frisch-Waugh-Lowell</h2>
<p>Regression anatomy theorem helps us interpret a single slope coefficient in a multiple regression model</p>
<p>Also, help us understand &ldquo;OLS&rdquo; as a &ldquo;matching estimator&rdquo; (try to compare observations that are alike in the Xs)</p>
<p>See <a href="https://econ.lse.ac.uk/staff/spischke/ec533/Griliches_measurement%20error.pdf">https://econ.lse.ac.uk/staff/spischke/ec533/Griliches_measurement%20error.pdf</a> and <a href="https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107">https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107</a></p>
<h2 id="what-is-it">What is it?</h2>
<p>This theorem tells us how to interpret a regression coefficient. Suppose we have a model</p>
<p><img alt="Y_{i} = \\alpha + \\beta_1 X_{1i} + \\beta_2X_{2i} + e_i" src="https://latex.codecogs.com/svg.latex?Y_%7Bi%7D%20%3D%20%5Calpha%20%2B%20%5Cbeta_1%20X_%7B1i%7D%20%2B%20%5Cbeta_2X_%7B2i%7D%20%2B%20e_i" title="Y_{i} = \\alpha + \\beta_1 X_{1i} + \\beta_2X_{2i} + e_i"></p>
<p>The theorem says that</p>
<p><img alt="\\beta_{1i}=\\frac{COV(Y_i,\\tilde{X}_{1i})}{VAR(\\tilde{X}_{1i})}" src="https://latex.codecogs.com/svg.latex?%5Cbeta_%7B1i%7D%3D%5Cfrac%7BCOV%28Y_i%2C%5Ctilde%7BX%7D_%7B1i%7D%29%7D%7BVAR%28%5Ctilde%7BX%7D_%7B1i%7D%29%7D" title="\\beta_{1i}=\\frac{COV(Y_i,\\tilde{X}_{1i})}{VAR(\\tilde{X}_{1i})}"></p>
<p>where <img alt="\\tilde{X}_{1i}" src="https://latex.codecogs.com/svg.latex?%5Ctilde%7BX%7D_%7B1i%7D" title="\\tilde{X}_{1i}"> are the residuals from a regression of <img alt="X_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7B1i%7D" title="X_{1i}"> on <img alt="X_{2i}" src="https://latex.codecogs.com/svg.latex?X_%7B2i%7D" title="X_{2i}">:</p>
<p><img alt="X_{i1}=\\pi_0+\\pi_1X_{2i}+\\tilde{X}_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7Bi1%7D%3D%5Cpi_0%2B%5Cpi_1X_%7B2i%7D%2B%5Ctilde%7BX%7D_%7B1i%7D" title="X_{i1}=\\pi_0+\\pi_1X_{2i}+\\tilde{X}_{1i}"></p>
<p>Suppose we care about <img alt="X_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7B1i%7D" title="X_{1i}"> it tells us that controlling for <img alt="X_{ki}" src="https://latex.codecogs.com/svg.latex?X_%7Bki%7D" title="X_{ki}"> for all <img alt="k \\neq 1" src="https://latex.codecogs.com/svg.latex?k%20%5Cneq%201" title="k \\neq 1"> that we&rsquo;re looking at just the variation in <img alt="y" src="https://latex.codecogs.com/svg.latex?y" title="y"> explained by <img alt="X_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7B1i%7D" title="X_{1i}"> and not by the variation in <img alt="X_{ki}" src="https://latex.codecogs.com/svg.latex?X_%7Bki%7D" title="X_{ki}">.</p>
<p>The auxillary regression of <img alt="X_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7B1i%7D" title="X_{1i}"> on <img alt="X_{2i}" src="https://latex.codecogs.com/svg.latex?X_%7B2i%7D" title="X_{2i}"> contains the term <img alt="\\tilde{X}_{1i}" src="https://latex.codecogs.com/svg.latex?%5Ctilde%7BX%7D_%7B1i%7D" title="\\tilde{X}_{1i}"> which partials out (i.e. removes) the influence of <img alt="X_{2i}" src="https://latex.codecogs.com/svg.latex?X_%7B2i%7D" title="X_{2i}"> on <img alt="X_{1i}" src="https://latex.codecogs.com/svg.latex?X_%7B1i%7D" title="X_{1i}"></p>
<p>We can also use it to explain the omitted variable bias formula (Short equals long plus the effect of omitted in long times the regression of omitted on included). Suppose we have two regressions <img alt="Y_{i} = \\beta_0 + \\beta_1 X_{1i} + e_i" src="https://latex.codecogs.com/svg.latex?Y_%7Bi%7D%20%3D%20%5Cbeta_0%20%2B%20%5Cbeta_1%20X_%7B1i%7D%20%2B%20e_i" title="Y_{i} = \\beta_0 + \\beta_1 X_{1i} + e_i"> and <img alt="Y_{i} = \\beta_0^* + \\beta_1^* X_{1i} + \\beta_2^*X_{2i} + v_i" src="https://latex.codecogs.com/svg.latex?Y_%7Bi%7D%20%3D%20%5Cbeta_0%5E%2A%20%2B%20%5Cbeta_1%5E%2A%20X_%7B1i%7D%20%2B%20%5Cbeta_2%5E%2AX_%7B2i%7D%20%2B%20v_i" title="Y_{i} = \\beta_0^* + \\beta_1^* X_{1i} + \\beta_2^*X_{2i} + v_i">, then <img alt="\\beta_1 = \\frac{Cov(X_{1i},Y_i)}{Var(X_i)}=\\beta_{1i}^*+\\gamma \\delta_{X_2X_1}" src="https://latex.codecogs.com/svg.latex?%5Cbeta_1%20%3D%20%5Cfrac%7BCov%28X_%7B1i%7D%2CY_i%29%7D%7BVar%28X_i%29%7D%3D%5Cbeta_%7B1i%7D%5E%2A%2B%5Cgamma%20%5Cdelta_%7BX_2X_1%7D" title="\\beta_1 = \\frac{Cov(X_{1i},Y_i)}{Var(X_i)}=\\beta_{1i}^*+\\gamma \\delta_{X_2X_1}"> where <img alt="\\delta_{X_2X_1}" src="https://latex.codecogs.com/svg.latex?%5Cdelta_%7BX_2X_1%7D" title="\\delta_{X_2X_1}"> is the regression of <img alt="X_2" src="https://latex.codecogs.com/svg.latex?X_2" title="X_2"> on <img alt="X_1" src="https://latex.codecogs.com/svg.latex?X_1" title="X_1">. Note that this is the opposite! order of regression anatomy. The auxillary regression in regression anatomy is <img alt="X_1" src="https://latex.codecogs.com/svg.latex?X_1" title="X_1"> on <img alt="X_2" src="https://latex.codecogs.com/svg.latex?X_2" title="X_2"></p>
<h2 id="why-is-it-useful">Why is it useful?</h2>
<p>We can break a multivariate regression with K regressors into K simpler bivariate models.</p>
<ol>
<li>
<p>It allows you to construct a bi-dimensional scatterplot of a dependent variable an independent variable of interest using the coefficients from a multiple regression. I.e. you run the regressions above and then plot the coefficient of interest from the multiple regression with the original <img alt="y_i" src="https://latex.codecogs.com/svg.latex?y_i" title="y_i"> on the y-axis and the residuals (<img alt="\\tilde{X}_{1i}" src="https://latex.codecogs.com/svg.latex?%5Ctilde%7BX%7D_%7B1i%7D" title="\\tilde{X}_{1i}">) on the x-axis.</p>
</li>
<li>
<p>It shows us why multicollinearity is a problem in regression - it means that most of the variation is between the regressors and not between the variable of interest (or the residual variable <img alt="\\tilde{X_{1i}}" src="https://latex.codecogs.com/svg.latex?%5Ctilde%7BX_%7B1i%7D%7D" title="\\tilde{X_{1i}}">) and the dependent variable. This means that the coefficient is unlikely to be significant because most of the variation between the multi-co-linear coefficients is between the coefficients and not between the coefficients and y.</p>
</li>
<li>
<p>We can use it in a multivariate OLS model to decompose the variance of each individual variable into three components (although we only really care about b and c):</p>
</li>
</ol>
<!-- -->
<ol>
<li>Variance not associated with y</li>
<li>Variance associated with y and shared with other regressors</li>
<li>Variance associated with y and not shared with other regressors</li>
</ol>
<p>When you construct an OLS model, the inclusion of a new regressor is valuable when the additional explaining power contained in it is not already fully captured by the other K regressors. Accordingly, the new variable must mainly provide the kind of variance denoted with (c). (<a href="https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107">https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107</a>)</p>
<h2 id="how-do-we-use-it">How do we use it?</h2>
<ol>
<li>Scatterplots and analysis of the contribution of each independent variable</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">sysuse</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># see https://github.com/scunning1975/mixtape/blob/master/R/reganat.R</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># load data</span>
</span></span><span class="line"><span class="cl"><span class="n">auto</span> <span class="o">&lt;-</span> <span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">mutate</span><span class="p">(</span><span class="n">length</span> <span class="o">=</span> <span class="n">length</span> <span class="o">-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">length</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># show regression anatomy</span>
</span></span><span class="line"><span class="cl"><span class="n">long_regression</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">length</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">headroom</span> <span class="o">+</span> <span class="n">mpg</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># below to show reg anatomy and FWL result in same estimate</span>
</span></span><span class="line"><span class="cl"><span class="n">long_without_coef_interest</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">headroom</span> <span class="o">+</span> <span class="n">mpg</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># we care about effect of length on price</span>
</span></span><span class="line"><span class="cl"><span class="n">x_residuals_regression</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">length</span> <span class="o">~</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">headroom</span> <span class="o">+</span> <span class="n">mpg</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x_residuals</span> <span class="o">&lt;-</span> <span class="n">x_residuals_regression</span><span class="o">$</span><span class="n">residuals</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># show regression anatomy - all are the same! So</span>
</span></span><span class="line"><span class="cl"><span class="n">long_regression</span><span class="o">$</span><span class="n">coefficients[2]</span>
</span></span></code></pre></div><pre><code>   length 
-94.49651 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># this is regression anatomy</span>
</span></span><span class="line"><span class="cl"><span class="nf">cov</span><span class="p">(</span><span class="n">auto</span><span class="o">$</span><span class="n">price</span><span class="p">,</span> <span class="n">x_residuals</span><span class="p">)</span><span class="o">/</span><span class="nf">var</span><span class="p">(</span><span class="n">x_residuals</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] -94.49651
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># this is also regression anatomy</span>
</span></span><span class="line"><span class="cl"><span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">x_residuals</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span><span class="o">$</span><span class="n">coefficients[2]</span>
</span></span></code></pre></div><pre><code>x_residuals 
  -94.49651 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">long_expressed_as_short</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">x_residuals</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># can also do Frisch–Waugh–Lovell.  It&#39;s regression anatomy but with the residuals from a regression of y on x2 and x3 (i.e without x1)</span>
</span></span><span class="line"><span class="cl"><span class="nf">cov</span><span class="p">(</span><span class="n">long_without_coef_interest</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">x_residuals</span><span class="p">)</span><span class="o">/</span><span class="nf">var</span><span class="p">(</span><span class="n">x_residuals</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] -94.49651
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># Now for the plotting!</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># show bias in short regression first</span>
</span></span><span class="line"><span class="cl"><span class="n">bivariate_incorrect</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">length</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># how do we plot?  For geom_smooth you need a y and an x that&#39;s linear.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the x&#39;s are just the x residuals for everything - so those are easy</span>
</span></span><span class="line"><span class="cl"><span class="c1"># the y&#39;s are different!  the main difference between the two is that short uses the bivariate coefficient (times the value of the data itself), and the long uses the &#39;correct&#39; coefficient from the long regression.  you could also use the coefficient from the bivariate regression of y on the residuals from the auxillary regression (the x1 on x2 through xn).  To keep the data consistent with the plotted points, we use the intercept from the bivariate regression of the y regressed on the residuals from the auxillary regression.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># want a whole bunch of x,y pairs to plot</span>
</span></span><span class="line"><span class="cl"><span class="n">short</span> <span class="o">&lt;-</span> <span class="nf">tibble</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">price</span> <span class="o">=</span> <span class="n">long_expressed_as_short</span><span class="o">$</span><span class="n">coefficients[1]</span> <span class="o">+</span> <span class="n">bivariate_incorrect</span><span class="o">$</span><span class="n">coefficients[2]</span> <span class="o">*</span> <span class="n">auto</span><span class="o">$</span><span class="n">length</span> <span class="p">,</span> 
</span></span><span class="line"><span class="cl">  <span class="n">x_residuals</span> <span class="o">=</span> <span class="n">x_residuals</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">long</span> <span class="o">&lt;-</span> <span class="nf">tibble</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">price</span> <span class="o">=</span> <span class="n">long_expressed_as_short</span><span class="o">$</span><span class="n">coefficients[1]</span> <span class="o">+</span> <span class="n">long_expressed_as_short</span><span class="o">$</span><span class="n">coefficients[2]</span> <span class="o">*</span> <span class="n">auto</span><span class="o">$</span><span class="n">length</span> <span class="p">,</span> 
</span></span><span class="line"><span class="cl">  <span class="n">x_residuals</span> <span class="o">=</span> <span class="n">x_residuals</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">  <span class="c1"># note could also have price as price = long_regression$coefficients[1] + long_expressed_as_short$coefficients[2] * auto$length </span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">auto</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_residuals</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">price</span><span class="p">))</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_smooth</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">short</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;blue&#34;</span><span class="p">,</span><span class="n">method</span> <span class="o">=</span> <span class="s">&#34;lm&#34;</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="bp">F</span><span class="p">)</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_smooth</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">long</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;red&#34;</span><span class="p">,</span><span class="n">method</span> <span class="o">=</span> <span class="s">&#34;lm&#34;</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="bp">F</span><span class="p">)</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">    <span class="nf">labs</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Regression Anatomy&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">caption</span> <span class="o">=</span> <span class="s">&#34;
</span></span></span><span class="line"><span class="cl"><span class="s">    We&#39;ve fixed these lines to have the same intercept \n and just let the slope vary to show the difference between the long and the short \n
</span></span></span><span class="line"><span class="cl"><span class="s">    The x, y pairs are the original y&#39;s and the x residuals \n
</span></span></span><span class="line"><span class="cl"><span class="s">    Blue is the effect of y on x1, not controlling for x2 \n
</span></span></span><span class="line"><span class="cl"><span class="s">    Red is the effect of y on x1 controlling for x2&#34;</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># To plot correctly for future use</span>
</span></span><span class="line"><span class="cl"><span class="n">auto</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">ggplot</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_residuals</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">price</span><span class="p">))</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">geom_smooth</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">long</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">&#34;red&#34;</span><span class="p">,</span><span class="n">method</span> <span class="o">=</span> <span class="s">&#34;lm&#34;</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="bp">F</span><span class="p">)</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">labs</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">title</span> <span class="o">=</span> <span class="s">&#34;Regression Anatomy&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">caption</span> <span class="o">=</span> <span class="s">&#34;Your long reference footnote goes in here&#34;</span><span class="p">)</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">theme_minimal</span><span class="p">()</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">  <span class="nf">theme</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">legend.position</span><span class="o">=</span><span class="s">&#34;none&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot.title</span> <span class="o">=</span> <span class="nf">element_text</span><span class="p">(</span><span class="n">hjust</span> <span class="o">=</span> <span class="m">0.5</span><span class="p">,</span> <span class="c1">#title</span>
</span></span><span class="line"><span class="cl">                                <span class="c1">#family = font,           #set font family</span>
</span></span><span class="line"><span class="cl">                                <span class="n">size</span> <span class="o">=</span> <span class="m">18</span><span class="p">,</span>                <span class="c1">#set font size</span>
</span></span><span class="line"><span class="cl">                                <span class="n">face</span> <span class="o">=</span> <span class="s">&#39;bold&#39;</span><span class="p">,</span>            <span class="c1">#bold typeface</span>
</span></span><span class="line"><span class="cl">                                <span class="n">vjust</span> <span class="o">=</span> <span class="m">0</span><span class="p">))</span>
</span></span></code></pre></div><img src="figs/FWL%20coding-1.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<img src="figs/FWL%20coding-2.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<ol>
<li>Contribution of the variance of each variable to the model</li>
</ol>
<p>For the above note that a partial correlation is computed between two residuals. A semipartial is computed between one residual and another raw or unresidualized variable.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">magrittr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#reganat price length mpg weight, dis(length) semip</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># change back to non-demeaned length so that we can check our results with https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107#:~:text=The%20regression%20anatomy%20theorem%20is,issue%20in%20time%2Dseries%20econometrics.</span>
</span></span><span class="line"><span class="cl"><span class="n">auto</span> <span class="o">&lt;-</span> <span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># need for semi partial correlation</span>
</span></span><span class="line"><span class="cl"><span class="n">long_regression</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">length</span> <span class="o">+</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># need for partial correlation</span>
</span></span><span class="line"><span class="cl"><span class="n">long_without_coef_interest</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># we care about effect of length on price</span>
</span></span><span class="line"><span class="cl"><span class="n">x_residuals_regression</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">length</span> <span class="o">~</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># regression anatomy works</span>
</span></span><span class="line"><span class="cl"><span class="c1"># bivariate &lt;- lm(formula = price ~ x_residuals_regression$residuals, data = auto)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#### PARTIAL AND SEMI-PARTIAL COEFFICIENTS ####</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># partial of price with length</span>
</span></span><span class="line"><span class="cl"><span class="n">partial</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">x_residuals_regression</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">long_without_coef_interest</span><span class="o">$</span><span class="n">residuals</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># semipartial correlations of price with length</span>
</span></span><span class="line"><span class="cl"><span class="n">semi_partial</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">x_residuals_regression</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">auto</span><span class="o">$</span><span class="n">price</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># the below is the contribution to the r^2 or the variance explained by x1</span>
</span></span><span class="line"><span class="cl"><span class="n">semi_partial^2</span>
</span></span></code></pre></div><pre><code>[1] 0.06398732
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># make sure first x is the coef of interest</span>
</span></span><span class="line"><span class="cl"><span class="c1"># see page 14 https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300107</span>
</span></span><span class="line"><span class="cl"><span class="n">importance_of_coef</span> <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">xvars</span><span class="p">,</span><span class="kc">...</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># select variables</span>
</span></span><span class="line"><span class="cl">  <span class="n">df</span> <span class="o">%&lt;&gt;%</span>
</span></span><span class="line"><span class="cl">    <span class="nf">select</span><span class="p">(</span><span class="nf">all_of</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="nf">all_of</span><span class="p">(</span><span class="n">xvars</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">y_numeric</span> <span class="o">&lt;-</span> <span class="n">df</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">    <span class="nf">select</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">pull</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1"># run long regression</span>
</span></span><span class="line"><span class="cl">  <span class="n">long_formula</span> <span class="o">&lt;-</span> <span class="nf">as.formula</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="nf">paste</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nf">paste</span><span class="p">(</span><span class="n">xvars</span><span class="p">,</span> <span class="n">collapse</span> <span class="o">=</span> <span class="s">&#34; + &#34;</span><span class="p">),</span> <span class="n">sep</span> <span class="o">=</span> <span class="s">&#34; ~ &#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1"># Saturated model </span>
</span></span><span class="line"><span class="cl">  <span class="n">long_regression</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">long_formula</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1"># Saturated model without coef of interest</span>
</span></span><span class="line"><span class="cl">  <span class="n">long_formula_without_coef_interest</span> <span class="o">&lt;-</span> <span class="nf">as.formula</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="nf">paste</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nf">paste</span><span class="p">(</span><span class="n">xvars[</span><span class="m">-1</span><span class="n">]</span><span class="p">,</span> <span class="n">collapse</span> <span class="o">=</span> <span class="s">&#34; + &#34;</span><span class="p">),</span> <span class="n">sep</span> <span class="o">=</span> <span class="s">&#34; ~ &#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1"># long residuals without coefficient of interest</span>
</span></span><span class="line"><span class="cl">  <span class="n">long_residuals_wo_ci</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">long_formula_without_coef_interest</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1"># X residuals</span>
</span></span><span class="line"><span class="cl">  <span class="n">x_residuals_formula</span> <span class="o">&lt;-</span> <span class="nf">as.formula</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="nf">paste</span><span class="p">(</span><span class="n">xvars[1]</span><span class="p">,</span> <span class="nf">paste</span><span class="p">(</span><span class="n">xvars[</span><span class="m">-1</span><span class="n">]</span><span class="p">,</span> <span class="n">collapse</span> <span class="o">=</span> <span class="s">&#34; + &#34;</span><span class="p">),</span> <span class="n">sep</span> <span class="o">=</span> <span class="s">&#34; ~ &#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">x_residuals</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">x_residuals_formula</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">partial</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">x_residuals</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">long_residuals_wo_ci</span><span class="o">$</span><span class="n">residuals</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">semi_partial</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">x_residuals</span><span class="o">$</span><span class="n">residuals</span><span class="p">,</span> <span class="n">y_numeric</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="n">semi_partial_squared</span> <span class="o">&lt;-</span> <span class="n">semi_partial^2</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">   <span class="c1">#Under normal conditions, the sum of the squared semipartials can be subtracted from the overall R2 for the complete OLS regression to get the value of common variance shared by the independent variables with y.</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># the semi-partial formula here doens&#39;t work</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># numerator &lt;- summary(long_regression)$r.squared - summary(long_residuals_wo_ci)$r.squared</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># denominator &lt;- (1 - summary(long_regression)$r.squared)*(length(summary(long_regression)$residuals)-summary(long_regression)$df[1]-1)</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># contribution &lt;- numerator/denominator</span>
</span></span><span class="line"><span class="cl">  <span class="c1">#   contribution</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">  <span class="c1">#INTERPRETATION - semipartial squared is the variance explained by an x individually</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># if you sum up the semi-partials of all the x&#39;s, you get all of their individual contributions to the model</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># if you then subtract that number from the overall r^2 you get the amount of variance common to the x&#39;s </span>
</span></span><span class="line"><span class="cl">  <span class="kr">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">partial</span> <span class="o">=</span> <span class="n">partial</span><span class="p">,</span> <span class="n">semi_partial</span> <span class="o">=</span> <span class="n">semi_partial</span><span class="p">,</span> <span class="n">semi_partial_squared</span> <span class="o">=</span> <span class="n">semi_partial_squared</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">lenth_cont</span> <span class="o">&lt;-</span> <span class="nf">importance_of_coef</span><span class="p">(</span><span class="n">auto</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s">&#34;price&#34;</span><span class="p">,</span> <span class="n">xvars</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;length&#34;</span><span class="p">,</span><span class="s">&#34;weight&#34;</span><span class="p">,</span><span class="s">&#34;mpg&#34;</span><span class="p">))</span><span class="o">$</span><span class="n">semi_partial_squared</span>
</span></span><span class="line"><span class="cl"><span class="n">weight_cont</span> <span class="o">&lt;-</span> <span class="nf">importance_of_coef</span><span class="p">(</span><span class="n">auto</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s">&#34;price&#34;</span><span class="p">,</span> <span class="n">xvars</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;weight&#34;</span><span class="p">,</span><span class="s">&#34;length&#34;</span><span class="p">,</span><span class="s">&#34;mpg&#34;</span><span class="p">))</span><span class="o">$</span><span class="n">semi_partial_squared</span>
</span></span><span class="line"><span class="cl"><span class="n">mpg_cont</span> <span class="o">&lt;-</span> <span class="nf">importance_of_coef</span><span class="p">(</span><span class="n">auto</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s">&#34;price&#34;</span><span class="p">,</span> <span class="n">xvars</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;mpg&#34;</span><span class="p">,</span><span class="s">&#34;weight&#34;</span><span class="p">,</span><span class="s">&#34;length&#34;</span><span class="p">))</span><span class="o">$</span><span class="n">semi_partial_squared</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Variance of x&#39;s individually</span>
</span></span><span class="line"><span class="cl"><span class="n">lenth_cont</span> <span class="o">+</span> <span class="n">weight_cont</span> <span class="o">+</span> <span class="n">mpg_cont</span>
</span></span></code></pre></div><pre><code>[1] 0.2021244
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># Variance common to the x&#39;s</span>
</span></span><span class="line"><span class="cl"><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">price</span> <span class="o">~</span> <span class="n">length</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">mpg</span><span class="p">,</span> <span class="n">auto</span><span class="p">))</span><span class="o">$</span><span class="n">r.squared</span> <span class="o">-</span> <span class="p">(</span><span class="n">lenth_cont</span> <span class="o">+</span> <span class="n">weight_cont</span> <span class="o">+</span> <span class="n">mpg_cont</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] 0.155252
</code></pre>
<h1 id="omitted-variable-bias-ovb">Omitted Variable Bias (OVB)</h1>
<h2 id="what-is-it-1">What is it?</h2>
<p>Suppose you have a short regression with K regressors and a long regression with K + n regressors where n is less than the number of observations in your sample:</p>
<p>Short equals long plus the effect(s) of omitted times the regression(s) of omitted on included, all computed in a model maintaining the set of controls included in both short and long</p>
<h2 id="why-is-it-useful-1">Why is it useful?</h2>
<p>It shows us the effect of omitting variables from our regression.</p>
<h2 id="how-do-we-use-it-1">How do we use it?</h2>
<p>It shows the impact of omitting variables in a regression. It&rsquo;s mostly theoretical since you&rsquo;d use variables if you had them!</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># see http://www.masteringmetrics.com/wp-content/uploads/2020/07/lny20n08MRU_R2.pdf</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">auto</span> <span class="o">&lt;-</span> <span class="n">sysuse</span><span class="o">::</span><span class="n">auto</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">short</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">mpg</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">long</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">length</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">ommitted_on_included</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">length</span> <span class="o">~</span> <span class="n">mpg</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#Show this works</span>
</span></span><span class="line"><span class="cl"><span class="n">short</span><span class="o">$</span><span class="n">coefficients[2]</span>
</span></span></code></pre></div><pre><code>      mpg 
-238.8943 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">long</span><span class="o">$</span><span class="n">coefficients[2]</span> <span class="o">+</span> <span class="n">ommitted_on_included</span><span class="o">$</span><span class="n">coefficients[2]</span> <span class="o">*</span> <span class="n">long</span><span class="o">$</span><span class="n">coefficients[3]</span>
</span></span></code></pre></div><pre><code>      mpg 
-238.8943 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># round(as.numeric(short$coefficients[2]),5) == round(as.numeric(long$coefficients[2]) + as.numeric(ommitted_on_included$coefficients[2]) * as.numeric(long$coefficients[3]),5)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Now do OVB for a regression with more than 2 variables</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">short</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">long</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">price</span> <span class="o">~</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">length</span> <span class="o">+</span> <span class="n">trunk</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Need to do two of these regressions! then take effect of MPG on each</span>
</span></span><span class="line"><span class="cl"><span class="n">ommitted_on_included_1</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">length</span> <span class="o">~</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ommitted_on_included_2</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">trunk</span> <span class="o">~</span> <span class="n">mpg</span> <span class="o">+</span> <span class="n">weight</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">auto</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Coefficient on MPG in short </span>
</span></span><span class="line"><span class="cl"><span class="n">short</span><span class="o">$</span><span class="n">coefficients[2]</span>
</span></span></code></pre></div><pre><code>      mpg 
-49.51222 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">long</span><span class="o">$</span><span class="n">coefficients[2]</span> <span class="o">+</span> <span class="n">ommitted_on_included_1</span><span class="o">$</span><span class="n">coefficients[2]</span> <span class="o">*</span> <span class="n">long</span><span class="o">$</span><span class="n">coefficients[4]</span> <span class="o">+</span> <span class="n">ommitted_on_included_2</span><span class="o">$</span><span class="n">coefficients[2]</span> <span class="o">*</span> <span class="n">long</span><span class="o">$</span><span class="n">coefficients[5]</span>
</span></span></code></pre></div><pre><code>      mpg 
-49.51222 
</code></pre>
<h1 id="the-hat-matrix">The Hat Matrix!</h1>
<p>This stuff from <a href="https://allmodelsarewrong.github.io/ols.html">https://allmodelsarewrong.github.io/ols.html</a></p>
<p>We know that <img alt="\\beta = (X'X)^{-1}X'Y" src="https://latex.codecogs.com/svg.latex?%5Cbeta%20%3D%20%28X%27X%29%5E%7B-1%7DX%27Y" title="\\beta = (X&#39;X)^{-1}X&#39;Y">. We can use this to show how we predict <img alt="\\hat{y}" src="https://latex.codecogs.com/svg.latex?%5Chat%7By%7D" title="\\hat{y}">. We know that <img alt="\\hat{y}=X\\beta + e" src="https://latex.codecogs.com/svg.latex?%5Chat%7By%7D%3DX%5Cbeta%20%2B%20e" title="\\hat{y}=X\\beta + e">. Substitute in <img alt="\\beta = (X'X)^{-1}X'Y" src="https://latex.codecogs.com/svg.latex?%5Cbeta%20%3D%20%28X%27X%29%5E%7B-1%7DX%27Y" title="\\beta = (X&#39;X)^{-1}X&#39;Y"> into the previous equation such that</p>
<p><img alt="\\hat{y}=X(X'X)^{-1}X'Y" src="https://latex.codecogs.com/svg.latex?%5Chat%7By%7D%3DX%28X%27X%29%5E%7B-1%7DX%27Y" title="\\hat{y}=X(X&#39;X)^{-1}X&#39;Y"></p>
<p>If we call <img alt="H=X(X'X)^{-1}X'" src="https://latex.codecogs.com/svg.latex?H%3DX%28X%27X%29%5E%7B-1%7DX%27" title="H=X(X&#39;X)^{-1}X&#39;">, then we can say <img alt="\\hat{y}=Hy" src="https://latex.codecogs.com/svg.latex?%5Chat%7By%7D%3DHy" title="\\hat{y}=Hy"></p>
<p>It makes hats, what else does it do? It allows us to detect leverage points. A zero value, <img alt="H_{ii} = 0" src="https://latex.codecogs.com/svg.latex?H_%7Bii%7D%20%3D%200" title="H_{ii} = 0">, indicates a point which has no influence on prediction.</p>
<p>Points are high leverage if their eigen value is greater than <img alt="2\\sum_n^i h_{ii}/n" src="https://latex.codecogs.com/svg.latex?2%5Csum_n%5Ei%20h_%7Bii%7D%2Fn" title="2\\sum_n^i h_{ii}/n"></p>
<p>Show the leverage of any point</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl">    <span class="n">fit</span> <span class="o">=</span> <span class="nf">lm</span><span class="p">(</span><span class="n">mpg</span> <span class="o">~</span> <span class="n">wt</span><span class="p">,</span> <span class="n">mtcars</span><span class="p">)</span> <span class="c1"># OLS including all points</span>
</span></span><span class="line"><span class="cl">    <span class="n">X</span> <span class="o">=</span> <span class="nf">model.matrix</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span> <span class="c1"># X model matrix</span>
</span></span><span class="line"><span class="cl">    <span class="n">hat_matrix</span> <span class="o">=</span> <span class="n">X</span><span class="o">%*%</span><span class="p">(</span><span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">%*%</span><span class="n">X</span><span class="p">)</span><span class="o">%*%</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="c1"># Hat matrix</span>
</span></span><span class="line"><span class="cl">    <span class="nf">diag</span><span class="p">(</span><span class="n">hat_matrix</span><span class="p">)</span><span class="n">[1]</span> <span class="c1"># First diagonal point in Hat matrix</span>
</span></span></code></pre></div><pre><code> Mazda RX4 
0.04326896 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl">    <span class="n">fitwithout1</span> <span class="o">=</span> <span class="nf">lm</span><span class="p">(</span><span class="n">mpg</span> <span class="o">~</span> <span class="n">wt</span><span class="p">,</span> <span class="n">mtcars[</span><span class="m">-1</span><span class="p">,</span><span class="n">]</span><span class="p">)</span> <span class="c1"># OLS excluding first data point.</span>
</span></span><span class="line"><span class="cl">    <span class="n">new</span> <span class="o">=</span> <span class="nf">data.frame</span><span class="p">(</span><span class="n">wt</span><span class="o">=</span><span class="n">mtcars[1</span><span class="p">,</span><span class="s">&#39;wt&#39;</span><span class="n">]</span><span class="p">)</span> <span class="c1"># Predicting y hat in this OLS w/o first point.</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_hat_without</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">fitwithout1</span><span class="p">,</span> <span class="n">newdata</span><span class="o">=</span><span class="n">new</span><span class="p">)</span> <span class="c1"># ... here it is.</span>
</span></span><span class="line"><span class="cl">    <span class="nf">residuals</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="n">[1]</span> <span class="c1"># The residual when OLS includes data point.</span>
</span></span></code></pre></div><pre><code>Mazda RX4 
-2.282611 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl">    <span class="n">lev</span> <span class="o">=</span> <span class="m">1</span> <span class="o">-</span> <span class="p">(</span><span class="nf">residuals</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="n">[1]</span><span class="o">/</span><span class="p">(</span><span class="n">mtcars[1</span><span class="p">,</span><span class="s">&#39;mpg&#39;</span><span class="n">]</span> <span class="o">-</span>  <span class="n">y_hat_without</span><span class="p">))</span> <span class="c1"># Leverage</span>
</span></span><span class="line"><span class="cl">    <span class="nf">all.equal</span><span class="p">(</span><span class="nf">diag</span><span class="p">(</span><span class="n">hat_matrix</span><span class="p">)</span><span class="n">[1]</span><span class="p">,</span><span class="n">lev</span><span class="p">)</span> <span class="c1">#TRUE</span>
</span></span></code></pre></div><pre><code>[1] TRUE
</code></pre>
<h1 id="interpretation">Interpretation</h1>
<p><strong>No logged DV, no logged IV</strong> A one unit increase in x, increases y by coefficient (<img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta">) units</p>
<ul>
<li>Suppose our <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is 5. The interpretation is &ldquo;a one unit increase in x increases the dependent variable by 5 units.&rdquo; Or, a one standard deviation increase in x increases the dependent variable by <img alt="\\beta \\times \\text{sd}(x)" src="https://latex.codecogs.com/svg.latex?%5Cbeta%20%5Ctimes%20%5Ctext%7Bsd%7D%28x%29" title="\\beta \\times \\text{sd}(x)"> units.</li>
<li>Suppose our <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is <img alt="-5" src="https://latex.codecogs.com/svg.latex?-5" title="-5">. The interpretation is &ldquo;a one unit increase in x decreases the dependent variable by 5 units.&rdquo; Or, a one standard deviation increase in x decreases the dependent variable by <img alt="\\beta \\times \\text{sd}(x)" src="https://latex.codecogs.com/svg.latex?%5Cbeta%20%5Ctimes%20%5Ctext%7Bsd%7D%28x%29" title="\\beta \\times \\text{sd}(x)"> units.</li>
</ul>
<p><strong>No logged DV, logged IV</strong> Divide the coefficient by 100, this tells us that a 1% increase in the independent variable increases (decreases) the dependent variable by coefficient/100 units. Example: the coefficient is 0.198. 0.198/100 = 0.00198. For every 1% increase in the independent variable, our dependent variable increases by about 0.002. For an 10% increase multiply the coefficient by log(1.1) (for x increase log(1.x)). Example: For every 10% increase in the independent variable, our dependent variable increases by about <img alt="0.198 \\times \\log(1.10) = 0.02" src="https://latex.codecogs.com/svg.latex?0.198%20%5Ctimes%20%5Clog%281.10%29%20%3D%200.02" title="0.198 \\times \\log(1.10) = 0.02">.</p>
<ul>
<li>
<p>Suppose our <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is 5. The interpretation is that a one percent increase in x increases the dependent variable by 0.05. For every 10% increase in the independent variable, the dependent variable increases by <img alt="5 \\times \\log(1.1) = 0.48" src="https://latex.codecogs.com/svg.latex?5%20%5Ctimes%20%5Clog%281.1%29%20%3D%200.48" title="5 \\times \\log(1.1) = 0.48"></p>
</li>
<li>
<p>Suppose our <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is <img alt="-5" src="https://latex.codecogs.com/svg.latex?-5" title="-5">. A one percent increase in x decreases the dependent variable by 0.05 units. For every 10% increase in x, the dependent variable decreases by <img alt="5 \\times \\log(1.1) = 0.48" src="https://latex.codecogs.com/svg.latex?5%20%5Ctimes%20%5Clog%281.1%29%20%3D%200.48" title="5 \\times \\log(1.1) = 0.48"> units</p>
</li>
</ul>
<p><strong>Logged DV, No logged IV</strong> Exponentiate the coefficient. This gives the multiplicative factor for every one-unit increase in the independent variable. Example: the coefficient is 0.198. exp(0.198) = 1.218962. For every one-unit increase in the independent variable, our dependent variable increases by a factor of about 1.22, or 22%. Recall that multiplying a number by 1.22 is the same as increasing the number by 22%. A shortcut is to exponentiate the coefficient, then subtract one from this number, and multiply by 100 (i.e. <img alt="(e^\\beta-1)\\times100" src="https://latex.codecogs.com/svg.latex?%28e%5E%5Cbeta-1%29%5Ctimes100" title="(e^\\beta-1)\\times100">). This gives the percent increase (or decrease) in the y for every one-unit increase in the independent variable. &ldquo;In summary, when the outcome variable is log transformed, it is natural to interpret the exponentiated regression coefficients. These values correspond to changes in the ratio of the expected geometric means of the original outcome variable.&rdquo;</p>
<ul>
<li>
<p>Suppose our <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is 5. A one unit increase in x increases the dependent variable by a factor of <img alt="\\exp(5)=148.4" src="https://latex.codecogs.com/svg.latex?%5Cexp%285%29%3D148.4" title="\\exp(5)=148.4"> or <img alt="14,740\\" src="https://latex.codecogs.com/svg.latex?14%2C740%5C%25" title="14,740\\%"> (remember to subtract the one from the beta).</p>
</li>
<li>
<p>Suppose our <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is <img alt="-5" src="https://latex.codecogs.com/svg.latex?-5" title="-5">. A one unit increase in x decreases the dependent variable by a factor of <img alt="\\exp(-5)=0.007" src="https://latex.codecogs.com/svg.latex?%5Cexp%28-5%29%3D0.007" title="\\exp(-5)=0.007"> or 99.32% <img alt="((\\exp(-5)-1) \\times 100)" src="https://latex.codecogs.com/svg.latex?%28%28%5Cexp%28-5%29-1%29%20%5Ctimes%20100%29" title="((\\exp(-5)-1) \\times 100)"></p>
</li>
</ul>
<p><strong>Logged DV, logged IV</strong> Interpret the coefficient as the percent increase in the dependent variable for every 1% increase in the independent variable. Suppose the coefficient is 0.198. For every 1% increase in the independent variable, the dependent variable increases by 0.2%. Suppose, instead, we wanted an x percent increase - use the formula <img alt="(1.x^{0.198} &ndash; 1) * 100" src="https://latex.codecogs.com/svg.latex?%281.x%5E%7B0.198%7D%20%E2%80%93%201%29%20%2A%20100" title="(1.x^{0.198} – 1) * 100">. For example, a 10% increase in our independent variable increases our dependent variable by <img alt="(1.1^{0.198} - 1) \\times 100 = 1.91\\" src="https://latex.codecogs.com/svg.latex?%281.1%5E%7B0.198%7D%20-%201%29%20%5Ctimes%20100%20%3D%201.91%5C%25" title="(1.1^{0.198} - 1) \\times 100 = 1.91\\%"></p>
<ul>
<li>
<p>Suppose our <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is <img alt="5" src="https://latex.codecogs.com/svg.latex?5" title="5">. A 1% increase in the independent variable increases the dependent variably by 5%.</p>
</li>
<li>
<p>Suppose our <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is <img alt="-5" src="https://latex.codecogs.com/svg.latex?-5" title="-5">. A 1% increase in the independent variable decreases the dependent variably by 5%.</p>
</li>
</ul>
<h3 id="why-this-interpretation-for-logged-dependent-variable-values">Why this interpretation for logged dependent variable values?</h3>
<p>&ldquo;Our independent variable has a multiplicative relationship with our dependent variable instead of the usual additive relationship. Hence the need to express the effect of a one-unit change in x on y as a percent.&rdquo;</p>
<p><img alt="\\log(y)=\\beta_0+\\beta_1x" src="https://latex.codecogs.com/svg.latex?%5Clog%28y%29%3D%5Cbeta_0%2B%5Cbeta_1x" title="\\log(y)=\\beta_0+\\beta_1x"></p>
<p><img alt="\\exp(\\log(y))=\\exp(\\beta_0+\\beta_1x)" src="https://latex.codecogs.com/svg.latex?%5Cexp%28%5Clog%28y%29%29%3D%5Cexp%28%5Cbeta_0%2B%5Cbeta_1x%29" title="\\exp(\\log(y))=\\exp(\\beta_0+\\beta_1x)"></p>
<p><img alt="y=\\exp(\\beta_0+\\beta_1x)" src="https://latex.codecogs.com/svg.latex?y%3D%5Cexp%28%5Cbeta_0%2B%5Cbeta_1x%29" title="y=\\exp(\\beta_0+\\beta_1x)"></p>
<p><img alt="y=\\exp(\\beta_0)\\times(\\beta_1x)" src="https://latex.codecogs.com/svg.latex?y%3D%5Cexp%28%5Cbeta_0%29%5Ctimes%28%5Cbeta_1x%29" title="y=\\exp(\\beta_0)\\times(\\beta_1x)"></p>
<h3 id="logged-independent-variable">Logged Independent Variable</h3>
<p><img alt="y=\\beta_0 + \\beta_1\\log(x_1) + \\beta_2(x_2)" src="https://latex.codecogs.com/svg.latex?y%3D%5Cbeta_0%20%2B%20%5Cbeta_1%5Clog%28x_1%29%20%2B%20%5Cbeta_2%28x_2%29" title="y=\\beta_0 + \\beta_1\\log(x_1) + \\beta_2(x_2)"></p>
<p><img alt="e^y=e^{\\beta_0 + \\beta_1\\log(x_1) + \\beta_2(x_2)}" src="https://latex.codecogs.com/svg.latex?e%5Ey%3De%5E%7B%5Cbeta_0%20%2B%20%5Cbeta_1%5Clog%28x_1%29%20%2B%20%5Cbeta_2%28x_2%29%7D" title="e^y=e^{\\beta_0 + \\beta_1\\log(x_1) + \\beta_2(x_2)}"></p>
<p><img alt="e^y=e^{\\beta_0 + \\beta_1\\log(x_1) + \\beta_2(x_2)}" src="https://latex.codecogs.com/svg.latex?e%5Ey%3De%5E%7B%5Cbeta_0%20%2B%20%5Cbeta_1%5Clog%28x_1%29%20%2B%20%5Cbeta_2%28x_2%29%7D" title="e^y=e^{\\beta_0 + \\beta_1\\log(x_1) + \\beta_2(x_2)}"></p>
<p><img alt="e^y=e^{\\beta_0}\\times e^{\\beta_1\\log(x_1)} \\times e^ {\\beta_2(x_2)}" src="https://latex.codecogs.com/svg.latex?e%5Ey%3De%5E%7B%5Cbeta_0%7D%5Ctimes%20e%5E%7B%5Cbeta_1%5Clog%28x_1%29%7D%20%5Ctimes%20e%5E%20%7B%5Cbeta_2%28x_2%29%7D" title="e^y=e^{\\beta_0}\\times e^{\\beta_1\\log(x_1)} \\times e^ {\\beta_2(x_2)}"></p>
<p><img alt="e^y=e^{\\beta_0}\\times e^{\\beta_1\\log(x_1)} \\times e^ {\\beta_2(x_2)}" src="https://latex.codecogs.com/svg.latex?e%5Ey%3De%5E%7B%5Cbeta_0%7D%5Ctimes%20e%5E%7B%5Cbeta_1%5Clog%28x_1%29%7D%20%5Ctimes%20e%5E%20%7B%5Cbeta_2%28x_2%29%7D" title="e^y=e^{\\beta_0}\\times e^{\\beta_1\\log(x_1)} \\times e^ {\\beta_2(x_2)}"></p>
<p><img alt="e^y=e^{\\beta_0}\\times x_1^{\\beta_1} \\times e^ {\\beta_2(x_2)}" src="https://latex.codecogs.com/svg.latex?e%5Ey%3De%5E%7B%5Cbeta_0%7D%5Ctimes%20x_1%5E%7B%5Cbeta_1%7D%20%5Ctimes%20e%5E%20%7B%5Cbeta_2%28x_2%29%7D" title="e^y=e^{\\beta_0}\\times x_1^{\\beta_1} \\times e^ {\\beta_2(x_2)}"></p>
<p>Source</p>
<ul>
<li>
<p><a href="https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/">https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/</a></p>
</li>
<li>
<p><a href="https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/">https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/</a></p>
</li>
<li>
<p><a href="https://www.statalist.org/forums/forum/general-stata-discussion/general/1362222-interpretation-of-coefficients-for-log-transformed-dependent-variable-panel-data">https://www.statalist.org/forums/forum/general-stata-discussion/general/1362222-interpretation-of-coefficients-for-log-transformed-dependent-variable-panel-data</a></p>
</li>
</ul>
<h1 id="ols-diagnostics">OLS Diagnostics</h1>
<p>So, you have estimates and you understand the estimates. Great. Now let&rsquo;s see where the models are wrong. Run the command plot() after OLS and you&rsquo;ll get the following plots that help with regression diagnostics.</p>
<ol>
<li>
<p>Residuals vs Fitted - checks linear relationship assumption of linear regression. A linear relationship will demonstrate a horizontal red line here. Deviations from a horizontal line suggest nonlinearity and that a different approach may be necessary.</p>
</li>
<li>
<p>Normal Q-Q - checks whether or not the residuals (the difference between the observed and predicted values) from the model are normally distributed. The best fit models points fall along the dashed line on the plot. Deviation from this line suggests that a different analytical approach may be required.</p>
</li>
<li>
<p>Scale-Location - checks the homoscedasticity of the model. A horizontal red line with points equally spread out indicates a well-fit model. A non-horizontal line or points that cluster together suggests that your data are not homoscedastic.</p>
</li>
<li>
<p>Residuals vs Leverage - helps to identify outlier or extreme values that may disproportionately affect the model&rsquo;s results. Their inclusion or exclusion from the analysis may affect the results of the analysis. Note that the top three most extreme values are identified with numbers next to the points in all four plots.</p>
</li>
</ol>
<p>Sources: - <a href="https://sscc.wisc.edu/sscc/pubs/RegDiag-R/linearity.html">https://sscc.wisc.edu/sscc/pubs/RegDiag-R/linearity.html</a></p>
<ul>
<li><a href="https://jhudatascience.org/tidyversecourse/model.html#linear-modeling">https://jhudatascience.org/tidyversecourse/model.html#linear-modeling</a></li>
</ul>
<h1 id="lots-of-fixed-effects">Lots of Fixed Effects</h1>
<p>Sometimes we have a model with a lot (i.e. more than 50) fixed effects. Base R handles these models poorly as it computes the fixed effects individually. There&rsquo;s a way around this - demeaning all the independent variables or using the PLM package.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">tidyr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">broom</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">&lt;-</span> 
</span></span><span class="line"><span class="cl"><span class="nf">structure</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">6</span><span class="p">,</span> <span class="m">2.3</span><span class="p">,</span> <span class="m">4</span><span class="p">,</span> <span class="m">5.7</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">4.5</span><span class="p">,</span> <span class="m">6</span><span class="p">),</span> <span class="n">x2</span> <span class="o">=</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">9</span><span class="p">),</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">4.4</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl"><span class="m">6.8</span><span class="p">,</span> <span class="m">9.2</span><span class="p">,</span> <span class="m">0.54</span><span class="p">,</span> <span class="m">1.9</span><span class="p">,</span> <span class="m">3.26</span><span class="p">,</span> <span class="m">-6.92</span><span class="p">,</span> <span class="m">-5.495</span><span class="p">,</span> <span class="m">-4.07</span><span class="p">),</span> <span class="n">g</span> <span class="o">=</span> <span class="nf">structure</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">1L</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl"><span class="m">1L</span><span class="p">,</span> <span class="m">1L</span><span class="p">,</span> <span class="m">2L</span><span class="p">,</span> <span class="m">2L</span><span class="p">,</span> <span class="m">2L</span><span class="p">,</span> <span class="m">3L</span><span class="p">,</span> <span class="m">3L</span><span class="p">,</span> <span class="m">3L</span><span class="p">),</span> <span class="n">.Label</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;a&#34;</span><span class="p">,</span> <span class="s">&#34;b&#34;</span><span class="p">,</span> <span class="s">&#34;c&#34;</span><span class="p">),</span> <span class="n">class</span> <span class="o">=</span> <span class="s">&#34;factor&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">3</span><span class="p">),</span> <span class="n">x3</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">0</span><span class="p">)),</span> <span class="n">class</span> <span class="o">=</span> <span class="s">&#34;data.frame&#34;</span><span class="p">,</span> <span class="n">row.names</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="kc">NA</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl"><span class="m">-9L</span><span class="p">))</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">group_by</span><span class="p">(</span> <span class="n">g</span> <span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">mutate</span><span class="p">(</span> 
</span></span><span class="line"><span class="cl">    <span class="n">mean.x</span><span class="o">=</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">    <span class="n">x.demeaned</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mean.x</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">mean.y</span><span class="o">=</span><span class="nf">mean</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">    <span class="n">y.demeaned</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">mean.y</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">mean.x2</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">x2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">x2.demeaned</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">mean.x2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">mean.x3</span> <span class="o">=</span> <span class="nf">mean</span><span class="p">(</span><span class="n">x3</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">x3.demeaned</span> <span class="o">=</span> <span class="n">x3</span> <span class="o">-</span> <span class="n">mean.x3</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">ungroup</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">palette</span><span class="p">(</span> <span class="nf">c</span><span class="p">(</span> <span class="s">&#34;steelblue&#34;</span><span class="p">,</span> <span class="s">&#34;darkred&#34;</span><span class="p">,</span> <span class="s">&#34;darkgreen&#34;</span>  <span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">plot</span><span class="p">(</span> <span class="n">data</span><span class="o">$</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">y</span><span class="p">,</span> <span class="n">bty</span><span class="o">=</span><span class="s">&#34;n&#34;</span><span class="p">,</span> <span class="n">pch</span><span class="o">=</span><span class="m">19</span><span class="p">,</span> <span class="n">cex</span><span class="o">=</span><span class="m">3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span><span class="m">10</span><span class="p">),</span> <span class="n">xlim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">8</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">      <span class="n">col</span> <span class="o">=</span> <span class="nf">as.factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">g</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">      <span class="n">ylab</span> <span class="o">=</span> <span class="s">&#34;Economic growth (%)&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">      <span class="n">xlab</span> <span class="o">=</span> <span class="s">&#34;International aid ($M)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">id.label</span> <span class="o">&lt;-</span> <span class="nf">paste0</span><span class="p">(</span> <span class="nf">toupper</span><span class="p">(</span> <span class="n">data</span><span class="o">$</span><span class="n">g</span> <span class="p">),</span> <span class="s">&#34;[t==&#34;</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">t</span><span class="p">,</span> <span class="s">&#34;]&#34;</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">abline</span><span class="p">(</span> <span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="o">~</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">g</span><span class="o">==</span><span class="s">&#34;a&#34;</span><span class="p">)</span> <span class="p">),</span> <span class="n">lwd</span><span class="o">=</span><span class="m">3</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">col</span><span class="o">=</span><span class="nf">adjustcolor</span><span class="p">(</span><span class="s">&#34;gray40&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="m">0.2</span><span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">abline</span><span class="p">(</span> <span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="o">~</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span><span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">g</span><span class="o">==</span><span class="s">&#34;b&#34;</span><span class="p">)</span> <span class="p">),</span> <span class="n">lwd</span><span class="o">=</span><span class="m">3</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">col</span><span class="o">=</span><span class="nf">adjustcolor</span><span class="p">(</span><span class="s">&#34;gray40&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="m">0.2</span><span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">abline</span><span class="p">(</span> <span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="o">~</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span><span class="n">subset</span><span class="o">=</span><span class="p">(</span><span class="n">g</span><span class="o">==</span><span class="s">&#34;c&#34;</span><span class="p">)</span> <span class="p">),</span> <span class="n">lwd</span><span class="o">=</span><span class="m">3</span><span class="p">,</span> <span class="n">lty</span><span class="o">=</span><span class="m">3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">col</span><span class="o">=</span><span class="nf">adjustcolor</span><span class="p">(</span><span class="s">&#34;gray40&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="m">0.2</span><span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">text</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">y</span><span class="p">,</span> <span class="nf">parse</span><span class="p">(</span> <span class="n">text</span><span class="o">=</span><span class="n">id.label</span> <span class="p">),</span> 
</span></span><span class="line"><span class="cl">     <span class="n">col</span><span class="o">=</span><span class="s">&#34;gray60&#34;</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="m">2</span><span class="p">,</span> <span class="n">cex</span><span class="o">=</span><span class="m">2</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="m">1</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">palette</span><span class="p">(</span> <span class="nf">c</span><span class="p">(</span> <span class="s">&#34;steelblue&#34;</span><span class="p">,</span> <span class="s">&#34;darkred&#34;</span><span class="p">,</span> <span class="s">&#34;darkgreen&#34;</span>  <span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">palette</span><span class="p">(</span> <span class="nf">adjustcolor</span><span class="p">(</span> <span class="nf">palette</span><span class="p">(),</span> <span class="n">alpha.f</span> <span class="o">=</span> <span class="m">0.4</span> <span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">par</span><span class="p">(</span> <span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">)</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">plot</span><span class="p">(</span> <span class="n">data</span><span class="o">$</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">y</span><span class="p">,</span> <span class="n">bty</span><span class="o">=</span><span class="s">&#34;n&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="c1"># ylim = c(-5,5), xlim = c(-3,3), bty=&#34;n&#34;, </span>
</span></span><span class="line"><span class="cl">     <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">,</span> <span class="n">cex</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">g</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">     <span class="n">ylab</span> <span class="o">=</span> <span class="s">&#34;Economic growth (%)&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">     <span class="n">xlab</span> <span class="o">=</span> <span class="s">&#34;International aid ($M)&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="n">main</span><span class="o">=</span><span class="s">&#34;POOLED MODEL&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">plot</span><span class="p">(</span> <span class="n">data</span><span class="o">$</span><span class="n">x.demeaned</span><span class="p">,</span> <span class="n">data</span><span class="o">$</span><span class="n">y.demeaned</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">     <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-5</span><span class="p">,</span><span class="m">5</span><span class="p">),</span> <span class="n">xlim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span><span class="m">3</span><span class="p">),</span> <span class="n">bty</span><span class="o">=</span><span class="s">&#34;n&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">     <span class="n">pch</span> <span class="o">=</span> <span class="m">19</span><span class="p">,</span> <span class="n">cex</span><span class="o">=</span><span class="m">4</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nf">factor</span><span class="p">(</span><span class="n">data</span><span class="o">$</span><span class="n">g</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">     <span class="n">ylab</span> <span class="o">=</span> <span class="s">&#34;Economic growth (%)&#34;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">     <span class="n">xlab</span> <span class="o">=</span> <span class="s">&#34;International aid ($M)&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="n">main</span><span class="o">=</span><span class="s">&#34;DEMEANED DATA&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">points</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">-4</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&#34;steelblue&#34;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">5</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">points</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">-4.5</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&#34;darkred&#34;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">5</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">points</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">-5</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&#34;darkgreen&#34;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">5</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">text</span><span class="p">(</span><span class="m">2.4</span><span class="p">,</span> <span class="m">-4</span><span class="p">,</span> <span class="s">&#34;A&#34;</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&#34;steelblue&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">text</span><span class="p">(</span><span class="m">2.4</span><span class="p">,</span> <span class="m">-4.5</span><span class="p">,</span> <span class="s">&#34;B&#34;</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&#34;darkred&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">text</span><span class="p">(</span><span class="m">2.4</span><span class="p">,</span> <span class="m">-5</span><span class="p">,</span> <span class="s">&#34;C&#34;</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&#34;darkgreen&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Pooled_OLS</span> <span class="o">=</span> <span class="nf">lm </span><span class="p">(</span> <span class="n">y</span> <span class="o">~</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x2</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Demean_OLS</span> <span class="o">=</span> <span class="nf">lm </span><span class="p">(</span> <span class="n">y.demeaned</span> <span class="o">~</span> <span class="n">x.demeaned</span> <span class="o">+</span> <span class="n">x2.demeaned</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">fixed_reg</span> <span class="o">=</span> <span class="nf">lm </span><span class="p">(</span> <span class="n">y</span> <span class="o">~</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">+</span> <span class="nf">as.factor</span><span class="p">(</span><span class="n">g</span><span class="p">),</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plm_ols</span> <span class="o">=</span> <span class="n">plm</span><span class="o">::</span><span class="nf">plm</span><span class="p">(</span><span class="n">y</span> <span class="o">~</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x2</span><span class="p">,</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span><span class="n">index</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;g&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_rows</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_cols</span><span class="p">(</span><span class="s">&#34;Pooled&#34;</span><span class="p">,</span><span class="nf">tidy</span><span class="p">(</span><span class="n">Pooled_OLS</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_cols</span><span class="p">(</span><span class="s">&#34;De-meaned&#34;</span><span class="p">,</span><span class="nf">tidy</span><span class="p">(</span><span class="n">Demean_OLS</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_cols</span><span class="p">(</span><span class="s">&#34;Fixed Effects&#34;</span><span class="p">,</span><span class="nf">tidy</span><span class="p">(</span><span class="n">fixed_reg</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="n">dplyr</span><span class="o">::</span><span class="nf">bind_cols</span><span class="p">(</span><span class="s">&#34;PLM&#34;</span><span class="p">,</span><span class="nf">tidy</span><span class="p">(</span><span class="n">plm_ols</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">rename</span><span class="p">(</span><span class="n">Model</span> <span class="o">=</span> <span class="m">1</span><span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">arrange</span><span class="p">(</span><span class="n">term</span><span class="p">)</span>
</span></span></code></pre></div><pre><code># A tibble: 13 × 6
   Model         term           estimate std.error statistic    p.value
   &lt;chr&gt;         &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
 1 Pooled        (Intercept)   -4.41e+ 0    6.51   -6.77e- 1 0.524     
 2 De-meaned     (Intercept)    6.60e-18    0.101   6.53e-17 1         
 3 Fixed Effects (Intercept)    2.82e+ 0    0.484   5.82e+ 0 0.00434   
 4 Fixed Effects as.factor(g)b -4.92e+ 0    0.311  -1.58e+ 1 0.0000935 
 5 Fixed Effects as.factor(g)c -1.28e+ 1    0.343  -3.74e+ 1 0.00000305
 6 Pooled        x              8.98e- 1    1.37    6.56e- 1 0.536     
 7 Fixed Effects x              1.01e+ 0    0.0890  1.13e+ 1 0.000348  
 8 PLM           x              1.01e+ 0    0.0890  1.13e+ 1 0.000348  
 9 De-meaned     x.demeaned     1.01e+ 0    0.0727  1.39e+ 1 0.00000880
10 Pooled        x2             2.38e+ 0    1.95    1.22e+ 0 0.268     
11 Fixed Effects x2            -3.58e- 2    0.142  -2.52e- 1 0.814     
12 PLM           x2            -3.58e- 2    0.142  -2.52e- 1 0.814     
13 De-meaned     x2.demeaned   -3.58e- 2    0.116  -3.08e- 1 0.768     
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># stargazer::stargazer( Pooled_OLS, Demean_OLS, fixed_reg,plm_ols,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#            #type = &#34;html&#34;, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#            dep.var.labels = c(&#34;Economic growth&#34;, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#                               &#34;Economic growth&#34;, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#                               &#34;Economic growth (de-meaned)&#34;,</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                               &#34;Economic growth (PLM)&#34;),</span>
</span></span><span class="line"><span class="cl"><span class="c1">#            column.labels = c(&#34;Cross-Sectional OLS&#34;, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#                              &#34;Pooled OLS&#34;, &#34;De-meaned OLS&#34;),</span>
</span></span><span class="line"><span class="cl"><span class="c1">#            covariate.labels = c(&#34;Constant&#34;, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                 &#34;International aid - Year 1&#34;, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                 &#34;International aid&#34;, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#                                 &#34;International aid - De-meaned OLS&#34;),</span>
</span></span><span class="line"><span class="cl"><span class="c1">#            omit.stat = &#34;all&#34;, </span>
</span></span><span class="line"><span class="cl"><span class="c1">#            digits = 2, intercept.bottom = FALSE )</span>
</span></span></code></pre></div><img src="figs/unnamed-chunk-11-1.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<img src="figs/unnamed-chunk-11-2.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<p>So, what&rsquo;s going on above? We have a plot where a simple regression of y on x will result in a negative coefficient - i.e. aid reduces growth. But, when we account for the grouping in the data with fixed effects, the relationship is positive.</p>
<p>The regression results show that we can get the same coefficients using fixed effects regressions, the plm package, or demeaning the data. The standard errors, however, are not the same!</p>
<p>Source:</p>
<ul>
<li><a href="https://ds4ps.org/pe4ps-textbook/docs/p-040-fixed-effects.html#de-meaned-ols-model">https://ds4ps.org/pe4ps-textbook/docs/p-040-fixed-effects.html#de-meaned-ols-model</a></li>
</ul>
<h1 id="effective-sample">Effective Sample</h1>
<p>Essentially, `` Causal effects estimated via multiple regression differentially weight each unit&rsquo;s contribution. The &ldquo;effective sample&rdquo; that regression uses to generate the estimate may bear little resemblance to the population of interest, and the results may be nonrepresentative in a manner similar to what quasi-experimental methods or experiments with convenience samples produce.&quot;</p>
<p>Our formal analysis builds on results of Angrist and Krueger (1999, 1311&ndash;12) and Angrist and Pischke (2009, chap. 3), who show that multiple regression estimates are equivalent to weighted averages of unit-specific contributions, with the resulting multiple regression weights driven by the conditional variance of the causal factor of interest.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># run auxillary regression, take residuals from that, square them and then you have the weightns</span>
</span></span><span class="line"><span class="cl"><span class="n">effective_sample</span> <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span><span class="n">lm_aux</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">group</span><span class="p">){</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># lm_formula = &#34;log(cumulative_sequence_length_over_10 + 1) ~ Coalition + log(totrevenue + 1)&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># lm_formula &lt;- as.formula(lm_formula)</span>
</span></span><span class="line"><span class="cl">  <span class="n">lm_aux</span> <span class="o">&lt;-</span> <span class="nf">as.formula</span><span class="p">(</span><span class="n">lm_aux</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">fit.d</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">lm_aux</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">d.tilde</span> <span class="o">&lt;-</span> <span class="nf">as.numeric</span><span class="p">(</span><span class="nf">residuals</span><span class="p">(</span><span class="n">fit.d</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">w</span> <span class="o">&lt;-</span> <span class="n">d.tilde^2</span>
</span></span><span class="line"><span class="cl">  <span class="n">w1</span> <span class="o">&lt;-</span> <span class="nf">tapply</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">data</span> <span class="o">%&gt;%</span> <span class="nf">select</span><span class="p">(</span><span class="nf">all_of</span><span class="p">(</span><span class="n">group</span><span class="p">)),</span> <span class="n">mean</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">output</span> <span class="o">&lt;-</span> <span class="nf">tibble</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="nf">names</span><span class="p">(</span><span class="n">w1</span><span class="p">))</span> <span class="o">%&gt;%</span> <span class="nf">arrange</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="n">w1</span><span class="p">))</span> <span class="o">%&gt;%</span> <span class="nf">mutate</span><span class="p">(</span><span class="n">pct</span> <span class="o">=</span> <span class="nf">round</span><span class="p">(</span><span class="n">w1</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">w1</span><span class="p">)</span><span class="o">*</span><span class="m">100</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="kr">return</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span></code></pre></div><p>See <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12185">https://onlinelibrary.wiley.com/doi/full/10.1111/ajps.12185</a></p>
<h2 id="regression-in-machine-learning">Regression in Machine Learning</h2>
<p>Here we care about prediction instead of explaining.</p>
<p>Sum of Squared Error <img alt="= \\sum_{i=1}^n(\\hat{Y_i}-Y_i)^2" src="https://latex.codecogs.com/svg.latex?%3D%20%5Csum_%7Bi%3D1%7D%5En%28%5Chat%7BY_i%7D-Y_i%29%5E2" title="= \\sum_{i=1}^n(\\hat{Y_i}-Y_i)^2"></p>
<p>Mean Squared Error <img alt="=\\frac{1}{n}\\sum_{i=1}^n(\\hat{Y_i}-Y_i)^2" src="https://latex.codecogs.com/svg.latex?%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28%5Chat%7BY_i%7D-Y_i%29%5E2" title="=\\frac{1}{n}\\sum_{i=1}^n(\\hat{Y_i}-Y_i)^2"></p>
<p>Here <img alt="SSE = n \\times MSE" src="https://latex.codecogs.com/svg.latex?SSE%20%3D%20n%20%5Ctimes%20MSE" title="SSE = n \\times MSE"></p>
<p>We can derive OLS using MSE instead or SSE if we want. That is, we can minimize MSE instead of SSE.</p>
<p><img alt="MSE = \\frac{1}{n}\\sum_{i=1}^ne_i^2" src="https://latex.codecogs.com/svg.latex?MSE%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5Ene_i%5E2" title="MSE = \\frac{1}{n}\\sum_{i=1}^ne_i^2"></p>
<p><img alt="MSE = \\frac{1}{n}\\sum_{i=1}^n (\\hat{y_i} - y_i)^2" src="https://latex.codecogs.com/svg.latex?MSE%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%20%28%5Chat%7By_i%7D%20-%20y_i%29%5E2" title="MSE = \\frac{1}{n}\\sum_{i=1}^n (\\hat{y_i} - y_i)^2"></p>
<p><img alt="MSE = \\frac{1}{n}(Xb -y)'(Xb-y)" src="https://latex.codecogs.com/svg.latex?MSE%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%28Xb%20-y%29%27%28Xb-y%29" title="MSE = \\frac{1}{n}(Xb -y)&#39;(Xb-y)"></p>
<p>Distribute the terms above so that we can take the derivative with respect to b</p>
<p><img alt="MSE = \\frac{1}{n}( b'X'Xb - b'X'y - y'Xb + y'y)" src="https://latex.codecogs.com/svg.latex?MSE%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%28%20b%27X%27Xb%20-%20b%27X%27y%20-%20y%27Xb%20%2B%20y%27y%29" title="MSE = \\frac{1}{n}( b&#39;X&#39;Xb - b&#39;X&#39;y - y&#39;Xb + y&#39;y)"></p>
<p><img alt="MSE = \\frac{1}{n}(b'X'Xb - 2b'X'y + y'y)" src="https://latex.codecogs.com/svg.latex?MSE%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%28b%27X%27Xb%20-%202b%27X%27y%20%2B%20y%27y%29" title="MSE = \\frac{1}{n}(b&#39;X&#39;Xb - 2b&#39;X&#39;y + y&#39;y)"></p>
<p>The above takes advantage of the fact that the transpose of a scaler is a scaler. I.e. if b is k x 1 and b&rsquo; is 1 x k then b&rsquo;X&rsquo;y is 1 x 1 and so is y&rsquo;Xb</p>
<p><img alt="\\frac{\\partial}{\\partial{b}} \\text{MSE} = \\frac{1}{n}(2X'Xb - 2X'y)" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%7Bb%7D%7D%20%5Ctext%7BMSE%7D%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%282X%27Xb%20-%202X%27y%29" title="\\frac{\\partial}{\\partial{b}} \\text{MSE} = \\frac{1}{n}(2X&#39;Xb - 2X&#39;y)"></p>
<p><img alt="\\frac{\\partial}{\\partial{b}} \\text{MSE} = \\frac{2}{n}(X'Xb) - \\frac{2}{n}(X'y)" src="https://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%7Bb%7D%7D%20%5Ctext%7BMSE%7D%20%3D%20%5Cfrac%7B2%7D%7Bn%7D%28X%27Xb%29%20-%20%5Cfrac%7B2%7D%7Bn%7D%28X%27y%29" title="\\frac{\\partial}{\\partial{b}} \\text{MSE} = \\frac{2}{n}(X&#39;Xb) - \\frac{2}{n}(X&#39;y)"></p>
<p><img alt="0 = \\frac{2}{n}(X'Xb) - \\frac{2}{n}(X'y)" src="https://latex.codecogs.com/svg.latex?0%20%3D%20%5Cfrac%7B2%7D%7Bn%7D%28X%27Xb%29%20-%20%5Cfrac%7B2%7D%7Bn%7D%28X%27y%29" title="0 = \\frac{2}{n}(X&#39;Xb) - \\frac{2}{n}(X&#39;y)"></p>
<p><img alt="0 = X'Xb - X'y" src="https://latex.codecogs.com/svg.latex?0%20%3D%20X%27Xb%20-%20X%27y" title="0 = X&#39;Xb - X&#39;y"></p>
<p><img alt="X'Xb = X'y" src="https://latex.codecogs.com/svg.latex?X%27Xb%20%3D%20X%27y" title="X&#39;Xb = X&#39;y"></p>
<p><img alt="b = (X'X)^{-1}X'y" src="https://latex.codecogs.com/svg.latex?b%20%3D%20%28X%27X%29%5E%7B-1%7DX%27y" title="b = (X&#39;X)^{-1}X&#39;y"></p>
<p>If we only have two predictors <img alt="X_1" src="https://latex.codecogs.com/svg.latex?X_1" title="X_1"> and <img alt="X_2" src="https://latex.codecogs.com/svg.latex?X_2" title="X_2"> then our error can be visualized in the following way <a href="https://allmodelsarewrong.github.io/ols.html">https://allmodelsarewrong.github.io/ols.html</a></p>
<p><img alt="MSE = E(\\hat{y},y) = \\frac{1}{n} (\\beta' x' x \\beta - 2 y'x\\beta + y'y)" src="https://latex.codecogs.com/svg.latex?MSE%20%3D%20E%28%5Chat%7By%7D%2Cy%29%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%20%28%5Cbeta%27%20x%27%20x%20%5Cbeta%20-%202%20y%27x%5Cbeta%20%2B%20y%27y%29" title="MSE = E(\\hat{y},y) = \\frac{1}{n} (\\beta&#39; x&#39; x \\beta - 2 y&#39;x\\beta + y&#39;y)"></p>
<p>. The first term is a quadratic, the second is linear, the third is a scalar</p>
<h2 id="ridge-regression">Ridge Regression</h2>
<p>Ridge is OLS with a budget <img alt="\\lambda" src="https://latex.codecogs.com/svg.latex?%5Clambda" title="\\lambda">. The coefficients won&rsquo;t drop out, like in LASSO, but they&rsquo;ll tend towards 0. I.e. ridge regression does not perform variable selection. This is known as an l2 norm.</p>
<p><img alt="\\beta_{rr}=(X'X + \\lambda I)^{-1}X'Y" src="https://latex.codecogs.com/svg.latex?%5Cbeta_%7Brr%7D%3D%28X%27X%20%2B%20%5Clambda%20I%29%5E%7B-1%7DX%27Y" title="\\beta_{rr}=(X&#39;X + \\lambda I)^{-1}X&#39;Y"></p>
<p>Ridge regression is a constrained maximization problem. Where:</p>
<p><img alt="\\min_b \\\\frac{1}{n}(X'X)^{-1}(X'Y)\\ \\text{ such that } b'b \\leq c" src="https://latex.codecogs.com/svg.latex?%5Cmin_b%20%5C%7B%5Cfrac%7B1%7D%7Bn%7D%28X%27X%29%5E%7B-1%7D%28X%27Y%29%5C%7D%20%5Ctext%7B%20such%20that%20%7D%20b%27b%20%5Cleq%20c" title="\\min_b \\{\\frac{1}{n}(X&#39;X)^{-1}(X&#39;Y)\\} \\text{ such that } b&#39;b \\leq c"></p>
<p>The solution is above. How do you find c? or <img alt="\\lambda" src="https://latex.codecogs.com/svg.latex?%5Clambda" title="\\lambda">?</p>
<p>``In ridge regression, <img alt="\\lambda" src="https://latex.codecogs.com/svg.latex?%5Clambda" title="\\lambda"> is a tuning parameter, and therefore it cannot be found analytically. Instead, you have to implement a trial and error process with various values for <img alt="\\lambda" src="https://latex.codecogs.com/svg.latex?%5Clambda" title="\\lambda">, and determine which seems to be a good one. How? Typically with cross-validation, or other type of resampling approach.&quot;</p>
<p>Basically: 1) Select a small <img alt="\\lambda" src="https://latex.codecogs.com/svg.latex?%5Clambda" title="\\lambda"> (if lambda is big the coefs all go to 0) 2) For each fold, (so no we&rsquo;re in the second for loop), fit a ridge regression and store the errors 3) Compare all of them, take the smallest one and use that <img alt="\\lambda" src="https://latex.codecogs.com/svg.latex?%5Clambda" title="\\lambda"></p>
<h2 id="least-absolute-shrinkage-and-selection-operator-lasso">Least Absolute Shrinkage and Selection Operator (LASSO)</h2>
<p>This is used in variable selection. It&rsquo;s also known as a l1 norm. It&rsquo;s like Ridge, except it&rsquo;s a constrained minimization problem subject to the norm <img alt="|b|\\leq c" src="https://latex.codecogs.com/svg.latex?%7Cb%7C%5Cleq%20c" title="|b|\\leq c"></p>
<p><a href="https://allmodelsarewrong.github.io/lasso.html">https://allmodelsarewrong.github.io/lasso.html</a></p>
<h1 id="measurement-error">Measurement Error</h1>
<p>What does measurement error in X and Y do to the coefficeints?</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="s">&#34;arm&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">ggpubr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">&lt;-</span> <span class="m">0.2</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">&lt;-</span> <span class="m">0.4</span>
</span></span><span class="line"><span class="cl"><span class="n">sigma</span> <span class="o">&lt;-</span> <span class="m">0.5</span>
</span></span><span class="line"><span class="cl"><span class="n">sigma_ystar</span> <span class="o">&lt;-</span> <span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="n">sigma_x</span> <span class="o">&lt;-</span> <span class="m">4</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fake_data</span> <span class="o">&lt;-</span>
</span></span><span class="line"><span class="cl">  <span class="nf">tibble</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="nf">runif</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> <span class="n">min</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">max</span> <span class="o">=</span> <span class="m">10</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_star</span> <span class="o">=</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="n">sigma_ystar</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_star</span> <span class="o">=</span>  <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sigma_x</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># now add error in y</span>
</span></span><span class="line"><span class="cl"><span class="n">no_error</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">fake_data</span><span class="p">,</span><span class="n">y</span><span class="o">~</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">error_y</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">fake_data</span><span class="p">,</span><span class="n">y_star</span><span class="o">~</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">error_x</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">fake_data</span><span class="p">,</span><span class="n">y</span><span class="o">~</span><span class="n">x_star</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">error_xy</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">fake_data</span><span class="p">,</span><span class="n">y_star</span><span class="o">~</span><span class="n">x_star</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">no_error_plot</span> <span class="o">&lt;-</span> <span class="nf">ggplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">fake_data</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="p">))</span> <span class="o">+</span> <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span> <span class="nf">geom_smooth</span><span class="p">(</span><span class="n">method</span> <span class="o">=</span> <span class="s">&#34;lm&#34;</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="bp">F</span><span class="p">)</span> <span class="o">+</span> <span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">xlim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span> <span class="m">20</span><span class="p">),</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span> <span class="m">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">error_y_plot</span> <span class="o">&lt;-</span> <span class="nf">ggplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">fake_data</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_star</span> <span class="p">))</span> <span class="o">+</span> <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span> <span class="nf">geom_smooth</span><span class="p">(</span><span class="n">method</span> <span class="o">=</span> <span class="s">&#34;lm&#34;</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="bp">F</span><span class="p">)</span><span class="o">+</span> <span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">xlim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span> <span class="m">20</span><span class="p">),</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span> <span class="m">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">error_x_plot</span> <span class="o">&lt;-</span> <span class="nf">ggplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">fake_data</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_star</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="p">))</span> <span class="o">+</span> <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span> <span class="nf">geom_smooth</span><span class="p">(</span><span class="n">method</span> <span class="o">=</span> <span class="s">&#34;lm&#34;</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="bp">F</span><span class="p">)</span><span class="o">+</span> <span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">xlim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span> <span class="m">20</span><span class="p">),</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span> <span class="m">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">error_xy_plot</span> <span class="o">&lt;-</span> <span class="nf">ggplot</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">fake_data</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_star</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_star</span> <span class="p">))</span> <span class="o">+</span> <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span> <span class="nf">geom_smooth</span><span class="p">(</span><span class="n">method</span> <span class="o">=</span> <span class="s">&#34;lm&#34;</span><span class="p">,</span> <span class="n">se</span> <span class="o">=</span> <span class="bp">F</span><span class="p">)</span><span class="o">+</span> <span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">xlim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span> <span class="m">20</span><span class="p">),</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span> <span class="m">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nf">ggarrange</span><span class="p">(</span><span class="n">no_error_plot</span><span class="p">,</span> <span class="n">error_y_plot</span><span class="p">,</span> <span class="n">error_x_plot</span><span class="p">,</span><span class="n">error_xy_plot</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">          <span class="n">labels</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;No Error&#34;</span><span class="p">,</span> <span class="s">&#34;Error Y&#34;</span><span class="p">,</span> <span class="s">&#34;Error X&#34;</span><span class="p">,</span><span class="s">&#34;Error X &amp; Y&#34;</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">          <span class="n">ncol</span> <span class="o">=</span> <span class="m">2</span><span class="p">,</span> <span class="n">nrow</span> <span class="o">=</span> <span class="m">2</span><span class="p">)</span>
</span></span></code></pre></div><img src="figs/unnamed-chunk-13-1.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<h1 id="causal-quantities">Causal Quantities</h1>
<p>Stolen from <a href="https://mixtape.scunning.com/04-potential_outcomes#randomization-inference">https://mixtape.scunning.com/04-potential_outcomes#randomization-inference</a></p>
<p>Note: these are all population means.</p>
<p>Average Treatment Effect (ATE)</p>
<p><img alt="\\text{ATE}=\\mathbb{E}[Y_{i}(1)]-\\mathbb{E}[Y_{i}(0)]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATE%7D%3D%5Cmathbb%7BE%7D%5BY_%7Bi%7D%281%29%5D-%5Cmathbb%7BE%7D%5BY_%7Bi%7D%280%29%5D" title="\\text{ATE}=\\mathbb{E}[Y_{i}(1)]-\\mathbb{E}[Y_{i}(0)]"></p>
<p><img alt="\\text{ATE}=\\mathbb{E}[Y_{i}^1]-\\mathbb{E}[Y_{i}^0]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATE%7D%3D%5Cmathbb%7BE%7D%5BY_%7Bi%7D%5E1%5D-%5Cmathbb%7BE%7D%5BY_%7Bi%7D%5E0%5D" title="\\text{ATE}=\\mathbb{E}[Y_{i}^1]-\\mathbb{E}[Y_{i}^0]"></p>
<p>Notice, as with our definition of individual-level treatment effects, that the average treatment effect requires both potential outcomes for each unit. Since we only know one of these by the switching equation, the average treatment effect, or the ATE, is inherently unknowable. Thus, the ATE, like the individual treatment effect, is not a quantity that can be calculated. But it can be estimated</p>
<p>The second parameter of interest is the average treatment effect for the treatment group. That&rsquo;s a mouthful, but let me explain. There exist two groups of people in this discussion we&rsquo;ve been having: a treatment group and a control group. The average treatment effect for the treatment group, or ATT for short, is simply that population mean treatment effect for the group of units that had been assigned the treatment in the first place according to the switching equation. Insofar as differs across the population, the ATT will likely differ from the ATE. In observational data involving human beings, it almost always will be different from the ATE, and that&rsquo;s because individuals will be endogenously sorting into some treatment based on the gains they expect from it. Like the ATE, the ATT is unknowable, because like the ATE, it also requires two observations per treatment unit</p>
<p>Average Treatment effect on the Treated (ATT)</p>
<p><img alt="\\text{ATT}=\\mathbb{E}[Y_{i}(1)|D=1]-\\mathbb{E}[Y_{i}(0)|D=1]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATT%7D%3D%5Cmathbb%7BE%7D%5BY_%7Bi%7D%281%29%7CD%3D1%5D-%5Cmathbb%7BE%7D%5BY_%7Bi%7D%280%29%7CD%3D1%5D" title="\\text{ATT}=\\mathbb{E}[Y_{i}(1)|D=1]-\\mathbb{E}[Y_{i}(0)|D=1]"></p>
<p><img alt="\\text{ATT}=\\mathbb{E}[Y_{i}^1|D=1]-\\mathbb{E}[Y_{i}^0|D=1]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATT%7D%3D%5Cmathbb%7BE%7D%5BY_%7Bi%7D%5E1%7CD%3D1%5D-%5Cmathbb%7BE%7D%5BY_%7Bi%7D%5E0%7CD%3D1%5D" title="\\text{ATT}=\\mathbb{E}[Y_{i}^1|D=1]-\\mathbb{E}[Y_{i}^0|D=1]"></p>
<p>The final parameter of interest is called the average treatment effect for the control group, or untreated group. It&rsquo;s shorthand is ATU, which stands for average treatment effect for the untreated. And like ATT, the ATU is simply the population mean treatment effect for those units who sorted into the control group.11 Given heterogeneous treatment effects, it&rsquo;s probably the case that the <img alt="\\text{ATT}\\neq\\text{ATU}" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATT%7D%5Cneq%5Ctext%7BATU%7D" title="\\text{ATT}\\neq\\text{ATU}">, especially in an observational setting. The formula for the ATU is as follows:</p>
<p><img alt="\\text{ATU}=\\mathbb{E}[Y_{i}(1)|D=0]-\\mathbb{E}[Y_{i}(0)|D=0]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATU%7D%3D%5Cmathbb%7BE%7D%5BY_%7Bi%7D%281%29%7CD%3D0%5D-%5Cmathbb%7BE%7D%5BY_%7Bi%7D%280%29%7CD%3D0%5D" title="\\text{ATU}=\\mathbb{E}[Y_{i}(1)|D=0]-\\mathbb{E}[Y_{i}(0)|D=0]"></p>
<p><img alt="\\text{ATU}=\\mathbb{E}[Y_{i}^1|D=0]-\\mathbb{E}[Y_{i}^0|D=0]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATU%7D%3D%5Cmathbb%7BE%7D%5BY_%7Bi%7D%5E1%7CD%3D0%5D-%5Cmathbb%7BE%7D%5BY_%7Bi%7D%5E0%7CD%3D0%5D" title="\\text{ATU}=\\mathbb{E}[Y_{i}^1|D=0]-\\mathbb{E}[Y_{i}^0|D=0]"></p>
<p>CATE</p>
<p><img alt="\\text{CATE}=\\mathbb{E}[Y_{i}(1)|D_i=1,X_i=x]-\\mathbb{E}[Y_{i}(0)|D_i=0,X_i=x]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BCATE%7D%3D%5Cmathbb%7BE%7D%5BY_%7Bi%7D%281%29%7CD_i%3D1%2CX_i%3Dx%5D-%5Cmathbb%7BE%7D%5BY_%7Bi%7D%280%29%7CD_i%3D0%2CX_i%3Dx%5D" title="\\text{CATE}=\\mathbb{E}[Y_{i}(1)|D_i=1,X_i=x]-\\mathbb{E}[Y_{i}(0)|D_i=0,X_i=x]"></p>
<p>LATE</p>
<p>The LATE theorem
states that under a set of basic identifying conditions, an
instrumental variable identifies the average causal effect for
the subpopulation of units whose treatment status is in fact
moved by the instrument. Summary statistics describing this
subpopulation can be computed using the kappa-weighting
results of Abadie (2003). The result from Aronow and Samii
(2016) described above is a LATE-type result, showing that
under the relevant identifying assumptions, linear regression
estimates are consistent for the average causal effect local to a
subpopulation whose traits can be characterized by reweighting the nominal sample by the multiple regression weights. [link to paper]<a href="https://gregoryeady.com/ResearchMethodsCourse/assets/readings/Samii,%20Cyrus%20-%202016%20-%20Causal%20Empiricism%20in%20Quantitative%20Research.pdf">https://gregoryeady.com/ResearchMethodsCourse/assets/readings/Samii,%20Cyrus%20-%202016%20-%20Causal%20Empiricism%20in%20Quantitative%20Research.pdf</a></p>
<h1 id="difference-in-differencess">Difference-in-Differencess</h1>
<p>Suppose that we want to isolate (i.e. identify) the effect of a policy on some outcome we care about.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> Our question here is causal because we care about the effects of the policy itself and not whether the outcome we&rsquo;re studying changed for other reasons. Some examples of causal questions are: did expanding Medicaid make people healthier? Did a tax reform stimulate investment?</p>
<p>There are a number of ways we could estimate this effect:</p>
<ul>
<li>
<p>We could subtract the mean value of some outcome between the treated and non-treated groups after the treatment (i.e. <img alt="Y_{1,1}-Y_{0,1}" src="https://latex.codecogs.com/svg.latex?Y_%7B1%2C1%7D-Y_%7B0%2C1%7D" title="Y_{1,1}-Y_{0,1}">). This approach, however, does not account for differences in average outcomes between the treated and not-treated groups. That is, it doesn&rsquo;t do anything about selection bias.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
</li>
<li>
<p>We could also look at differences in the treated units and see how much its outcome changed as a result of the treatment (i.e. <img alt="Y_{1,1}-Y_{1,0}" src="https://latex.codecogs.com/svg.latex?Y_%7B1%2C1%7D-Y_%7B1%2C0%7D" title="Y_{1,1}-Y_{1,0}">). This approach also fails because the change might not have been due to the treatment but something else independent of the treatment</p>
</li>
<li>
<p>Note that we could classify the first two bullets here as a Pre-Post analysis</p>
</li>
<li>
<p>We could do a regression or reweighing or double machine learning models. This approach fails because you can&rsquo;t rule out selection on unobservables. We would need to have data on everything that affects treatment timing and the outcome of interest (unconfoundedness assumption).<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
</li>
</ul>
<p>So we need a better technique, like difference-in-differences or DiD for short. DiD is an estimator that, under certain assumptions (parallel trends and no-anticipation), identify a causal effect of an intervention on an outcome. DiD methods exploit variation in time (before vs. after) and across groups (treated vs. untreated) to recover causal effects of interest. The advantage? It allow for selection on unobservables and for time-trends.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
<p>To start our DiD journey, we need to transform our question into a <em>target estimand</em> - a statistical representation of the part of the policy that we care about.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> In the Medicaid example above, our target estimand might be the difference in average health care spending in a state that expanded Medicaid after they expanded minus the average spending in that state if the state had not expanded Medicaid. Note that the target estimand is expressed in terms of potential outcomes.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> In our example scenario, the state that expanded has two <em>potential outcomes</em>: health care spending under the new law and health care spending without the new law. Only one of these is observable (spending with the new law); the other is unobservable because it didn&rsquo;t happen (spending without the new law).</p>
<p>To estimate the effect of the law on health care spending we need data on health care spending. Our DiD algorithm that takes data as an input and produces a value of the estimand is called the estimator. The estimator&rsquo;s output, given data input, is called the estimate. This value represents our best guess at the estimand, given the data we have. The goal of DiD is using an estimator (e.g. simple means or ordinary least squares) to produce an estimate that is the sample equivalent of the theoretical estimand (the average treatment effect on the treated or ATT).<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></p>
<p>To recap:</p>
<ol>
<li>We have a causal question that can&rsquo;t be answered convincingly (in this case causally) by a simple difference between population averages because of selection bias</li>
<li>We select an estimand (e.g. an ATT) that helps us answer the target question</li>
<li>We choose an estimator (difference-in-differences)</li>
<li>We gather data</li>
<li>We compute an estimate (get a numerical value of) of our estimand. Note that because the estimand relies on unobservable quantities, our estimate is our best guess of the estimand</li>
</ol>
<h3 id="canonical-difference-in-differences">Canonical Difference-in-Differences</h3>
<p>Note: Most of this is blatantly stolen from Roth, Jonathan, Sant&rsquo;Anna, Pedro H. C., Bilinski, Alyssa, Poe, John. 2023. What&rsquo;s trending in difference-in-differences? A synthesis of the recent econometrics literature. <em>Journal of Econometrics</em> (235)(8): 2218-2244<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<p>In the canonical difference-in-differences model, where two time periods are available, there is a treated population of units that receives a treatment of interest beginning in the second period, and a comparison population that does not receive the treatment in either period. (As as aside: the canonical design is often referred to as <img alt="2 \\times 2" src="https://latex.codecogs.com/svg.latex?2%20%5Ctimes%202" title="2 \\times 2"> which means 2 groups (treated/untreated) by 2 periods (pre-treatment/post-treatment).)</p>
<p>The key identifying assumption (i.e. the assumption that allows us to identify/isolate the causal effect we care about) is that the average outcome among the treated and comparison populations would have followed (1) <strong>parallel trends</strong> in the absence of treatment. We also assume that the treatment has (2) no causal effect before its implementation (<strong>no anticipation</strong>). Together, these assumptions allow us to identify the average treatment effect on the treated (ATT) (see the proof below for why we need these assumptions to derive an estimator that we can use). If we observe a large number of independent clusters from the treated and comparison populations, the ATT can be consistently estimated using a two-way fixed effects (TWFE) regression specification, and clustered standard errors provide asymptotically valid inference.</p>
<p>The figure below shows how the parallel trends assumption works and why it&rsquo;s important to justify it.</p>
<img src="figs/unnamed-chunk-14-1.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" alt="Idnefitying Assumption Visualized" />
<h3 id="average-treatment-effect-on-the-treated-att">Average Treatment effect on the Treated (ATT)</h3>
<p>The causal estimand of primary interest in the canonical DiD setup is the average treatment effect on the treated (ATT) in period t = 2. This is defined as:</p>
<p><img alt="\\tau_2 = E[Y_{i,2}(1) − Y_{i,2}(0) | D_i = 1]" src="https://latex.codecogs.com/svg.latex?%5Ctau_2%20%3D%20E%5BY_%7Bi%2C2%7D%281%29%20%E2%88%92%20Y_%7Bi%2C2%7D%280%29%20%7C%20D_i%20%3D%201%5D" title="\\tau_2 = E[Y_{i,2}(1) − Y_{i,2}(0) | D_i = 1]"></p>
<p>here <img alt="Y_{\\color{blue}{i},\\color{red}{2}}(\\color{green}{1})" src="https://latex.codecogs.com/svg.latex?Y_%7B%5Ccolor%7Bblue%7D%7Bi%7D%2C%5Ccolor%7Bred%7D%7B2%7D%7D%28%5Ccolor%7Bgreen%7D%7B1%7D%29" title="Y_{\\color{blue}{i},\\color{red}{2}}(\\color{green}{1})"> is the outcome for unit <img alt="\\color{blue}{i}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bblue%7D%7Bi%7D" title="\\color{blue}{i}"> in period <img alt="\\color{red}{2}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bred%7D%7B2%7D" title="\\color{red}{2}"> when treated <img alt="\\color{green}{1}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bgreen%7D%7B1%7D" title="\\color{green}{1}">. It simply measures the average causal effect on treated units in the period that they are treated (t = 2). Note that this is an estimand and <strong>not</strong> an estimate because it requires a counterfactual population (<img alt="\\mathbb{E}[Y_{i,2}(0)|D=1]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5BY_%7Bi%2C2%7D%280%29%7CD%3D1%5D" title="\\mathbb{E}[Y_{i,2}(0)|D=1]">) - which is the effect of an untreated population in period 2 given the population was treated.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
<p>One difficulty in the causal inference literature is that potential outcomes are not expressed uniformly. So familiarity with different ways of expressing the same quantities is important.</p>
<p>Expressed a different way:</p>
<p><img alt="\\text{ATT} \\equiv \\mathbb{E}[Y^1(2)-Y^0(2)|A=1]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATT%7D%20%5Cequiv%20%5Cmathbb%7BE%7D%5BY%5E1%282%29-Y%5E0%282%29%7CA%3D1%5D" title="\\text{ATT} \\equiv \\mathbb{E}[Y^1(2)-Y^0(2)|A=1]"></p>
<p>where <img alt="Y^\\color{green}{a}(\\color{red}{t})" src="https://latex.codecogs.com/svg.latex?Y%5E%5Ccolor%7Bgreen%7D%7Ba%7D%28%5Ccolor%7Bred%7D%7Bt%7D%29" title="Y^\\color{green}{a}(\\color{red}{t})"> is the potential outcome given treatment <img alt="\\color{green}{a}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bgreen%7D%7Ba%7D" title="\\color{green}{a}"> at time <img alt="\\color{red}{t}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bred%7D%7Bt%7D" title="\\color{red}{t}">. Above, <img alt="t=2" src="https://latex.codecogs.com/svg.latex?t%3D2" title="t=2"> represents the post-treatment period, <img alt="a=1" src="https://latex.codecogs.com/svg.latex?a%3D1" title="a=1"> represents treatment and <img alt="a=0" src="https://latex.codecogs.com/svg.latex?a%3D0" title="a=0"> represents no treatment. (See, even the notation of periods is different! Above t = 0 was pre-treatment and t = 1 was post-treatment.) So what does the ATT look like in non-math terms? Suppose CT enacted a new law expanding health insurance and policymakers want to see if the law increases spending on healthcare related services. Translated literally, the equation above is Population Means[Spending in CT with the new law − Spending in CT without the new law |Given new law implemented]. You&rsquo;ll immediately note that the second quantity - spending without the new law - is unobservable because the new law was enacted.</p>
<p>So how do we estimate the ATT when the some of the potential outcomes are unobservable? In diff-in-diff, we use data from the control group to impute untreated outcomes in the treated group. In the Connecticut example above, that would mean using a similar state that didn&rsquo;t enact a similar law. For Medicaid this becomes tricky because comparable states like Massachusetts and Rhode Island expanded coverage. So you&rsquo;d have to use a state like South Carolina and make a case that it&rsquo;s similar to Connecticut.<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> These control groups are the &ldquo;secret sauce&rdquo; of diff-in-diff because they help us learn something about the unobservable counterfactual outcomes of the treated group. In the next section I&rsquo;ll show you the proof of how we can use the sample to calculate the sample analog of the estimand/ATT.<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup></p>
<h4 id="deriving-the-att">Deriving the ATT</h4>
<p>This proof shows that the ATT can be expressed as the difference in the differences between the treated and untreated observations for the canonical <img alt="2 \\times 2" src="https://latex.codecogs.com/svg.latex?2%20%5Ctimes%202" title="2 \\times 2"> case. This is useful because it shows (a) how we use the parallel trends and consistency assumptions and (b) how we go from theoretical quantities to sample analogs. Note that the steps in the proof are numbered and explanations of each step are below as needed.<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup></p>
<ol>
<li></li>
</ol>
<pre><code>![\text{ATT} \equiv {\mathbb{E}\[Y^1(2)-Y^0(2)\|A=1\]}\text{ (Definition of the ATT)}](https://latex.codecogs.com/svg.latex?%5Ctext%7BATT%7D%20%5Cequiv%20%7B%5Cmathbb%7BE%7D%5BY%5E1%282%29-Y%5E0%282%29%7CA%3D1%5D%7D%5Ctext%7B%20%28Definition%20of%20the%20ATT%29%7D &quot;\text{ATT} \equiv {\mathbb{E}[Y^1(2)-Y^0(2)|A=1]}\text{ (Definition of the ATT)}&quot;)
</code></pre>
<ul>
<li>Note that none of the terms above are estimable because they all depend on potential outcomes</li>
</ul>
<ol>
<li></li>
</ol>
<pre><code>![= \underbrace{\mathbb{E}\[Y^1(2)\|A=1\]}\_{\color{green}{\text{sample analog exists}}} - \underbrace{\mathbb{E}\[Y^0(2)\|A=1\]}\_{\color{red}{\text{sample analog doesn\'t exist}}}\text{ (Linearity of expectations)}](https://latex.codecogs.com/svg.latex?%3D%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY%5E1%282%29%7CA%3D1%5D%7D_%7B%5Ccolor%7Bgreen%7D%7B%5Ctext%7Bsample%20analog%20exists%7D%7D%7D%20-%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY%5E0%282%29%7CA%3D1%5D%7D_%7B%5Ccolor%7Bred%7D%7B%5Ctext%7Bsample%20analog%20doesn%27t%20exist%7D%7D%7D%5Ctext%7B%20%28Linearity%20of%20expectations%29%7D &quot;= \underbrace{\mathbb{E}[Y^1(2)|A=1]}_{\color{green}{\text{sample analog exists}}} - \underbrace{\mathbb{E}[Y^0(2)|A=1]}_{\color{red}{\text{sample analog doesn't exist}}}\text{ (Linearity of expectations)}&quot;)
</code></pre>
<p>In the line above, expectations are linear, so we can split them. I.e. <img alt="\\mathbb{E}[x+y|A=1]=\\mathbb{E}[x|A=1]+\\mathbb{E}[y|A=1]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5Bx%2By%7CA%3D1%5D%3D%5Cmathbb%7BE%7D%5Bx%7CA%3D1%5D%2B%5Cmathbb%7BE%7D%5By%7CA%3D1%5D" title="\\mathbb{E}[x+y|A=1]=\\mathbb{E}[x|A=1]+\\mathbb{E}[y|A=1]">.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup></p>
<ol>
<li></li>
</ol>
<pre><code>![= \underbrace{\mathbb{E}\[Y^1(2)\|A=1\]}\_{\color{green}{\text{sample analog exists}}} - \underbrace{(\mathbb{E}\[Y^0(2)-Y^0(1)\|A=0\]}\_{\color{green}{\text{sample analog exists}}} + \underbrace{\mathbb{E}\[Y^0(1)\|A=1\]}\_{\color{green}{\text{sample analog exists}}})\text{ (Counterfactual/parallel trends assumption)}](https://latex.codecogs.com/svg.latex?%3D%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY%5E1%282%29%7CA%3D1%5D%7D_%7B%5Ccolor%7Bgreen%7D%7B%5Ctext%7Bsample%20analog%20exists%7D%7D%7D%20-%20%5Cunderbrace%7B%28%5Cmathbb%7BE%7D%5BY%5E0%282%29-Y%5E0%281%29%7CA%3D0%5D%7D_%7B%5Ccolor%7Bgreen%7D%7B%5Ctext%7Bsample%20analog%20exists%7D%7D%7D%20%2B%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY%5E0%281%29%7CA%3D1%5D%7D_%7B%5Ccolor%7Bgreen%7D%7B%5Ctext%7Bsample%20analog%20exists%7D%7D%7D%29%5Ctext%7B%20%28Counterfactual%2Fparallel%20trends%20assumption%29%7D &quot;= \underbrace{\mathbb{E}[Y^1(2)|A=1]}_{\color{green}{\text{sample analog exists}}} - \underbrace{(\mathbb{E}[Y^0(2)-Y^0(1)|A=0]}_{\color{green}{\text{sample analog exists}}} + \underbrace{\mathbb{E}[Y^0(1)|A=1]}_{\color{green}{\text{sample analog exists}}})\text{ (Counterfactual/parallel trends assumption)}&quot;)
</code></pre>
<p>In the line above, we use the parallel trends assumption. I.e. &ldquo;the change in outcomes from pre- to post-intervention in the control group is a good proxy for the counterfactual change in untreated potential outcomes in the treated group.&rdquo; In other words, the potential outcomes of the control and treatment groups are the same. So the treated - by assumption - aren&rsquo;t behaving differently than the control would be when they get the treatment. In math notation <img alt="\\mathbb{E}[\\color{green}{Y^0(2)}-\\color{red}{Y^0(1)}|A=1]=\\mathbb{E}[\\color{green}{Y^0(2)-Y^0(1)|A=0}]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5B%5Ccolor%7Bgreen%7D%7BY%5E0%282%29%7D-%5Ccolor%7Bred%7D%7BY%5E0%281%29%7D%7CA%3D1%5D%3D%5Cmathbb%7BE%7D%5B%5Ccolor%7Bgreen%7D%7BY%5E0%282%29-Y%5E0%281%29%7CA%3D0%7D%5D" title="\\mathbb{E}[\\color{green}{Y^0(2)}-\\color{red}{Y^0(1)}|A=1]=\\mathbb{E}[\\color{green}{Y^0(2)-Y^0(1)|A=0}]">. To get from <img alt="\\color{red}{\\mathbb{E}[Y^0(2)|A=1]}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bred%7D%7B%5Cmathbb%7BE%7D%5BY%5E0%282%29%7CA%3D1%5D%7D" title="\\color{red}{\\mathbb{E}[Y^0(2)|A=1]}"> in step 2 to <img alt="\\color{green}{\\mathbb{E}[Y^0(2)-Y^0(1)|A=0] + \\mathbb{E}[Y^0(1)|A=1]}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bgreen%7D%7B%5Cmathbb%7BE%7D%5BY%5E0%282%29-Y%5E0%281%29%7CA%3D0%5D%20%2B%20%5Cmathbb%7BE%7D%5BY%5E0%281%29%7CA%3D1%5D%7D" title="\\color{green}{\\mathbb{E}[Y^0(2)-Y^0(1)|A=0] + \\mathbb{E}[Y^0(1)|A=1]}"> in step 3, first separate each term from the definition by linearity (as we did in step 1). Then rearrange.</p>
<ul>
<li><img alt="\\mathbb{E}[Y^0(2)-Y^0(1)|A=1]=\\mathbb{E}[Y^0(2)-Y^0(1)|A=0]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5BY%5E0%282%29-Y%5E0%281%29%7CA%3D1%5D%3D%5Cmathbb%7BE%7D%5BY%5E0%282%29-Y%5E0%281%29%7CA%3D0%5D" title="\\mathbb{E}[Y^0(2)-Y^0(1)|A=1]=\\mathbb{E}[Y^0(2)-Y^0(1)|A=0]"></li>
<li><img alt="\\mathbb{E}[Y^0(2)|A=1]-\\mathbb{E}[Y^0(1)|A=1]=\\mathbb{E}[Y^0(2)|A=0]-\\mathbb{E}[Y^0(1)|A=0]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5BY%5E0%282%29%7CA%3D1%5D-%5Cmathbb%7BE%7D%5BY%5E0%281%29%7CA%3D1%5D%3D%5Cmathbb%7BE%7D%5BY%5E0%282%29%7CA%3D0%5D-%5Cmathbb%7BE%7D%5BY%5E0%281%29%7CA%3D0%5D" title="\\mathbb{E}[Y^0(2)|A=1]-\\mathbb{E}[Y^0(1)|A=1]=\\mathbb{E}[Y^0(2)|A=0]-\\mathbb{E}[Y^0(1)|A=0]"></li>
<li><img alt="\\mathbb{E}[Y^0(2)|A=1]= \\mathbb{E}[Y^0(2)|A=0]- \\mathbb{E}[Y^0(1)|A=0] + \\mathbb{E}[Y^0(1)|A=1]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5BY%5E0%282%29%7CA%3D1%5D%3D%20%5Cmathbb%7BE%7D%5BY%5E0%282%29%7CA%3D0%5D-%20%5Cmathbb%7BE%7D%5BY%5E0%281%29%7CA%3D0%5D%20%2B%20%5Cmathbb%7BE%7D%5BY%5E0%281%29%7CA%3D1%5D" title="\\mathbb{E}[Y^0(2)|A=1]= \\mathbb{E}[Y^0(2)|A=0]- \\mathbb{E}[Y^0(1)|A=0] + \\mathbb{E}[Y^0(1)|A=1]"></li>
</ul>
<ol>
<li></li>
</ol>
<pre><code>![= {(\mathbb{E}\[Y^1(2)-Y^0(1)\|A=1\]) - (\mathbb{E}\[Y^0(2)-Y^0(1)\|A=0\] )} \text{ (Rearranging terms)}](https://latex.codecogs.com/svg.latex?%3D%20%7B%28%5Cmathbb%7BE%7D%5BY%5E1%282%29-Y%5E0%281%29%7CA%3D1%5D%29%20-%20%28%5Cmathbb%7BE%7D%5BY%5E0%282%29-Y%5E0%281%29%7CA%3D0%5D%20%29%7D%20%5Ctext%7B%20%28Rearranging%20terms%29%7D &quot;= {(\mathbb{E}[Y^1(2)-Y^0(1)|A=1]) - (\mathbb{E}[Y^0(2)-Y^0(1)|A=0] )} \text{ (Rearranging terms)}&quot;)
</code></pre>
<ol start="2">
<li></li>
</ol>
<pre><code>![= (\mathbb{E}\[Y(2)-Y(1)\|A=1\]) - (\mathbb{E}\[Y(2)-Y(1)\|A=0\] ) \text{ (Consistency Assumption)}](https://latex.codecogs.com/svg.latex?%3D%20%28%5Cmathbb%7BE%7D%5BY%282%29-Y%281%29%7CA%3D1%5D%29%20-%20%28%5Cmathbb%7BE%7D%5BY%282%29-Y%281%29%7CA%3D0%5D%20%29%20%5Ctext%7B%20%28Consistency%20Assumption%29%7D &quot;= (\mathbb{E}[Y(2)-Y(1)|A=1]) - (\mathbb{E}[Y(2)-Y(1)|A=0] ) \text{ (Consistency Assumption)}&quot;)
</code></pre>
<p>The Consistency Assumption is written mathematically as <img alt="Y(t)=(1−A)\\times Y^0(t) + A \\times Y^1(t)" src="https://latex.codecogs.com/svg.latex?Y%28t%29%3D%281%E2%88%92A%29%5Ctimes%20Y%5E0%28t%29%20%2B%20A%20%5Ctimes%20Y%5E1%28t%29" title="Y(t)=(1−A)\\times Y^0(t) + A \\times Y^1(t)">, Every unit has two potential outcomes, but we only observe one &mdash; the one corresponding to their actual treatment status. The consistency assumption links the potential outcomes with the real world outcomes. That is, it links <img alt="Y^a(t)" src="https://latex.codecogs.com/svg.latex?Y%5Ea%28t%29" title="Y^a(t)"> at time <img alt="t" src="https://latex.codecogs.com/svg.latex?t" title="t"> with treatment <img alt="a" src="https://latex.codecogs.com/svg.latex?a" title="a"> to the observed outcomes <img alt="Y(t)" src="https://latex.codecogs.com/svg.latex?Y%28t%29" title="Y(t)">. If a unit is treated <img alt="(A=1)" src="https://latex.codecogs.com/svg.latex?%28A%3D1%29" title="(A=1)">, then the observed outcome is the potential outcome with treatment <img alt="Y(t)=Y^1(t)" src="https://latex.codecogs.com/svg.latex?Y%28t%29%3DY%5E1%28t%29" title="Y(t)=Y^1(t)"> and the potential outcome with no treatment <img alt="Y^0(t)" src="https://latex.codecogs.com/svg.latex?Y%5E0%28t%29" title="Y^0(t)"> is unobserved. If a unit is not treated <img alt="(A=0)" src="https://latex.codecogs.com/svg.latex?%28A%3D0%29" title="(A=0)">, then <img alt="Y(t)=Y^0(t)" src="https://latex.codecogs.com/svg.latex?Y%28t%29%3DY%5E0%28t%29" title="Y(t)=Y^0(t)"> and <img alt="Y^1(t)" src="https://latex.codecogs.com/svg.latex?Y%5E1%28t%29" title="Y^1(t)"> is unobserved</p>
<p>Let&rsquo;s write this a slightly different way to show what the parallel trends assumption is doing.</p>
<p>In the proof below the <img alt="\\color{green}{\\text{green}}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bgreen%7D%7B%5Ctext%7Bgreen%7D%7D" title="\\color{green}{\\text{green}}"> terms are estimable and the <img alt="\\color{red}{\\text{red}}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bred%7D%7B%5Ctext%7Bred%7D%7D" title="\\color{red}{\\text{red}}"> ones are not. This idea is shamelessly stolen from <a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a>. Since the notation differs again, here <img alt="Y_t(g)" src="https://latex.codecogs.com/svg.latex?Y_t%28g%29" title="Y_t(g)"> is the potential outcome at period <img alt="t" src="https://latex.codecogs.com/svg.latex?t" title="t"> given units were exposed to treatment in period <img alt="g" src="https://latex.codecogs.com/svg.latex?g" title="g">. If <img alt="g=\\infty" src="https://latex.codecogs.com/svg.latex?g%3D%5Cinfty" title="g=\\infty"> this means that the units were never exposed to treatment.</p>
<ol>
<li></li>
</ol>
<pre><code>![\text{ATT} \equiv \underbrace{\mathbb{E}\[Y\_{i,t=2}(2)\|G_i=1\]}\_{\color{green}{\text{estimable from the data}}} - \underbrace{\mathbb{E}\[Y\_{i,t=2}(\infty)\|G_i=0\]}\_{\color{red}{\text{counterfactual / not estimable from the data}}}](https://latex.codecogs.com/svg.latex?%5Ctext%7BATT%7D%20%5Cequiv%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY_%7Bi%2Ct%3D2%7D%282%29%7CG_i%3D1%5D%7D_%7B%5Ccolor%7Bgreen%7D%7B%5Ctext%7Bestimable%20from%20the%20data%7D%7D%7D%20-%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY_%7Bi%2Ct%3D2%7D%28%5Cinfty%29%7CG_i%3D0%5D%7D_%7B%5Ccolor%7Bred%7D%7B%5Ctext%7Bcounterfactual%20%2F%20not%20estimable%20from%20the%20data%7D%7D%7D &quot;\text{ATT} \equiv \underbrace{\mathbb{E}[Y_{i,t=2}(2)|G_i=1]}_{\color{green}{\text{estimable from the data}}} - \underbrace{\mathbb{E}[Y_{i,t=2}(\infty)|G_i=0]}_{\color{red}{\text{counterfactual / not estimable from the data}}}&quot;)
</code></pre>
<p><img alt="= \\color{green}{\\mathbb{E}[Y_{i,t=2}|G_i=1]} - \\color{red}{\\mathbb{E}[Y_{i,t=2}(\\infty)|G_i=0]}" src="https://latex.codecogs.com/svg.latex?%3D%20%5Ccolor%7Bgreen%7D%7B%5Cmathbb%7BE%7D%5BY_%7Bi%2Ct%3D2%7D%7CG_i%3D1%5D%7D%20-%20%5Ccolor%7Bred%7D%7B%5Cmathbb%7BE%7D%5BY_%7Bi%2Ct%3D2%7D%28%5Cinfty%29%7CG_i%3D0%5D%7D" title="= \\color{green}{\\mathbb{E}[Y_{i,t=2}|G_i=1]} - \\color{red}{\\mathbb{E}[Y_{i,t=2}(\\infty)|G_i=0]}"></p>
<p>The <img alt="\\color{green}{\\text{green object}}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bgreen%7D%7B%5Ctext%7Bgreen%20object%7D%7D" title="\\color{green}{\\text{green object}}"> is estimable from data. The <img alt="\\color{red}{\\text{red object}}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bred%7D%7B%5Ctext%7Bred%20object%7D%7D" title="\\color{red}{\\text{red object}}"> still depends on potential outcomes, and our goal is to find ways to &lsquo;impute&rsquo; it. This is where PT and no-anticipation come into play!</p>
<h4 id="summing-up-att">Summing Up ATT</h4>
<p>The causal effect we&rsquo;re after is:</p>
<p><img alt="\\text{ATT} = \\tau_2 = \\mathbb{E}[\\underbrace{Y_{i,2}(1)}_{\\text{Observable}}-\\underbrace{Y_{i,2}(0)}_{\\text{Unobservable}}\\underbrace{|D=1}_{\\text{ Given treated}}]" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BATT%7D%20%3D%20%5Ctau_2%20%3D%20%5Cmathbb%7BE%7D%5B%5Cunderbrace%7BY_%7Bi%2C2%7D%281%29%7D_%7B%5Ctext%7BObservable%7D%7D-%5Cunderbrace%7BY_%7Bi%2C2%7D%280%29%7D_%7B%5Ctext%7BUnobservable%7D%7D%5Cunderbrace%7B%7CD%3D1%7D_%7B%5Ctext%7B%20Given%20treated%7D%7D%5D" title="\\text{ATT} = \\tau_2 = \\mathbb{E}[\\underbrace{Y_{i,2}(1)}_{\\text{Observable}}-\\underbrace{Y_{i,2}(0)}_{\\text{Unobservable}}\\underbrace{|D=1}_{\\text{ Given treated}}]"></p>
<p>This is identified as:</p>
<p><img alt="\\tau_2 = \\underbrace{\\mathbb{E}[Y_{i,2}{\\color{blue}{\\textbf{-}}}Y_{i,1}|D=1]}_{\\text{Change for} D_i = 1} {\\color{green}{\\textbf{-}}} \\underbrace{\\mathbb{E}[Y_{i,2}{\\color{blue}{\\textbf{-}}}Y_{i,1}|D=0]}_{\\text{Change for} D_i = 0}" src="https://latex.codecogs.com/svg.latex?%5Ctau_2%20%3D%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY_%7Bi%2C2%7D%7B%5Ccolor%7Bblue%7D%7B%5Ctextbf%7B-%7D%7D%7DY_%7Bi%2C1%7D%7CD%3D1%5D%7D_%7B%5Ctext%7BChange%20for%7D%20D_i%20%3D%201%7D%20%7B%5Ccolor%7Bgreen%7D%7B%5Ctextbf%7B-%7D%7D%7D%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY_%7Bi%2C2%7D%7B%5Ccolor%7Bblue%7D%7B%5Ctextbf%7B-%7D%7D%7DY_%7Bi%2C1%7D%7CD%3D0%5D%7D_%7B%5Ctext%7BChange%20for%7D%20D_i%20%3D%200%7D" title="\\tau_2 = \\underbrace{\\mathbb{E}[Y_{i,2}{\\color{blue}{\\textbf{-}}}Y_{i,1}|D=1]}_{\\text{Change for} D_i = 1} {\\color{green}{\\textbf{-}}} \\underbrace{\\mathbb{E}[Y_{i,2}{\\color{blue}{\\textbf{-}}}Y_{i,1}|D=0]}_{\\text{Change for} D_i = 0}"></p>
<p>This is the <img alt="\\color{green}{\\text{difference}}\\text{-in-}\\color{blue}{\\text{differences}}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bgreen%7D%7B%5Ctext%7Bdifference%7D%7D%5Ctext%7B-in-%7D%5Ccolor%7Bblue%7D%7B%5Ctext%7Bdifferences%7D%7D" title="\\color{green}{\\text{difference}}\\text{-in-}\\color{blue}{\\text{differences}}"> of population means!</p>
<p>Good news: The static specification yields a sensible estimand when there is no heterogeneity in treatment effects across either time or units.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup> That is if <img alt="\\underbrace{Y_{it}}_{\\text{unit i and time t}}(\\underbrace{g}_{\\text{treatment}})−Y_{it}(\\underbrace{\\infty}_{\\text{Never treated}}) \\equiv \\tau" src="https://latex.codecogs.com/svg.latex?%5Cunderbrace%7BY_%7Bit%7D%7D_%7B%5Ctext%7Bunit%20i%20and%20time%20t%7D%7D%28%5Cunderbrace%7Bg%7D_%7B%5Ctext%7Btreatment%7D%7D%29%E2%88%92Y_%7Bit%7D%28%5Cunderbrace%7B%5Cinfty%7D_%7B%5Ctext%7BNever%20treated%7D%7D%29%20%5Cequiv%20%5Ctau" title="\\underbrace{Y_{it}}_{\\text{unit i and time t}}(\\underbrace{g}_{\\text{treatment}})−Y_{it}(\\underbrace{\\infty}_{\\text{Never treated}}) \\equiv \\tau"> then <img alt="\\beta = \\tau" src="https://latex.codecogs.com/svg.latex?%5Cbeta%20%3D%20%5Ctau" title="\\beta = \\tau"> from <img alt="Y_{it} =\\alpha_i +\\phi_t +D_{it}\\beta+\\epsilon_{it}" src="https://latex.codecogs.com/svg.latex?Y_%7Bit%7D%20%3D%5Calpha_i%20%2B%5Cphi_t%20%2BD_%7Bit%7D%5Cbeta%2B%5Cepsilon_%7Bit%7D" title="Y_{it} =\\alpha_i +\\phi_t +D_{it}\\beta+\\epsilon_{it}">. We can estimate the causal effect of a policy on an outcome! In different words, in the simple two-period model, the estimand (population coefficient) of the two-way fixed effects specification corresponds with the ATT under the parallel trends and no anticipation assumptions.<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup></p>
<p>Bad news: the assumptions are rarely met. We delve into this in after discussing estimators.</p>
<h3 id="estimators-of-the-att">Estimators of the ATT</h3>
<p>So, how do we apply this to the data? For a straightforward estimate of the ATT, we could simply plug in the sample averages for the four expectations on the right-hand side:</p>
<ul>
<li>
<p>The post-intervention average (<img alt="\\bar{Y}_{1,2}" src="https://latex.codecogs.com/svg.latex?%5Cbar%7BY%7D_%7B1%2C2%7D" title="\\bar{Y}_{1,2}">) of the treated group for <img alt="\\mathbb{E}[Y(2)|A=1]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5BY%282%29%7CA%3D1%5D" title="\\mathbb{E}[Y(2)|A=1]"></p>
</li>
<li>
<p>The pre-intervention average (<img alt="\\bar{Y}_{1,1}" src="https://latex.codecogs.com/svg.latex?%5Cbar%7BY%7D_%7B1%2C1%7D" title="\\bar{Y}_{1,1}">) of the treated group for <img alt="\\mathbb{E}[Y(1)|A=1]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5BY%281%29%7CA%3D1%5D" title="\\mathbb{E}[Y(1)|A=1]"></p>
</li>
<li>
<p>The post-intervention average (<img alt="\\bar{Y}_{0,2}" src="https://latex.codecogs.com/svg.latex?%5Cbar%7BY%7D_%7B0%2C2%7D" title="\\bar{Y}_{0,2}">) of the control group for <img alt="\\mathbb{E}[Y(2)|A=0]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5BY%282%29%7CA%3D0%5D" title="\\mathbb{E}[Y(2)|A=0]"></p>
</li>
<li>
<p>The pre-intervention average (<img alt="\\bar{Y}_{0,1}" src="https://latex.codecogs.com/svg.latex?%5Cbar%7BY%7D_%7B0%2C1%7D" title="\\bar{Y}_{0,1}">) of the control group for <img alt="\\mathbb{E}[Y(1)|A=0]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5BY%281%29%7CA%3D0%5D" title="\\mathbb{E}[Y(1)|A=0]"></p>
</li>
</ul>
<p>This can be written as:</p>
<p><img alt="\\hat{\\tau}_{DiD}=(\\bar{Y}_{1,2}-\\bar{Y}_{1,1})-(\\bar{Y}_{0,2}-\\bar{Y}_{0,1})" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Ctau%7D_%7BDiD%7D%3D%28%5Cbar%7BY%7D_%7B1%2C2%7D-%5Cbar%7BY%7D_%7B1%2C1%7D%29-%28%5Cbar%7BY%7D_%7B0%2C2%7D-%5Cbar%7BY%7D_%7B0%2C1%7D%29" title="\\hat{\\tau}_{DiD}=(\\bar{Y}_{1,2}-\\bar{Y}_{1,1})-(\\bar{Y}_{0,2}-\\bar{Y}_{0,1})"></p>
<p>Where <img alt="\\bar{Y}_{dt}" src="https://latex.codecogs.com/svg.latex?%5Cbar%7BY%7D_%7Bdt%7D" title="\\bar{Y}_{dt}"> is sample mean for group <img alt="d" src="https://latex.codecogs.com/svg.latex?d" title="d"> in period <img alt="t" src="https://latex.codecogs.com/svg.latex?t" title="t"></p>
<p>Conveniently,<img alt="\\hat{\\tau}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Ctau%7D" title="\\hat{\\tau}"> is algebraically equal to OLS coefficient <img alt="\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D" title="\\hat{\\beta}"> from</p>
<p><img alt="Y_{i,t} =\\alpha_i +\\phi_t +D_{i,t}\\beta_{post}+\\epsilon_{i,t}" src="https://latex.codecogs.com/svg.latex?Y_%7Bi%2Ct%7D%20%3D%5Calpha_i%20%2B%5Cphi_t%20%2BD_%7Bi%2Ct%7D%5Cbeta_%7Bpost%7D%2B%5Cepsilon_%7Bi%2Ct%7D" title="Y_{i,t} =\\alpha_i +\\phi_t +D_{i,t}\\beta_{post}+\\epsilon_{i,t}"></p>
<p>where <img alt="D_{i,t} =D_i \\times 1[t=2]" src="https://latex.codecogs.com/svg.latex?D_%7Bi%2Ct%7D%20%3DD_i%20%5Ctimes%201%5Bt%3D2%5D" title="D_{i,t} =D_i \\times 1[t=2]"></p>
<p>This is also equivalent to a first-differences model:</p>
<p><img alt="\\Delta Y_i =\\alpha+\\Delta D_i \\beta+u_{it}" src="https://latex.codecogs.com/svg.latex?%5CDelta%20Y_i%20%3D%5Calpha%2B%5CDelta%20D_i%20%5Cbeta%2Bu_%7Bit%7D" title="\\Delta Y_i =\\alpha+\\Delta D_i \\beta+u_{it}"></p>
<p>Note that above the <img alt="\\Delta D_i" src="https://latex.codecogs.com/svg.latex?%5CDelta%20D_i" title="\\Delta D_i"> is one for switchers (i.e. untreated to treated) and zero for stayers (i.e. undtreated to untreated). We use the <img alt="\\Delta" src="https://latex.codecogs.com/svg.latex?%5CDelta" title="\\Delta"> notation to note that this is a &ldquo;first-differenced&rdquo; model.</p>
<p>The code below shows that in a <img alt="2 \\times 2" src="https://latex.codecogs.com/svg.latex?2%20%5Ctimes%202" title="2 \\times 2"> these are all equivalent.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">modelsummary</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">kableExtra</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">gt</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">skimr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># simulated simple DID</span>
</span></span><span class="line"><span class="cl"><span class="c1"># treated is the group dummy!  (if you have treated</span>
</span></span><span class="line"><span class="cl"><span class="c1"># as a dummy that only activates for the treated group</span>
</span></span><span class="line"><span class="cl"><span class="c1"># at the treated time, then your regression doesnt </span>
</span></span><span class="line"><span class="cl"><span class="c1"># run)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">treated1_pre</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="m">10</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">treated1_post</span> <span class="o">&lt;-</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="m">20</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">control_pre</span> <span class="o">&lt;-</span>  <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="m">5</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">control_post</span> <span class="o">&lt;-</span>  <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="m">1000</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="m">5</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="n">treated1_pre</span><span class="p">,</span> <span class="n">treated1_post</span><span class="p">,</span> <span class="n">control_pre</span><span class="p">,</span> <span class="n">control_post</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">simulated_did_data</span> <span class="o">&lt;-</span> <span class="nf">tibble</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="n">id</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">,</span> <span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">,</span> <span class="m">1001</span><span class="o">:</span><span class="m">2000</span><span class="p">,</span> <span class="m">1001</span><span class="o">:</span><span class="m">2000</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">  <span class="n">y</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="n">treated1_pre</span><span class="p">,</span> <span class="n">treated1_post</span><span class="p">,</span> <span class="n">control_pre</span><span class="p">,</span> <span class="n">control_post</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">  <span class="n">g</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">2</span><span class="p">,</span> <span class="m">1000</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="n">time</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1000</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="n">treated</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1000</span><span class="p">),</span> <span class="nf">rep</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1000</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">  <span class="n">did</span> <span class="o">=</span> <span class="n">time</span> <span class="o">*</span> <span class="n">treated</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># First differnce model, show </span>
</span></span><span class="line"><span class="cl"><span class="n">first_differenced</span> <span class="o">&lt;-</span> <span class="n">simulated_did_data</span> <span class="o">%&gt;%</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">group_by</span><span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="o">%&gt;%</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">mutate</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span><span class="o">-</span><span class="nf">lag</span><span class="p">(</span><span class="n">y</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">    <span class="n">did</span> <span class="o">=</span> <span class="n">treated</span> <span class="p">)</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">ungroup</span><span class="p">()</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">filter</span><span class="p">(</span><span class="o">!</span><span class="nf">is.na</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">first_differenced</span> <span class="o">%&gt;%</span> 
</span></span><span class="line"><span class="cl">  <span class="nf">ungroup</span><span class="p">()</span> <span class="o">%&gt;%</span>
</span></span><span class="line"><span class="cl">  <span class="nf">skim</span><span class="p">()</span>
</span></span></code></pre></div><table style="width: auto;" class="table table-condensed">
<caption>
Data summary
</caption>
<tbody>
<tr>
<td style="text-align:left;">
Name
</td>
<td style="text-align:left;">
Piped data
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of rows
</td>
<td style="text-align:left;">
4000
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of columns
</td>
<td style="text-align:left;">
7
</td>
</tr>
<tr>
<td style="text-align:left;">
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Column type frequency:
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:left;">
7
</td>
</tr>
<tr>
<td style="text-align:left;">
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Group variables
</td>
<td style="text-align:left;">
None
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
p0
</th>
<th style="text-align:right;">
p25
</th>
<th style="text-align:right;">
p50
</th>
<th style="text-align:right;">
p75
</th>
<th style="text-align:right;">
p100
</th>
<th style="text-align:left;">
hist
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
id
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1000.5
</td>
<td style="text-align:right;">
577.42
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
500.75
</td>
<td style="text-align:right;">
1000.50
</td>
<td style="text-align:right;">
1500.25
</td>
<td style="text-align:right;">
2000.00
</td>
<td style="text-align:left;">
▇▇▇▇▇
</td>
</tr>
<tr>
<td style="text-align:left;">
y
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
10.0
</td>
<td style="text-align:right;">
11.63
</td>
<td style="text-align:right;">
-29.14
</td>
<td style="text-align:right;">
2.01
</td>
<td style="text-align:right;">
9.89
</td>
<td style="text-align:right;">
17.60
</td>
<td style="text-align:right;">
52.36
</td>
<td style="text-align:left;">
▁▅▇▃▁
</td>
</tr>
<tr>
<td style="text-align:left;">
g
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1.5
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.50
</td>
<td style="text-align:right;">
2.00
</td>
<td style="text-align:right;">
2.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
time
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
treated
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
did
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.50
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:right;">
1.00
</td>
<td style="text-align:left;">
▇▁▁▁▇
</td>
</tr>
<tr>
<td style="text-align:left;">
diff
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:left;">
▁▁▇▁▁
</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">models</span> <span class="o">&lt;-</span> <span class="nf">list</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="s">&#34;one period&#34;</span> <span class="o">=</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">simulated_did_data</span><span class="p">,</span> <span class="n">formula</span> <span class="o">=</span> <span class="n">y</span> <span class="o">~</span> <span class="n">time</span> <span class="o">+</span> <span class="n">treated</span> <span class="o">+</span> <span class="n">did</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">  <span class="s">&#34;first differenced&#34;</span> <span class="o">=</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">first_differenced</span><span class="p">,</span><span class="n">formula</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">~</span> <span class="n">did</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model1</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">simulated_did_data</span><span class="p">,</span> <span class="n">formula</span> <span class="o">=</span> <span class="n">y</span> <span class="o">~</span> <span class="n">time</span> <span class="o">+</span> <span class="n">treated</span> <span class="o">+</span> <span class="n">did</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">fd</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">first_differenced</span><span class="p">,</span><span class="n">formula</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">~</span> <span class="n">did</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Show that forumla above works </span>
</span></span><span class="line"><span class="cl"><span class="n">model1</span><span class="o">$</span><span class="n">coefficients[4]</span>
</span></span></code></pre></div><pre><code>     did 
10.20638 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">fd</span><span class="o">$</span><span class="n">coefficients[2]</span>
</span></span></code></pre></div><pre><code>did 
  0 
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># this is the same!</span>
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="nf">mean</span><span class="p">(</span><span class="n">treated1_post</span><span class="p">)</span> <span class="o">-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">treated1_pre</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="nf">mean</span><span class="p">(</span><span class="n">control_post</span><span class="p">)</span><span class="o">-</span> <span class="nf">mean</span><span class="p">(</span><span class="n">control_pre</span><span class="p">))</span>
</span></span></code></pre></div><pre><code>[1] 10.20638
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># FWL decomposition - for negative weights discussed later</span>
</span></span><span class="line"><span class="cl"><span class="n">aux_regression</span> <span class="o">&lt;-</span> <span class="nf">lm</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">simulated_did_data</span><span class="p">,</span> <span class="n">formula</span> <span class="o">=</span> <span class="n">did</span> <span class="o">~</span> <span class="n">time</span> <span class="o">+</span> <span class="n">treated</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">cov</span><span class="p">(</span><span class="n">model1</span><span class="o">$</span><span class="n">model</span><span class="o">$</span><span class="n">y</span><span class="p">,</span> <span class="n">aux_regression</span><span class="o">$</span><span class="n">residuals</span><span class="p">)</span><span class="o">/</span><span class="nf">var</span><span class="p">(</span><span class="n">aux_regression</span><span class="o">$</span><span class="n">residuals</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>[1] 10.20638
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">aux_regression</span><span class="o">$</span><span class="n">fitted.values</span> <span class="o">%&gt;%</span> <span class="n">tibble</span> <span class="o">%&gt;%</span> <span class="nf">range</span><span class="p">()</span>
</span></span></code></pre></div><pre><code>[1] -0.25  0.75
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">modelsummary</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</span></span></code></pre></div><table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center">one period</th>
<th style="text-align:center">first differenced</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">(Intercept)</td>
<td style="text-align:center">5.091</td>
<td style="text-align:center">0.000</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(0.314)</td>
<td style="text-align:center">(0.000)</td>
</tr>
<tr>
<td style="text-align:left">time</td>
<td style="text-align:center">0.009</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(0.443)</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">treated</td>
<td style="text-align:center">4.696</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(0.443)</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">did</td>
<td style="text-align:center">10.206</td>
<td style="text-align:center">0.000</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">(0.627)</td>
<td style="text-align:center">(0.000)</td>
</tr>
<tr>
<td style="text-align:left">Num.Obs.</td>
<td style="text-align:center">4000</td>
<td style="text-align:center">4000</td>
</tr>
<tr>
<td style="text-align:left">R2</td>
<td style="text-align:center">0.274</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">R2 Adj.</td>
<td style="text-align:center">0.273</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">AIC</td>
<td style="text-align:center">29710.4</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">BIC</td>
<td style="text-align:center">29741.9</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Log.Lik.</td>
<td style="text-align:center">−14850.198</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">F</td>
<td style="text-align:center">502.448</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">RMSE</td>
<td style="text-align:center">9.91</td>
<td style="text-align:center">0.00</td>
</tr>
</tbody>
</table>
<p>So, this is all well and good, right? If we have two groups and meet the assumptions of parallel trends and no anticipation then yes. If, however, we have more than two groups and more than two periods, treatment effects can change. This presents a big problem!</p>
<h3 id="relaxing-did-assumptions-overview">Relaxing DiD Assumptions Overview</h3>
<p>The Roth et al. (2023) paper provides a good overview of the advances in the DiD literature. This section is largely stolen from them. I&rsquo;ll cite other literature, so assume this section is Roth et al (2023) unless noted otherwise. Like the first example is from a presentation by Sant&rsquo;Anna</p>
<p>Many DiD empirical applications, however, deviate from the standard DiD setup:</p>
<ul>
<li>Availability of covariates X</li>
<li>More than two time periods</li>
<li>Variation in treatment timing</li>
<li>Non-binary treatments</li>
<li>Parallel trends may not hold exactly</li>
<li>Only a few treated and untreated clusters are available</li>
</ul>
<p>We can group the recent innovations in DiD lit by which elements of the canonical model they relax:</p>
<ul>
<li>Multiple periods and staggered treatment timing</li>
<li>Relaxing or allowing PT to be violated</li>
<li>Inference with a small number of clusters</li>
</ul>
<p>A common theme is that these new estimators isolate &ldquo;clean&rdquo; comparisons between treated and not-yet-treated groups, and then aggregate them using user-specified weights to estimate a target parameter of economic interest.</p>
<p>Recall that in the simple two-period model, the estimand (population coefficient) of the two-way fixed effects specification corresponds with the ATT under the parallel trends and no anticipation assumptions. A substantial focus of the recent literature has been whether the estimand of commonly-used generalizations of this TWFE model to the multi-period, staggered timing case have a similar, intuitive causal interpretation. In short, the literature has shown that the estimand of TWFE specifications in the staggered setting often does not correspond with an intuitive causal parameter even under the natural extensions of the parallel trends and no-anticipation assumptions described above.</p>
<h4 id="estimators-that-alleviate-twfe-issues">Estimators that alleviate TWFE Issues</h4>
<p>Use R package did for the Callaway and Sant&rsquo;Anna (2021) solution</p>
<p>This section shamelessly stolen from Pedro Sant&rsquo;Anna&rsquo;s presentation at Population Dynamics and Health Program Workshop at the University of Michigan: <a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a></p>
<ul>
<li>
<p>Callaway and Sant&rsquo;Anna (2021) provides high-level conditions for one to consider more general first-step estimators that allows for covariates and some flexible &ldquo;data-adaptive&rdquo; (machine learning) procedures.</p>
</li>
<li>
<p>Sun and Abraham (2021): Proposed estimator coincides with CS when there are no covariates and use the never-treated/last-treated cohort as a comparison group. However, this paper has many other results about the pitfalls of TWFE that are not in CS.</p>
</li>
<li>
<p>Gardner (2021), Borusyak et al. (2021) and Wooldridge (2021b): Propose &ldquo;imputation&rdquo;/regression based methods to recover cohort-time ATT&rsquo;s . These three papers do not nest nor is nested by CS, but identification assumptions are sometimes stronger. Benefit: more precise estimates when these assumptions are correct.</p>
</li>
<li>
<p>Wooldridge (2021a): Propose estimators that are suitable for nonlinear models. It relies on alternative types of parallel trends assumptions, e.g. &lsquo;ratio-in-ratios&quot; if exponential model. If use canonical link functions, standard errors can be easily estimated</p>
</li>
<li>
<p>de Chaisemartin and D&rsquo;Haultfœuille (2020, 2021): Estimator coincides with CS when there are no covariates, uses not-yet-treated units as comparison group, and treatment is staggered. However, these two papers allow for treatment turning on-off, which is not allowed in CS. de Chaisemartin and D&rsquo;Haultfœuille (2020), though, rules out dynamic treatment effects. When covariates are available, these papers do not nest nor are nested by CS. However, they seem to implicitly impose homogeneity assumptions wrt to X (e.g., ATT does not vary according to age).</p>
</li>
<li>
<p>Roth and Sant&rsquo;Anna (2021): When treatment timing is as-good-as-random, we can do much better than DiD in terms of efficiency. However, it requires more than PT.</p>
</li>
<li>
<p>Callaway and Sant&rsquo;Anna also propose an an alogous estimator using not-yet-treated rather than never-treated units.</p>
</li>
<li>
<p>Sun and Abraham (2021) propose a similar estimator but with different comparisons groups (e.g. using last-to-be treated rather than not-yet-treated)</p>
</li>
<li>
<p>Borusyak et. al.(2021), Wooldridge (2021), Gardner (2021) propose &ldquo;imputation&rdquo; estimators that estimate the counterfactual <img alt="Y_{it}(0)" src="https://latex.codecogs.com/svg.latex?Y_%7Bit%7D%280%29" title="Y_{it}(0)"> using a TWFE model that is fit using only pre-treatment data</p>
<ul>
<li>Main difference from C&amp;S is that this uses more pre-treatment periods,not just period g−1</li>
<li>This can sometimes bemore efficient (if outcome not too serially correlated), but also relies on a stronger PT assumption that may be more susceptible to bias</li>
</ul>
</li>
<li>
<p>Roth and Sant&rsquo;Anna (2021) show that you can get even more precise estimates if you&rsquo;re willing to assume treatment timing is &ldquo;as good as random&rdquo;</p>
</li>
</ul>
<p>Advice from Roth In most cases, using the &ldquo;new&rdquo; DiD methods will not lead to a big change in your results (empirically, TE heterogeneity is not that large in most cases) - The exceptions are cases where there are periods where almost all units are treated&ndash; this is when &ldquo;forbidden comparisons&rdquo; get the most weight</p>
<h4 id="work-in-progress-below-here">Work in progress below here</h4>
<h4 id="multiple-periods-and-variation-in-treatment-timing">Multiple Periods and variation in treatment timing</h4>
<p>Stolen from <a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a></p>
<p>With multiple time periods and variation in treatment timing, TWFE does not respect our assumptions: OLS is &ldquo;variational hungry&rdquo; and makes many comparisons of means Some of these comparisons are bad: use already-treated units as a comparison group to &ldquo;later-treated&rdquo; groups This can lead to &ldquo;negative weighting&rdquo; problems. Solution to the TWFE problem is simple Separate the identification, aggregation and estimation/inference parts of the problem Use ATT(g,t) as a building block so we can transparently see how things are constructed Many different aggregation schemes are possible: they deliver different parameters! Can allow for covariates via regressions adjustments, IPW and DR.</p>
<p>From a technical point of view (read for example Goodman-Bacon 2019), the traditional TWFE model obtains a parameter for TE that is the average of all possible 2x2 designs that could be constructed from the above matrix. However, not all of them are good ones!</p>
<ul>
<li>
<p>The intuition for these negative results is that the TWFE OLS specification combines two sources of comparisons:</p>
<ol>
<li>
<p><strong>Clean comparisons:</strong> DiD&rsquo;s between treated and not-yet-treated units</p>
</li>
<li>
<p><strong>Forbidden comparisons:</strong> DiD&rsquo;s between two sets of already-treated units (who began treatment at different times)</p>
</li>
</ol>
</li>
<li>
<p>These forbidden comparisons can lead to negative weights: the &ldquo;control group&rdquo; is already treated, so we run into problems if their treatment effects change over time</p>
</li>
<li>
<p>Consider the two period model, except suppose now that our two groups are <strong>always-treated</strong> units (treated in both periods) and <strong>switchers</strong> (treated only in period 2)</p>
</li>
<li>
<p>With two periods, the coefficient <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> from <img alt="Y_{it} = \\alpha_i + \\phi_t + D_{it} \\beta + \\epsilon_{it}" src="https://latex.codecogs.com/svg.latex?Y_%7Bit%7D%20%3D%20%5Calpha_i%20%2B%20%5Cphi_t%20%2B%20D_%7Bit%7D%20%5Cbeta%20%2B%20%5Cepsilon_%7Bit%7D" title="Y_{it} = \\alpha_i + \\phi_t + D_{it} \\beta + \\epsilon_{it}"> is the same as from the first-differenced regression <img alt="\\Delta Y_i = \\alpha + \\Delta D_i \\beta + u_i" src="https://latex.codecogs.com/svg.latex?%5CDelta%20Y_i%20%3D%20%5Calpha%20%2B%20%5CDelta%20D_i%20%5Cbeta%20%2B%20u_i" title="\\Delta Y_i = \\alpha + \\Delta D_i \\beta + u_i"></p>
</li>
<li>
<p>Observe that <img alt="\\Delta D_i" src="https://latex.codecogs.com/svg.latex?%5CDelta%20D_i" title="\\Delta D_i"> is one for switchers and zero for stayers. That is, the stayers are the control group! Thus,</p>
<p><img alt="\\hat{\\beta}=\\underbrace{(\\bar{Y}_{\\text{Switchers,2}}-\\bar{Y}_{\\text{Switchers,1}})}_{\\text{Change for switchers}}-\\underbrace{(\\color{red}{\\bar{Y}_{\\text{Always Treated,2}}}-\\bar{Y}_{\\text{Always Treated,1}})}_{\\text{Change for Always Treated}}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D%3D%5Cunderbrace%7B%28%5Cbar%7BY%7D_%7B%5Ctext%7BSwitchers%2C2%7D%7D-%5Cbar%7BY%7D_%7B%5Ctext%7BSwitchers%2C1%7D%7D%29%7D_%7B%5Ctext%7BChange%20for%20switchers%7D%7D-%5Cunderbrace%7B%28%5Ccolor%7Bred%7D%7B%5Cbar%7BY%7D_%7B%5Ctext%7BAlways%20Treated%2C2%7D%7D%7D-%5Cbar%7BY%7D_%7B%5Ctext%7BAlways%20Treated%2C1%7D%7D%29%7D_%7B%5Ctext%7BChange%20for%20Always%20Treated%7D%7D" title="\\hat{\\beta}=\\underbrace{(\\bar{Y}_{\\text{Switchers,2}}-\\bar{Y}_{\\text{Switchers,1}})}_{\\text{Change for switchers}}-\\underbrace{(\\color{red}{\\bar{Y}_{\\text{Always Treated,2}}}-\\bar{Y}_{\\text{Always Treated,1}})}_{\\text{Change for Always Treated}}"></p>
</li>
<li>
<p>Problem: if the treatment effect for the always-treated grows over time, that will enter <img alt="\\hat{\\beta}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D" title="\\hat{\\beta}"> negatively!</p>
</li>
<li>
<p>Can also show similar intuition using Frish-Waugh-Lovell (see <a href="/post/ols-website-version/#sec-FWL">Section 9.1</a>)</p>
</li>
<li>
<p>The literature has placed a lot of emphasis on the fact that some treatment effects may get negative weights.</p>
</li>
<li>
<p>But even if the weights are non-negative,they might not give us the most intuitive parameter</p>
</li>
<li>
<p>For example, suppose each unit <img alt="i" src="https://latex.codecogs.com/svg.latex?i" title="i"> has treatment effect <img alt="\\tau_i" src="https://latex.codecogs.com/svg.latex?%5Ctau_i" title="\\tau_i"> in every period if they are treated (no dynamics). Then <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> gives a weighted average of the <img alt="\\tau_i" src="https://latex.codecogs.com/svg.latex?%5Ctau_i" title="\\tau_i"> where the weights are largest for units treated closest to the middle of the panel</p>
</li>
<li>
<p>It is not obvious that these weights are relevant for policy, even if they are all non-negative!</p>
</li>
<li>
<p>The possibility of negative weights is concerning because, for instance, all of the treatment effects (<img alt="\\tau_s" src="https://latex.codecogs.com/svg.latex?%5Ctau_s" title="\\tau_s"> show the treatment effect in the <img alt="s^{th}" src="https://latex.codecogs.com/svg.latex?s%5E%7Bth%7D" title="s^{th}"> period after treatment) could be positive and yet the coefficient <img alt="\\beta_{post}" src="https://latex.codecogs.com/svg.latex?%5Cbeta_%7Bpost%7D" title="\\beta_{post}"> may be negative! To see why, take the coefficient from the regression <img alt="\\color{red}{\\beta_{post}}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bred%7D%7B%5Cbeta_%7Bpost%7D%7D" title="\\color{red}{\\beta_{post}}"> <img alt="Y_{i,t} =\\alpha_i +\\phi_t +D_{i,t}\\color{red}{\\beta_{post}}+\\epsilon_{i,t}" src="https://latex.codecogs.com/svg.latex?Y_%7Bi%2Ct%7D%20%3D%5Calpha_i%20%2B%5Cphi_t%20%2BD_%7Bi%2Ct%7D%5Ccolor%7Bred%7D%7B%5Cbeta_%7Bpost%7D%7D%2B%5Cepsilon_%7Bi%2Ct%7D" title="Y_{i,t} =\\alpha_i +\\phi_t +D_{i,t}\\color{red}{\\beta_{post}}+\\epsilon_{i,t}">. In this case, <img alt="\\beta_{post} = \\sum_s \\omega_s \\tau_s" src="https://latex.codecogs.com/svg.latex?%5Cbeta_%7Bpost%7D%20%3D%20%5Csum_s%20%5Comega_s%20%5Ctau_s" title="\\beta_{post} = \\sum_s \\omega_s \\tau_s">. So even if all the <img alt="\\tau_s" src="https://latex.codecogs.com/svg.latex?%5Ctau_s" title="\\tau_s"> are positive, negative weights <img alt="\\omega_s" src="https://latex.codecogs.com/svg.latex?%5Comega_s" title="\\omega_s"> could lead to a negative <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta">! In particular, longer-run treatment effects will often receive negative weights. Thus, for example, it is possible that the effect of Medicaid expansion on insurance coverage is positive and grows over time since the expansion, and yet <img alt="\\color{red}{\\beta_{post}}" src="https://latex.codecogs.com/svg.latex?%5Ccolor%7Bred%7D%7B%5Cbeta_%7Bpost%7D%7D" title="\\color{red}{\\beta_{post}}"> <img alt="Y_{i,t} =\\alpha_i +\\phi_t +D_{i,t}\\color{red}{\\beta_{post}}+\\epsilon_{i,t}" src="https://latex.codecogs.com/svg.latex?Y_%7Bi%2Ct%7D%20%3D%5Calpha_i%20%2B%5Cphi_t%20%2BD_%7Bi%2Ct%7D%5Ccolor%7Bred%7D%7B%5Cbeta_%7Bpost%7D%7D%2B%5Cepsilon_%7Bi%2Ct%7D" title="Y_{i,t} =\\alpha_i +\\phi_t +D_{i,t}\\color{red}{\\beta_{post}}+\\epsilon_{i,t}"> will be negative. More generally, if treatment effects vary across both time and units, then <img alt="\\tau_{i,t}(g)" src="https://latex.codecogs.com/svg.latex?%5Ctau_%7Bi%2Ct%7D%28g%29" title="\\tau_{i,t}(g)"> may get negative weight in the TWFE estimand for some combinations of <img alt="t" src="https://latex.codecogs.com/svg.latex?t" title="t"> and <img alt="g" src="https://latex.codecogs.com/svg.latex?g" title="g">.</p>
</li>
<li>
<p>Goodman-Bacon provides some helpful intuition to understand this phenomenon. He shows that <img alt="\\hat{\\beta}_{post}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D_%7Bpost%7D" title="\\hat{\\beta}_{post}"> can be written as a convex weighted average of differences-in-differences comparisons between pairs of units and time periods in which one unit changed its treatment status and the other did not. Counterintuitively, however, this decomposition includes difference-in-differences that use as a &lsquo;&lsquo;control&rsquo;&rsquo; group units who were treated in earlier periods. For example, in 2016, a state that first expanded Medicaid in 2014 might be used as the &lsquo;&lsquo;control group&rsquo;&rsquo; for a state that first adopted Medicaid in 2016. Hence, an early-treated unit can get negative weights if it appears as a &lsquo;&lsquo;control&rsquo;&rsquo; for many later-treated units. This decomposition further highlights that βpost may not be a sensible estimand when treatment effects differ across either units or time, because of its inclusion of these &lsquo;&lsquo;forbidden comparisons&rsquo;&rsquo;</p>
</li>
<li>
<p>This decomposition makes clear that the static OLS coefficient <img alt="\\hat{\\beta}_{post}" src="https://latex.codecogs.com/svg.latex?%5Chat%7B%5Cbeta%7D_%7Bpost%7D" title="\\hat{\\beta}_{post}"> is not aggregating natural comparisons of units, and thus will not produce a sensible estimand when there is arbitrary heterogeneity. When treatment effects are homogeneous &mdash; i.e. <img alt="\\tau_i" src="https://latex.codecogs.com/svg.latex?%5Ctau_i" title="\\tau_i">,<img alt="t(g) \\equiv \\tau" src="https://latex.codecogs.com/svg.latex?t%28g%29%20%5Cequiv%20%5Ctau" title="t(g) \\equiv \\tau"> &mdash; the negative weights on <img alt="\\tau" src="https://latex.codecogs.com/svg.latex?%5Ctau" title="\\tau"> for some observations cancel out the positive weights for other observations, and thus <img alt="\\beta_{post}" src="https://latex.codecogs.com/svg.latex?%5Cbeta_%7Bpost%7D" title="\\beta_{post}"> recovers the causal effect under a suitable generalization of parallel trends.</p>
</li>
</ul>
<p>Several recent papers introduce diagnostic approaches for understanding the extent of the aggregation issues under staggered treatment timing, with a focus on the static specification (5). de Chaisemartin and D&rsquo;Haultfoeuille (2020) propose reporting the number/fraction of group-time ATTs that receive negative weights, as well as the degree of heterogeneity in treatment effects that would be necessary for the estimated treatment effect to have the &lsquo;&lsquo;wrong sign&rsquo;&rsquo;. Goodman-Bacon (2021) proposes reporting the weights that ˆβpost places on the different 2-group, 2-period difference-in-differences, which allows one to evaluate how much weight is being placed on &lsquo;&lsquo;forbidden&rsquo;&rsquo; comparisons of already-treated units and how removing the comparisons would change the estimate. Jakiela (2021) proposes evaluating both whether TWFE places negative weights on some treated units and whether the data rejects the constant treatment effects assumption.</p>
<ol>
<li>Negative results:TWFE OLS doesn&rsquo;t give us what we want with treatment effect heterogeneity</li>
<li>New estimators: perform better under treatment effect heterogeneity</li>
</ol>
<p>See e.g., <a href="http://fmwww.bc.edu/repec/frsug2022/France22_dHaultfoeuille.pdf">http://fmwww.bc.edu/repec/frsug2022/France22_dHaultfoeuille.pdf</a> and <a href="https://friosavila.github.io/playingwithstata/main_didmany.html#:~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say." class="uri"><a href="https://friosavila.github.io/playingwithstata/main_didmany.html">https://friosavila.github.io/playingwithstata/main_didmany.html#</a>:~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say.</a></p>
<p>I also explain why the negative weights occur: when already-treated units act as controls, changes in their treatment effects over time get subtracted from the DD estimate. This negative weighting only arises when treatment effects vary over time, in which case it typically biases regression DD estimates away from the sign of the true treatment effect. This does not imply a failure of the underlying design, but it does caution against the use of a single-coefficient two-way fixed effects specification to summarize time-varying effects.</p>
<ul>
<li>(Steve note - they are subtracted because they&rsquo;re on the right hand side of the equation!) <a href="https://www.nber.org/system/files/working_papers/w25018/w25018.pdf">https://www.nber.org/system/files/working_papers/w25018/w25018.pdf</a></li>
</ul>
<p><a href="https://stats.stackexchange.com/questions/529447/when-does-the-weight-of-dd-estimator-become-negative">https://stats.stackexchange.com/questions/529447/when-does-the-weight-of-dd-estimator-become-negative</a></p>
<p>&ldquo;We show that they estimate weighted sums of the average treatment effects (ATE) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive.&rdquo;</p>
<p>&ldquo;Almost 20% of empirical articles published in the AER between 2010 and 2012 use regressions with groups and period fixed effects to estimate treatment effects. In this paper, we show that under a common trends assumption, those regressions estimate weighted sums of the treatment effect in each group and period. The weights may be negative: in one application, we find that almost 50% of the weights are negative. The negative weights are an issue when the treatment effect is heterogeneous, between groups or over time. Then, one could have that the treatment&rsquo;s coefficient in those regressions is negative while the treatment effect is positive in every group and time period.&rdquo; See Clement de Chaisemartin and Xavier D&rsquo;Haultfoeuille&rsquo;s 2019 paper for more details.</p>
<h3 id="new-estimators">New Estimators</h3>
<h4 id="further-reading">Further Reading</h4>
<ul>
<li>
<p>DiD with continuous/multi-valued treatments. Callaway, Goodman-Bacon and Sant&rsquo;Anna (2021)</p>
</li>
<li>
<p>When is DiD sensitive to functional form assumptions? Roth and Sant&rsquo;Anna (2022a)</p>
</li>
<li>
<p>What types of selection models are compatible with parallel trends? Ghanem, Sant&rsquo;Anna and Wüthrich (2022)</p>
</li>
<li>
<p>How to incorporate Machine Learning into DiD? Chang (2020)</p>
<pre><code>What if we have multiple treatments?
</code></pre>
<p>de Chaisemartin and D&rsquo;Haultfœuille (2022)</p>
</li>
<li>
<p><a href="https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/">https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/</a> for general diff-in-diff</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2107.02637.pdf">https://arxiv.org/pdf/2107.02637.pdf</a> for continuous treatment</p>
</li>
<li>
<p><a href="https://pjakiela.github.io/TWFE/TWFE-2021-03-24.pdf">https://pjakiela.github.io/TWFE/TWFE-2021-03-24.pdf</a> diagnostics for two way fixed effects</p>
</li>
<li>
<p><a href="https://github.com/Mixtape-Sessions/Advanced-DID/blob/main/Slides/03-Violations.tex">https://github.com/Mixtape-Sessions/Advanced-DID/blob/main/Slides/03-Violations.tex</a> for Jonathan Roth&rsquo;s slides</p>
</li>
</ul>
<h1 id="appendix">Appendix</h1>
<p>Stuff to add to this when I get around to it!</p>
<h2 id="question">Question</h2>
<p>You accidentally double every observation, what happens to beta?</p>
<p>You end up with <img alt="\\beta = 1/2 \\times (X'X)^{-1}XY" src="https://latex.codecogs.com/svg.latex?%5Cbeta%20%3D%201%2F2%20%5Ctimes%20%28X%27X%29%5E%7B-1%7DXY" title="\\beta = 1/2 \\times (X&#39;X)^{-1}XY"></p>
<p>Suppose you accidentally append the data to itself, what happens?</p>
<p>Coefficient remains the same.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">10</span><span class="p">,</span> <span class="m">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl"><span class="n">x3</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="m">50</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="nf">rnorm</span><span class="p">(</span><span class="m">1000</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">doub_y</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">doub_x</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="o">~</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Call:
lm(formula = y ~ x)

Coefficients:
(Intercept)            x  
     50.051        0.987  
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="o">~</span><span class="n">x2</span><span class="p">)</span>  
</span></span></code></pre></div><pre><code>Call:
lm(formula = y ~ x2)

Coefficients:
(Intercept)           x2  
    50.0512       0.4935  
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="o">~</span><span class="n">x3</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Call:
lm(formula = y ~ x3)

Coefficients:
(Intercept)           x3  
     50.051        0.329  
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="nf">lm</span><span class="p">(</span><span class="n">doub_y</span> <span class="o">~</span> <span class="n">doub_x</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Call:
lm(formula = doub_y ~ doub_x)

Coefficients:
(Intercept)       doub_x  
     50.051        0.987  
</code></pre>
<h2 id="best-linear-predictor">Best Linear Predictor</h2>
<p>We&rsquo;re going to show that, for a population, <img alt="\\beta" src="https://latex.codecogs.com/svg.latex?%5Cbeta" title="\\beta"> is the best linear predictor for y in the mean-squared error sense (i.e, it has the lowest mean squared error).</p>
<ul>
<li><a href="http://www.its.caltech.edu/~mshum/stats/natural2.pdf">http://www.its.caltech.edu/~mshum/stats/natural2.pdf</a></li>
<li><a href="http://prob140.org/fa18/textbook/chapters/Chapter_25/02_Best_Linear_Predictor">http://prob140.org/fa18/textbook/chapters/Chapter_25/02_Best_Linear_Predictor</a></li>
</ul>
<h2 id="dealing-with-data">Dealing with Data</h2>
<p>Three types of extreme values</p>
<p>1 Outlier: extreme in the y direction</p>
<p>2 Leverage point: extreme in one x direction</p>
<p>3 Influence point: extreme in both directions</p>
<p>Is the data corrupted? I Fix the observation (obvious data entry errors) I Remove the observation I Be transparent either way</p>
<p>Is the outlier part of the data generating process? I Transform the dependent variable (log(y)) I Use a method that is robust to outliers (robust regression)</p>
<p><a href="https://scholar.princeton.edu/sites/default/files/bstewart/files/lecture9handout.pdf">https://scholar.princeton.edu/sites/default/files/bstewart/files/lecture9handout.pdf</a></p>
<p>Logistic Regression <a href="https://bookdown.org/egarpor/SSS2-UC3M/logreg-assumps.html">https://bookdown.org/egarpor/SSS2-UC3M/logreg-assumps.html</a></p>
<h2 id="bias-varience-tradeoff">Bias Varience Tradeoff</h2>
<p><a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></p>
<h2 id="l1-and-l2-regularization-in-linear-regression">L1 and L2 Regularization in Linear Regression</h2>
<p><a href="https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture02.pdf">https://www.cs.mcgill.ca/~dprecup/courses/ML/Lectures/ml-lecture02.pdf</a> A general, HUGELY IMPORTANT problem for all machine learning algorithms • We can find a hypothesis that predicts perfectly the training data but does not generalize well to new data</p>
<p>See <a href="https://uc-r.github.io/regularized_regression">https://uc-r.github.io/regularized_regression</a></p>
<p>Regularization is a technique that allows</p>
<p>With a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection.</p>
<p>Regularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model.</p>
<p>However, elastic nets, and regularization models in general, still assume linear relationships between the features and the target variable.</p>
<h2 id="the-math">The Math</h2>
<p>The objective function of regularized regression methods is very similar to OLS regression; however, we add a penalty parameter (P).</p>
<p>I.e. in a regression we minimize the sum of squared errors (SSE) as in <img alt="\\text{min}{e'e} = \\text{min}{(Y-X\\beta)'(Y-X\\beta)} \\rightarrow \\beta = (X'X)^{-1}X'Y" src="https://latex.codecogs.com/svg.latex?%5Ctext%7Bmin%7D%7Be%27e%7D%20%3D%20%5Ctext%7Bmin%7D%7B%28Y-X%5Cbeta%29%27%28Y-X%5Cbeta%29%7D%20%5Crightarrow%20%5Cbeta%20%3D%20%28X%27X%29%5E%7B-1%7DX%27Y" title="\\text{min}{e&#39;e} = \\text{min}{(Y-X\\beta)&#39;(Y-X\\beta)} \\rightarrow \\beta = (X&#39;X)^{-1}X&#39;Y"></p>
<p>Imagine, instead we add a penalty term to the minimization problem that constrains the coefficients and progressively shrink them towards zero.</p>
<p><img alt="\\text{Min{SSE - P}}" src="https://latex.codecogs.com/svg.latex?%5Ctext%7BMin%7BSSE%20-%20P%7D%7D" title="\\text{Min{SSE - P}}"></p>
<p>What values can P take? There are two main options - L1 and L2:</p>
<ul>
<li>L1 - or LASSO <img alt="P = \\lambda \\sum_{j=1}^{p}" src="https://latex.codecogs.com/svg.latex?P%20%3D%20%5Clambda%20%5Csum_%7Bj%3D1%7D%5E%7Bp%7D" title="P = \\lambda \\sum_{j=1}^{p}"> |<img alt="B_j" src="https://latex.codecogs.com/svg.latex?B_j" title="B_j">| - L2 - or Ridge Regression <img alt="P = \\lambda \\sum_{j=1}^{p}B_j^2" src="https://latex.codecogs.com/svg.latex?P%20%3D%20%5Clambda%20%5Csum_%7Bj%3D1%7D%5E%7Bp%7DB_j%5E2" title="P = \\lambda \\sum_{j=1}^{p}B_j^2"></li>
</ul>
<p>In both cases <img alt="\\lambda" src="https://latex.codecogs.com/svg.latex?%5Clambda" title="\\lambda"> is a tuning parameter that helps to control our model from over-fitting to the training data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-r" data-lang="r"><span class="line"><span class="cl"><span class="c1"># Regularization</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Ridge Regression in R</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Load libraries, get data &amp; set</span>
</span></span><span class="line"><span class="cl"><span class="c1"># seed for reproducibility </span>
</span></span><span class="line"><span class="cl"><span class="nf">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span>    
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">glmnet</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>   
</span></span><span class="line"><span class="cl"><span class="nf">library</span><span class="p">(</span><span class="n">psych</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="nf">data</span><span class="p">(</span><span class="s">&#34;mtcars&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Center y, X will be standardized </span>
</span></span><span class="line"><span class="cl"><span class="c1"># in the modelling function</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">&lt;-</span> <span class="n">mtcars</span> <span class="o">%&gt;%</span> <span class="n">dplyr</span><span class="o">::</span><span class="nf">select</span><span class="p">(</span><span class="n">mpg</span><span class="p">)</span> <span class="o">%&gt;%</span> 
</span></span><span class="line"><span class="cl">            <span class="nf">scale</span><span class="p">(</span><span class="n">center</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="kc">FALSE</span><span class="p">)</span> <span class="o">%&gt;%</span> 
</span></span><span class="line"><span class="cl">            <span class="nf">as.matrix</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">&lt;-</span> <span class="n">mtcars</span> <span class="o">%&gt;%</span> <span class="n">dplyr</span><span class="o">::</span><span class="nf">select</span><span class="p">(</span><span class="o">-</span><span class="n">mpg</span><span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">as.matrix</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Perform 10-fold cross-validation to select lambda</span>
</span></span><span class="line"><span class="cl"><span class="n">lambdas_to_try</span> <span class="o">&lt;-</span> <span class="m">10</span><span class="nf">^seq</span><span class="p">(</span><span class="m">-3</span><span class="p">,</span> <span class="m">5</span><span class="p">,</span> <span class="n">length.out</span> <span class="o">=</span> <span class="m">100</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Setting alpha = 0 implements ridge regression</span>
</span></span><span class="line"><span class="cl"><span class="n">ridge_cv</span> <span class="o">&lt;-</span> <span class="nf">cv.glmnet</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                      <span class="n">lambda</span> <span class="o">=</span> <span class="n">lambdas_to_try</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                      <span class="n">standardize</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">,</span> <span class="n">nfolds</span> <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Plot cross-validation results</span>
</span></span><span class="line"><span class="cl"><span class="nf">plot</span><span class="p">(</span><span class="n">ridge_cv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Best cross-validated lambda</span>
</span></span><span class="line"><span class="cl"><span class="n">lambda_cv</span> <span class="o">&lt;-</span> <span class="n">ridge_cv</span><span class="o">$</span><span class="n">lambda.min</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Fit final model, get its sum of squared</span>
</span></span><span class="line"><span class="cl"><span class="c1"># residuals and multiple R-squared</span>
</span></span><span class="line"><span class="cl"><span class="n">model_cv</span> <span class="o">&lt;-</span> <span class="nf">glmnet</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="n">lambda_cv</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                   <span class="n">standardize</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y_hat_cv</span> <span class="o">&lt;-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">model_cv</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ssr_cv</span> <span class="o">&lt;-</span> <span class="nf">t</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat_cv</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat_cv</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rsq_ridge_cv</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat_cv</span><span class="p">)</span><span class="n">^2</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># selecting lambda based on the information</span>
</span></span><span class="line"><span class="cl"><span class="n">X_scaled</span> <span class="o">&lt;-</span> <span class="nf">scale</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">aic</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">bic</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="kr">for</span> <span class="p">(</span><span class="n">lambda</span> <span class="kr">in</span> <span class="nf">seq</span><span class="p">(</span><span class="n">lambdas_to_try</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="c1"># Run model</span>
</span></span><span class="line"><span class="cl">  <span class="n">model</span> <span class="o">&lt;-</span> <span class="nf">glmnet</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">lambda</span> <span class="o">=</span> <span class="n">lambdas_to_try[lambda]</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                  <span class="n">standardize</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">  <span class="c1"># Extract coefficients and residuals (remove first </span>
</span></span><span class="line"><span class="cl">  <span class="c1"># row for the intercept)</span>
</span></span><span class="line"><span class="cl">  <span class="n">betas</span> <span class="o">&lt;-</span> <span class="nf">as.vector</span><span class="p">((</span><span class="nf">as.matrix</span><span class="p">(</span><span class="nf">coef</span><span class="p">(</span><span class="n">model</span><span class="p">))</span><span class="n">[</span><span class="m">-1</span><span class="p">,</span> <span class="n">]</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">resid</span> <span class="o">&lt;-</span> <span class="n">y</span> <span class="o">-</span> <span class="p">(</span><span class="n">X_scaled</span> <span class="o">%*%</span> <span class="n">betas</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">  <span class="c1"># Compute hat-matrix and degrees of freedom</span>
</span></span><span class="line"><span class="cl">  <span class="n">ld</span> <span class="o">&lt;-</span> <span class="n">lambdas_to_try[lambda]</span> <span class="o">*</span> <span class="nf">diag</span><span class="p">(</span><span class="nf">ncol</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  <span class="n">H</span> <span class="o">&lt;-</span> <span class="n">X_scaled</span> <span class="o">%*%</span> <span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X_scaled</span> <span class="o">+</span> <span class="n">ld</span><span class="p">)</span> <span class="o">%*%</span> <span class="nf">t</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">df</span> <span class="o">&lt;-</span> <span class="nf">tr</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">  <span class="c1"># Compute information criteria</span>
</span></span><span class="line"><span class="cl">  <span class="n">aic[lambda]</span> <span class="o">&lt;-</span> <span class="nf">nrow</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span> <span class="o">*</span> <span class="nf">log</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">resid</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">resid</span><span class="p">)</span> <span class="o">+</span> <span class="m">2</span> <span class="o">*</span> <span class="n">df</span>
</span></span><span class="line"><span class="cl">  <span class="n">bic[lambda]</span> <span class="o">&lt;-</span> <span class="nf">nrow</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span> <span class="o">*</span> <span class="nf">log</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">resid</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">resid</span><span class="p">)</span> <span class="o">+</span> <span class="m">2</span> <span class="o">*</span> <span class="n">df</span> <span class="o">*</span> <span class="nf">log</span><span class="p">(</span><span class="nf">nrow</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Plot information criteria against tried values of lambdas</span>
</span></span><span class="line"><span class="cl"><span class="nf">plot</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">lambdas_to_try</span><span class="p">),</span> <span class="n">aic</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&#34;orange&#34;</span><span class="p">,</span> <span class="n">type</span> <span class="o">=</span> <span class="s">&#34;l&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">190</span><span class="p">,</span> <span class="m">260</span><span class="p">),</span> <span class="n">ylab</span> <span class="o">=</span> <span class="s">&#34;Information Criterion&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">lines</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">lambdas_to_try</span><span class="p">),</span> <span class="n">bic</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="s">&#34;skyblue3&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">legend</span><span class="p">(</span><span class="s">&#34;bottomright&#34;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;orange&#34;</span><span class="p">,</span> <span class="s">&#34;skyblue3&#34;</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">       <span class="n">legend</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="s">&#34;AIC&#34;</span><span class="p">,</span> <span class="s">&#34;BIC&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Optimal lambdas according to both criteria</span>
</span></span><span class="line"><span class="cl"><span class="n">lambda_aic</span> <span class="o">&lt;-</span> <span class="n">lambdas_to_try</span><span class="nf">[which.min</span><span class="p">(</span><span class="n">aic</span><span class="p">)</span><span class="n">]</span>
</span></span><span class="line"><span class="cl"><span class="n">lambda_bic</span> <span class="o">&lt;-</span> <span class="n">lambdas_to_try</span><span class="nf">[which.min</span><span class="p">(</span><span class="n">bic</span><span class="p">)</span><span class="n">]</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># Fit final models, get their sum of </span>
</span></span><span class="line"><span class="cl"><span class="c1"># squared residuals and multiple R-squared</span>
</span></span><span class="line"><span class="cl"><span class="n">model_aic</span> <span class="o">&lt;-</span> <span class="nf">glmnet</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="n">lambda_aic</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">standardize</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y_hat_aic</span> <span class="o">&lt;-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">model_aic</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ssr_aic</span> <span class="o">&lt;-</span> <span class="nf">t</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat_aic</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat_aic</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rsq_ridge_aic</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat_aic</span><span class="p">)</span><span class="n">^2</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="n">model_bic</span> <span class="o">&lt;-</span> <span class="nf">glmnet</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="n">lambda_bic</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">                    <span class="n">standardize</span> <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y_hat_bic</span> <span class="o">&lt;-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">model_bic</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">ssr_bic</span> <span class="o">&lt;-</span> <span class="nf">t</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat_bic</span><span class="p">)</span> <span class="o">%*%</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat_bic</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rsq_ridge_bic</span> <span class="o">&lt;-</span> <span class="nf">cor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_hat_bic</span><span class="p">)</span><span class="n">^2</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># The higher the lambda, the more the </span>
</span></span><span class="line"><span class="cl"><span class="c1"># coefficients are shrinked towards zero.</span>
</span></span><span class="line"><span class="cl"><span class="n">res</span> <span class="o">&lt;-</span> <span class="nf">glmnet</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="m">0</span><span class="p">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="n">lambdas_to_try</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">              <span class="n">standardize</span> <span class="o">=</span> <span class="kc">FALSE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">plot</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">xvar</span> <span class="o">=</span> <span class="s">&#34;lambda&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nf">legend</span><span class="p">(</span><span class="s">&#34;bottomright&#34;</span><span class="p">,</span> <span class="n">lwd</span> <span class="o">=</span> <span class="m">1</span><span class="p">,</span> <span class="n">col</span> <span class="o">=</span> <span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">       <span class="n">legend</span> <span class="o">=</span> <span class="nf">colnames</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">cex</span> <span class="o">=</span> <span class="m">.7</span><span class="p">)</span>
</span></span></code></pre></div><img src="figs/unnamed-chunk-16-1.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<img src="figs/unnamed-chunk-16-2.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<img src="figs/unnamed-chunk-16-3.png" style="width:100.0%" data-fig-align="center" data-fig-pos="htbp" />
<h2 id="whats-your-estimand">What&rsquo;s your estimand</h2>
<p>&ldquo;In every quantitative paper we read, every quantitative talk we attend, and every quantitative article we write, we should all ask one question: what is the estimand? The estimand is the object of inquiry&mdash;it is the precise quantity about which we marshal data to draw an inference.&rdquo;</p>
<p>&ldquo;Our framework stands in contrast to the currently dominant mode of quantitative inquiry: hypotheses about regression coefficients. That mode of inquiry defines the research goal inside a particular statistical model. If your research goal is a coefficient of a particular model, then you are committed to that model: it becomes impossible to reason about other approaches to achieve the goal. By contrast, we advocate a statement of the goal outside the statistical model&mdash;like an average causal effect or a population mean&mdash;which opens the door to alternative estimation procedures that could answer the research question under more credible assumptions.&rdquo;</p>
<p>&ldquo;We introduce a term for the goal stated outside the model&mdash;the theoretical estimand&mdash;which has two components. The first is a unit-specific quantity, which could be a realized outcome (whether person i is employed), a potential outcome (whether person i would be employed if they received job training), or a difference in potential outcomes (the effect of job training on the employment of person i). It could also be a potential outcome that would be realized under intervention of more than one variable (whether person i would be employed if they received job training and childcare), thus unlocking numerous new causal questions. The unit-specific quantity clarifies whether the research goal is causal, and if so, what counterfactual intervention is being considered. The second component of the theoretical estimand is the target population: over whom or what do we aggregate that unit-specific quantity? The unit-specific quantity and target population combine to define the theoretical estimand: the thing we would like to know if we had data for the full population in all factual or counterfactual worlds of interest. A paper may have multiple theoretical estimands.&rdquo;</p>
<p>&ldquo;Each theoretical estimand is linked to an empirical estimand involving only observable quantities (e.g., a difference in means in a population) by assumptions about the relationship between the data we observe and the data we do not. These identification assumptions can be conveyed in a Directed Acyclic Graph (DAG). Finally, one chooses an estimation strategy to learn the empirical estimand (e.g., a regression model). We use the general term &ldquo;estimands&rdquo; to refer to both the theoretical and the empirical estimands.&rdquo;</p>
<img src="theory-to-estimation.png" id="fig-dataflow" alt="Figure 1: Data Analysis Workflow" />
<p>&ldquo;Too often, research papers involve pages of rich theory followed by pages of procedures applied to data, with a vague link between the two. The theoretical and empirical estimand fill the void by precisely stating both the theoretical quantity we would like to know and the empirical quantity that our procedures are most directly designed to approximate.&rdquo;</p>
<p>&quot; A clear statement of the target population allows a researcher to clarify which approach they are taking.&quot;</p>
<p>&ldquo;The IV design offers strong causal identification at a cost: the estimated causal effect is an average not over the full population, but only over the subpopulation of compliers whose treatment status is causally affected by the instrument (Imbens and Angrist 1994).&rdquo;</p>
<p>&ldquo;A lack of common support arises whenever some subpopulation defined by a confounder (e.g., terrorists) contains no treated units or no untreated units (e.g., those on probation or not). Common support problems leave researchers three options. They can argue that the feasible subpopulation&mdash;those with covariates at which both treated and control units are observed&mdash;is theoretically interesting (a leap at the link between theory and the theoretical estimand); they can argue that the feasible subpopulation is informative about the broader population (a leap between the theoretical and empirical estimand); or, they can lean heavily on a parametric model and extrapolate what is observed in the feasible subpopulation to what they think would happen in the space beyond common support (a leap in estimation). As in experiments and IV, there is no free lunch. A statement of the target population is an opportunity for authors to put the difficulty in the pages of the article and clarify how they address it.&rdquo;</p>
<p>&ldquo;To summarize, the theoretical estimand states the study aim in precise terms involving a unit-specific quantity aggregated over a target population. The theoretical estimand exists outside of any statistical model and liberates us to make complex research questions precise. Descriptive estimands can be stated even if some of the population would refuse all survey attempts or is structurally missing from administrative records. Causal estimands can be stated in terms of counterfactuals we could never observe. In contrast to the constraints of regression coefficients, a theoretical estimand allows us to formalize the quantity most relevant to theory.&rdquo;</p>
<p><sup id="fnref:17"><a href="#fn:17" class="footnote-ref" role="doc-noteref">17</a></sup></p>
<p>&quot; Instead of thinking about estimating the parameters of a model, we must think of the estimation algorithm as a tool to estimate the unknown components (e.g., conditional means) that appear in the empirical estimand. Doing so allows empirical evidence to inform the choice of an estimation strategy. Conceptual argument is central to the statement of the theoretical and empirical estimands, but selection of an estimation strategy can be largely data-driven.&quot;</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>&ldquo;Econometric identification really means just one thing: model parameters or features being uniquely determined from the observable population that generates the data.&rdquo; See more at <a href="http://fmwww.bc.edu/EC-P/wp957.pdf">http://fmwww.bc.edu/EC-P/wp957.pdf</a>. Cyrus Samii defines it as &ldquo;Identification refers generally to sufficiency for drawing a conclusion given the type of data that are available. [shorter link]<a href="https://uc76a733439785d112bbe0d38b98.dl.dropboxusercontent.com/cd/0/inline2/CQ61-3qehcE2gmowHjic11s015sJF6D1WJfI_poSUzkCRE9YxB8Db8l7Rj5D84svlwKIp9XQiqQ67gDu_T0hr8r8sqnKnFxqOxIdIm8M8Zt-aR2kauFd8_K9CxTtL7c9_M1kaikQZEj2XZ69YLdJE8yaQe-J5r7PROGzFXSAWWiJ5cCdo2ByxR8QZV3DjE2I4xRknw1LPeSwtIKBrJpiNQf7DU-sydqctqTQPUv6wqN4r1VPZF1Uvmip2evOt9dqZDp77xD1kxEsQV-AXjBrigqltjSVYYq2caOQELnzuIfov_TlNLPKRnW4lFoVEj5UccjZQLxlsMrJLUx0C1ZFQ4wsyZFnuPDyo0WQPtu8HQ-bpxE7fvBmTP3JeaWErj-Txmk/file">https://uc76a733439785d112bbe0d38b98.dl.dropboxusercontent.com/cd/0/inline2/CQ61-3qehcE2gmowHjic11s015sJF6D1WJfI_poSUzkCRE9YxB8Db8l7Rj5D84svlwKIp9XQiqQ67gDu_T0hr8r8sqnKnFxqOxIdIm8M8Zt-aR2kauFd8_K9CxTtL7c9_M1kaikQZEj2XZ69YLdJE8yaQe-J5r7PROGzFXSAWWiJ5cCdo2ByxR8QZV3DjE2I4xRknw1LPeSwtIKBrJpiNQf7DU-sydqctqTQPUv6wqN4r1VPZF1Uvmip2evOt9dqZDp77xD1kxEsQV-AXjBrigqltjSVYYq2caOQELnzuIfov_TlNLPKRnW4lFoVEj5UccjZQLxlsMrJLUx0C1ZFQ4wsyZFnuPDyo0WQPtu8HQ-bpxE7fvBmTP3JeaWErj-Txmk/file</a>&rdquo;&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://friosavila.github.io/playingwithstata/main_didmany.html#:~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say.">https://friosavila.github.io/playingwithstata/main_didmany.html#:~:text=In%20the%20basic%202x2%20DiD,identical%20in%20every%20single%20say.</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf">https://pdhp.isr.umich.edu/wp-content/uploads/2023/01/DiD_PDHP.pdf</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>This section mostly stolen from <a href="https://diff.healthpolicydatascience.org">https://diff.healthpolicydatascience.org</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>See <a href="/post/ols-website-version/#sec-terms">Section 3.2</a> for more detail&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>See Lundberg, I., Johnson, R. and Stewart, B.M., 2021. What is your estimand? Defining the target quantity connects statistical evidence to theory. American Sociological Review, 86(3), pp.532-565.<a href="https://www.rebeccajohnson.io/files/asr_estimands_pdf.pdf">https://www.rebeccajohnson.io/files/asr_estimands_pdf.pdf</a>. This article has a discussion about how to pick estimands.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>The problem, however, is that you don&rsquo;t know the true real-world value of the estimand. You only have the estimate. Which, as I discussed above, is a function of the estimand, bias, and noise (i.e. <img alt="\\text{estimate} = \\text{estimand} + \\text{bias} + \\text{noise}" src="https://latex.codecogs.com/svg.latex?%5Ctext%7Bestimate%7D%20%3D%20%5Ctext%7Bestimand%7D%20%2B%20%5Ctext%7Bbias%7D%20%2B%20%5Ctext%7Bnoise%7D" title="\\text{estimate} = \\text{estimand} + \\text{bias} + \\text{noise}">).&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p><a href="https://www.jonathandroth.com/assets/files/DiD_Review_Paper.pdf#:~:text=Our%20starting%20point%20in%20Section%202%20is,not%20receive%20the%20treatment%20in%20either%20period.">https://www.jonathandroth.com/assets/files/DiD_Review_Paper.pdf#:~:text=Our%20starting%20point%20in%20Section%202%20is,not%20receive%20the%20treatment%20in%20either%20period.</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>This is the like Medicare spending in a state that if they didn&rsquo;t expand Medicaid when they did expand Medicaid.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Note, however, there&rsquo;s nothing that stops an analyst from making a stupid decision like using TX or CA as a control group.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p><a href="https://diff.healthpolicydatascience.org">https://diff.healthpolicydatascience.org</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>This proof is shamelessly stolen from <a href="https://diff.healthpolicydatascience.org">https://diff.healthpolicydatascience.org</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>This applies to unconditional expectations too so <img alt="\\mathbb{E}[x+y]=\\mathbb{E}[x]+\\mathbb{E}[y]" src="https://latex.codecogs.com/svg.latex?%5Cmathbb%7BE%7D%5Bx%2By%5D%3D%5Cmathbb%7BE%7D%5Bx%5D%2B%5Cmathbb%7BE%7D%5By%5D" title="\\mathbb{E}[x+y]=\\mathbb{E}[x]+\\mathbb{E}[y]">&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p>This imposes that (a) all units have the same treatment effect, and (b) the treatment has the same effect regardless of how long it has been since treatment started. This would impose that the effect of Medicaid expansion on insurance coverage is the same both across states and across time. Then, under a suitable generalization of the parallel trends assumption and no anticipation assumption, the population regression coefficient <img alt="\\beta_post" src="https://latex.codecogs.com/svg.latex?%5Cbeta_post" title="\\beta_post"> from is equal to <img alt="\\tau" src="https://latex.codecogs.com/svg.latex?%5Ctau" title="\\tau">.&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p><a href="https://github.com/Mixtape-Sessions/Advanced-DID/?tab=readme-ov-file">https://github.com/Mixtape-Sessions/Advanced-DID/?tab=readme-ov-file</a>&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:17">
<p>Causal inference theory also clarifies the important distinction between a confounder and a collider. A confounder is a variable that causes both the exposure and the outcome. By contrast, a &ldquo;collider&rdquo; is a variable that is caused by at least two other variables (the causing variables &ldquo;collide&rdquo; in the collider). For example, if quality of life is affected by both smoking (exposure) and lung cancer (outcome), quality of life would be a collider and not a confounder for the association between smoking and lung cancer. This distinction is important, because methods designed to correct for confounding (e.g., regression analysis) can introduce bias if they are applied to colliders. For this reason, bias of this kind is termed &ldquo;collider bias&rdquo; (CB).&#160;<a href="#fnref:17" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div>
    <div class="post__footer">
      

      
    </div>

    
  </div>

      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        Steven Rashin
        2024
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js"
    integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ="
    crossorigin="anonymous"
  ></script></body>
</html>
